{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOFoNIiq47lfocnzIt6QGi/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rczhen/code_ml/blob/main/attention.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import inspect\n",
        "from dataclasses import dataclass\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F"
      ],
      "metadata": {
        "id": "bYaqKhZZCvgV"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclass\n",
        "class GPTConfig:\n",
        "    block_size: int = 1024\n",
        "    vocab_size: int = 50304 # GPT-2 vocab_size of 50257, padded up to nearest multiple of 64 for efficiency\n",
        "    n_layer: int = 12\n",
        "    n_head: int = 12\n",
        "    n_embd: int = 768\n",
        "    dropout: float = 0.0\n",
        "    bias: bool = True # True: bias in Linears and LayerNorms, like GPT-2. False: a bit better and faster\n",
        "\n",
        "@dataclass\n",
        "class Config:\n",
        "    block_size: int = 8\n",
        "    vocab_size: int = 50304 # GPT-2 vocab_size of 50257, padded up to nearest multiple of 64 for efficiency\n",
        "    n_layers: int = 2\n",
        "    n_heads: int = 4\n",
        "    d_model: int = 16\n",
        "    dropout_rate: float = 0.0\n",
        "    bias: bool = True # True: bias in Linears and LayerNorms, like GPT-2. False: a bit better and faster"
      ],
      "metadata": {
        "id": "aTdb4pojIvKR"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# v1: standard CausalSelfAttention"
      ],
      "metadata": {
        "id": "losC6CqJ2IAD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CausalSelfAttention(nn.Module):\n",
        "    def __init__(self, config) -> None:\n",
        "        super().__init__()\n",
        "        assert config.d_model % config.n_heads == 0\n",
        "\n",
        "        self.n_heads = config.n_heads\n",
        "        self.d_model = config.d_model\n",
        "        self.head_size = config.d_model // config.n_heads\n",
        "        self.block_size = config.block_size\n",
        "\n",
        "        self.attention_dropout = nn.Dropout(config.dropout_rate) # after softmax\n",
        "        self.residual_dropout = nn.Dropout(config.dropout_rate) # after attention block, before adding with residual connection\n",
        "\n",
        "        self.w_qkv = nn.Linear(self.d_model, 3 * self.d_model)\n",
        "        self.w_o = nn.Linear(self.d_model, self.d_model)\n",
        "\n",
        "        self.register_buffer(\"mask\", torch.tril(torch.ones(self.block_size, self.block_size)) # register buffer for low triangular matrix mask\n",
        "                                    .view(1, 1, self.block_size, self.block_size))  # reshape for (B, n_head, T, T) inputs\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, D = x.size() # batch size, sequence length, d_model\n",
        "\n",
        "        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n",
        "        q, k, v = self.w_qkv(x).split(self.d_model, dim=2) # (B, T, D) @ (D, 3D) --> (B, T, 3D) --> split at dim=2 --> (B, T, D)\n",
        "        q = q.view(B, T, self.n_heads, self.head_size).transpose(1, 2) # (B, T, nh, hs) --> (B, nh, T, hs), hs for head_size\n",
        "        k = k.view(B, T, self.n_heads, self.head_size).transpose(1, 2) # (B, T, nh, hs) --> (B, nh, T, hs), hs for head_size\n",
        "        v = v.view(B, T, self.n_heads, self.head_size).transpose(1, 2) # (B, T, nh, hs) --> (B, nh, T, hs), hs for head_size\n",
        "\n",
        "        # attention\n",
        "        attention = q @ k.transpose(-1, -2) # (B, nh, T, hs) @ (B, nh, hs, T) --> (B, nh, T, T)\n",
        "        attention *= self.head_size ** -0.5 # scaled dot product attention\n",
        "        attention = attention.masked_fill(self.mask[:, :, :T, :T] == 0, float('-inf'))\n",
        "        attention = F.softmax(attention, dim=-1)\n",
        "        attention = self.attention_dropout(attention)\n",
        "\n",
        "        # output\n",
        "        y = attention @ v # (B, nh, T, T) @ (B, nh, T, hs) --> (B, nh, T, hs)\n",
        "        y = y.transpose(1, 2).contiguous().view(B, T, D) # (B, nh, T, hs) --> (B, T, nh, hs) --> (B, T, D)\n",
        "        y = self.w_o(y) # (B, T, D) @ (D, D) --> (B, T, D)\n",
        "        y = self.residual_dropout(y)\n",
        "\n",
        "        return y\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "why scaled?\n",
        "if no normalization, the variance of weights wil be on the order of head_size, here is 16\n",
        "when deviding by sqrt(head_size), bring the variance back\n",
        "\"Scaled\" attention additional divides attention scores by 1/sqrt(head_size).\n",
        "so when input Q,K are unit variance, attentions will be unit variance too\n",
        "and Softmax will stay diffuse and not saturate too much.\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "id": "VCpKzgz7JaH3",
        "outputId": "ba7477ec-cfc2-4f35-81b9-ee007992eea8"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nwhy scaled? \\nif no normalization, the variance of weights wil be on the order of head_size, here is 16\\nwhen deviding by sqrt(head_size), bring the variance back\\n\"Scaled\" attention additional divides attention scores by 1/sqrt(head_size). \\nso when input Q,K are unit variance, attentions will be unit variance too\\nand Softmax will stay diffuse and not saturate too much.\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.rand(1, Config.block_size, Config.d_model)\n",
        "print(x)\n",
        "layer = CausalSelfAttention(Config)\n",
        "print(layer(x))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3_tilA7bPIQ3",
        "outputId": "a9375288-d9ef-4b36-b378-4f5697bf01f0"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[0.1335, 0.5033, 0.0119, 0.9521, 0.0588, 0.9284, 0.3936, 0.6225,\n",
            "          0.6452, 0.3971, 0.9477, 0.7729, 0.7849, 0.9677, 0.5737, 0.1898],\n",
            "         [0.0463, 0.7900, 0.2029, 0.2007, 0.9862, 0.8995, 0.8501, 0.4497,\n",
            "          0.0779, 0.4141, 0.6336, 0.6112, 0.7698, 0.5033, 0.5788, 0.3431],\n",
            "         [0.2839, 0.9898, 0.9354, 0.7228, 0.7328, 0.5463, 0.4106, 0.4918,\n",
            "          0.5689, 0.0622, 0.2844, 0.7631, 0.0073, 0.0599, 0.2667, 0.1991],\n",
            "         [0.1219, 0.4216, 0.6814, 0.1920, 0.7979, 0.9971, 0.0258, 0.6272,\n",
            "          0.6598, 0.0274, 0.6750, 0.7370, 0.5341, 0.4181, 0.9826, 0.5319],\n",
            "         [0.5073, 0.6594, 0.3810, 0.3264, 0.5320, 0.2145, 0.2166, 0.9515,\n",
            "          0.6532, 0.6341, 0.8759, 0.1684, 0.2516, 0.3369, 0.1295, 0.6470],\n",
            "         [0.8165, 0.4878, 0.8657, 0.7369, 0.4041, 0.3254, 0.9028, 0.6001,\n",
            "          0.1756, 0.1283, 0.1517, 0.7840, 0.8976, 0.4481, 0.9875, 0.7592],\n",
            "         [0.1032, 0.7290, 0.5401, 0.6724, 0.4569, 0.0051, 0.1456, 0.9306,\n",
            "          0.9603, 0.6033, 0.2806, 0.2433, 0.3356, 0.1294, 0.7252, 0.6078],\n",
            "         [0.5655, 0.2020, 0.1409, 0.4767, 0.6185, 0.9751, 0.9227, 0.1304,\n",
            "          0.0354, 0.6776, 0.1661, 0.8632, 0.4176, 0.5310, 0.0424, 0.8274]]])\n",
            "tensor([[[ 0.3622, -0.7585,  0.1746,  0.0592,  0.2357, -0.3599, -0.1901,\n",
            "           0.0520, -0.3618, -0.2674, -0.1691,  0.1149,  0.7611,  0.2544,\n",
            "           0.0546,  0.0048],\n",
            "         [ 0.2423, -0.6702,  0.1369,  0.0238,  0.1893, -0.2531, -0.2107,\n",
            "          -0.0359, -0.3884, -0.2788, -0.2291,  0.0507,  0.6437,  0.1947,\n",
            "           0.0010,  0.0230],\n",
            "         [ 0.1961, -0.6541,  0.1146,  0.0338,  0.1808, -0.1504, -0.2186,\n",
            "          -0.0599, -0.3505, -0.2530, -0.1988,  0.0311,  0.5926,  0.1497,\n",
            "          -0.0091,  0.0116],\n",
            "         [ 0.1464, -0.6422,  0.1174, -0.0093,  0.1411, -0.0987, -0.1941,\n",
            "          -0.0706, -0.3751, -0.3001, -0.2131, -0.0340,  0.5564,  0.1751,\n",
            "           0.0406,  0.0481],\n",
            "         [ 0.1530, -0.6416,  0.1218,  0.0176,  0.1322, -0.1102, -0.1940,\n",
            "          -0.0674, -0.3470, -0.2728, -0.2225, -0.0059,  0.5623,  0.1437,\n",
            "           0.0175,  0.0113],\n",
            "         [ 0.1296, -0.6246,  0.1097,  0.0144,  0.1494, -0.0535, -0.2091,\n",
            "          -0.0967, -0.3285, -0.2667, -0.2125, -0.0127,  0.5353,  0.1431,\n",
            "           0.0256,  0.0134],\n",
            "         [ 0.1069, -0.6356,  0.1028,  0.0202,  0.1504, -0.0275, -0.1921,\n",
            "          -0.0881, -0.3304, -0.2694, -0.2131, -0.0254,  0.5364,  0.1133,\n",
            "           0.0222,  0.0173],\n",
            "         [ 0.1162, -0.6369,  0.1127,  0.0172,  0.1424, -0.0450, -0.2095,\n",
            "          -0.0968, -0.3204, -0.2695, -0.2309, -0.0144,  0.5424,  0.1465,\n",
            "           0.0334, -0.0056]]], grad_fn=<ViewBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# v2: Talking Heads Attention"
      ],
      "metadata": {
        "id": "Db73Nty82UX-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CausalSelfAttention(nn.Module):\n",
        "    def __init__(self, config, talking_heads=True) -> None:\n",
        "        super().__init__()\n",
        "        assert config.d_model % config.n_heads == 0\n",
        "\n",
        "        self.n_heads = config.n_heads\n",
        "        self.d_model = config.d_model\n",
        "        self.head_size = config.d_model // config.n_heads\n",
        "        self.block_size = config.block_size\n",
        "\n",
        "        self.attention_dropout = nn.Dropout(config.dropout_rate) # after softmax\n",
        "        self.residual_dropout = nn.Dropout(config.dropout_rate) # after attention block, before adding with residual connection\n",
        "\n",
        "        self.w_qkv = nn.Linear(self.d_model, 3 * self.d_model)\n",
        "        self.w_o = nn.Linear(self.d_model, self.d_model)\n",
        "\n",
        "        # talking heads: 引入两个小的(n_heads, n_heads)矩阵, 在immediately before and after softmax, 进行head之间的线性变换\n",
        "        # 2 learnable linear transformations, process (1) attention scores (right after Q*V and scaling, before masking),\n",
        "        #   (2) logits (attention logits, after masking, optional padding, softmax, before weighted summing keys)\n",
        "        self.talking_heads = talking_heads\n",
        "        if self.talking_heads:\n",
        "            self.w_talking_weights = nn.Linear(self.n_heads, self.n_heads)\n",
        "            self.w_talking_logits = nn.Linear(self.n_heads, self.n_heads)\n",
        "\n",
        "        self.register_buffer(\"mask\", torch.tril(torch.ones(self.block_size, self.block_size)) # register buffer for low triangular matrix mask\n",
        "                                    .view(1, 1, self.block_size, self.block_size))  # reshape for (B, n_head, T, T) inputs\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, D = x.size() # batch size, sequence length, d_model\n",
        "\n",
        "        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n",
        "        q, k, v = self.w_qkv(x).split(self.d_model, dim=2) # (B, T, D) @ (D, 3D) --> (B, T, 3D) --> split at dim=2 --> (B, T, D)\n",
        "        q = q.view(B, T, self.n_heads, self.head_size).transpose(1, 2) # (B, T, nh, hs) --> (B, nh, T, hs), hs for head_size\n",
        "        k = k.view(B, T, self.n_heads, self.head_size).transpose(1, 2) # (B, T, nh, hs) --> (B, nh, T, hs), hs for head_size\n",
        "        v = v.view(B, T, self.n_heads, self.head_size).transpose(1, 2) # (B, T, nh, hs) --> (B, nh, T, hs), hs for head_size\n",
        "\n",
        "        # attention\n",
        "        attention = q @ k.transpose(-1, -2) # (B, nh, T, hs) @ (B, nh, hs, T) --> (B, nh, T, T)\n",
        "        attention *= self.head_size ** -0.5 # scaled dot product attention\n",
        "        if self.talking_heads:\n",
        "            attention = attention.permute(0, 2, 3, 1) # (B, nh, T, T) --> (B, T, T, nh); (B, nh, T_q, T_k) --> (B, T_q, T_k, nh) when query and key have different length\n",
        "            attention = self.w_talking_weights(attention) # (B, T, T, nh) @ (nh, nh) --> (B, T, T, nh)\n",
        "            attention = attention.permute(0, 3, 1, 2) # (B, T, T, nh) --> (B, nh, T, T)\n",
        "        attention = attention.masked_fill(self.mask[:, :, :T, :T] == 0, float('-inf'))\n",
        "        attention = F.softmax(attention, dim=-1)\n",
        "        if self.talking_heads:\n",
        "            attention = attention.permute(0, 2, 3, 1)  # (B, nh, T, T) --> (B, T, T, nh)\n",
        "            attention = self.w_talking_logits(attention) # (B, T, T, nh) @ (nh, nh) --> (B, T, T, nh)\n",
        "            attention = attention.permute(0, 3, 1, 2) # (B, T, T, nh) --> (B, nh, T, T)\n",
        "        attention = self.attention_dropout(attention)\n",
        "\n",
        "        # output\n",
        "        y = attention @ v # (B, nh, T, T) @ (B, nh, T, hs) --> (B, nh, T, hs)\n",
        "        y = y.transpose(1, 2).contiguous().view(B, T, D) # (B, nh, T, hs) --> (B, T, nh, hs) --> (B, T, D)\n",
        "        y = self.w_o(y) # (B, T, D) @ (D, D) --> (B, T, D)\n",
        "        y = self.residual_dropout(y)\n",
        "\n",
        "        return y"
      ],
      "metadata": {
        "id": "k3lQMWKcMETf"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.rand(1, Config.block_size, Config.d_model)\n",
        "print(x)\n",
        "layer = CausalSelfAttention(Config)\n",
        "print(layer(x))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mgVrfGkU3QHR",
        "outputId": "101b01d0-75d7-408d-981b-c7a00f65b7b7"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[0.3813, 0.6721, 0.2603, 0.6302, 0.2283, 0.3321, 0.0882, 0.1664,\n",
            "          0.2291, 0.5071, 0.8659, 0.1470, 0.1494, 0.0381, 0.5740, 0.2529],\n",
            "         [0.3449, 0.9917, 0.6983, 0.1112, 0.0900, 0.0011, 0.0673, 0.5607,\n",
            "          0.7021, 0.1708, 0.8078, 0.9408, 0.2962, 0.9869, 0.4653, 0.2010],\n",
            "         [0.6817, 0.4621, 0.2949, 0.4915, 0.9488, 0.6643, 0.8228, 0.3356,\n",
            "          0.9932, 0.5010, 0.5966, 0.1304, 0.2402, 0.8760, 0.5761, 0.6892],\n",
            "         [0.5934, 0.0857, 0.3116, 0.7343, 0.2737, 0.7082, 0.7385, 0.3015,\n",
            "          0.6424, 0.8895, 0.7715, 0.2097, 0.6284, 0.7914, 0.7759, 0.0373],\n",
            "         [0.9506, 0.1744, 0.7485, 0.9021, 0.7730, 0.3930, 0.5014, 0.2275,\n",
            "          0.1787, 0.6184, 0.5131, 0.0772, 0.2575, 0.4504, 0.1519, 0.9919],\n",
            "         [0.9278, 0.5570, 0.5830, 0.9215, 0.5986, 0.2957, 0.8409, 0.5637,\n",
            "          0.7597, 0.0286, 0.2481, 0.5496, 0.9791, 0.4818, 0.2578, 0.9870],\n",
            "         [0.8867, 0.8018, 0.6264, 0.6848, 0.3668, 0.9750, 0.4369, 0.2738,\n",
            "          0.5364, 0.0901, 0.5150, 0.4216, 0.0146, 0.4573, 0.2406, 0.2189],\n",
            "         [0.0794, 0.3033, 0.3535, 0.1117, 0.4001, 0.0584, 0.0970, 0.3857,\n",
            "          0.2837, 0.3838, 0.3005, 0.6019, 0.6721, 0.6087, 0.8271, 0.2449]]])\n",
            "tensor([[[ 0.1566,  0.2597, -0.1255,  0.5075,  0.0934,  0.4435, -0.6405,\n",
            "          -0.0073, -0.3818,  0.3221, -1.0635,  0.1825,  0.3751, -0.0871,\n",
            "           0.7212,  0.4482],\n",
            "         [ 0.1705,  0.2336, -0.1640,  0.5214,  0.0925,  0.4422, -0.6978,\n",
            "          -0.0421, -0.3700,  0.3573, -1.1248,  0.1655,  0.4205, -0.0820,\n",
            "           0.7655,  0.4988],\n",
            "         [ 0.1400,  0.2480, -0.1465,  0.5003,  0.1027,  0.4363, -0.7109,\n",
            "          -0.0296, -0.4008,  0.3653, -1.1365,  0.1659,  0.4053, -0.0909,\n",
            "           0.7999,  0.4760],\n",
            "         [ 0.1300,  0.2366, -0.1116,  0.5097,  0.0854,  0.4432, -0.6957,\n",
            "           0.0056, -0.4061,  0.3742, -1.1280,  0.1497,  0.4064, -0.1122,\n",
            "           0.7997,  0.4734],\n",
            "         [ 0.1282,  0.2518, -0.1038,  0.5027,  0.0896,  0.4423, -0.6895,\n",
            "           0.0120, -0.4238,  0.3759, -1.1262,  0.1495,  0.4005, -0.1111,\n",
            "           0.8042,  0.4675],\n",
            "         [ 0.1349,  0.2622, -0.0939,  0.5100,  0.0891,  0.4637, -0.6911,\n",
            "           0.0098, -0.4284,  0.4030, -1.1441,  0.1365,  0.4139, -0.1248,\n",
            "           0.8314,  0.4831],\n",
            "         [ 0.1279,  0.2583, -0.0928,  0.5087,  0.0888,  0.4670, -0.6921,\n",
            "           0.0096, -0.4267,  0.3989, -1.1415,  0.1340,  0.4135, -0.1368,\n",
            "           0.8353,  0.4842],\n",
            "         [ 0.1272,  0.2535, -0.0970,  0.5034,  0.0884,  0.4566, -0.6918,\n",
            "           0.0111, -0.4265,  0.3901, -1.1325,  0.1362,  0.4121, -0.1350,\n",
            "           0.8242,  0.4767]]], grad_fn=<ViewBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "so7Wjm353QSZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# v3: GQA, MQA, MLA"
      ],
      "metadata": {
        "id": "I3zZQz2G3RQP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiQueryAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    Multi-Query Attention: Uses single key and value heads shared across all query heads.\n",
        "    \"\"\"\n",
        "    def __init__(self, config, talking_heads=True) -> None:\n",
        "        super().__init__()\n",
        "        assert config.d_model % config.n_heads == 0\n",
        "\n",
        "        self.n_heads = config.n_heads\n",
        "        self.d_model = config.d_model\n",
        "        self.head_size = config.d_model // config.n_heads\n",
        "        self.block_size = config.block_size\n",
        "\n",
        "        self.attention_dropout = nn.Dropout(config.dropout_rate) # after softmax\n",
        "        self.residual_dropout = nn.Dropout(config.dropout_rate) # after attention block, before adding with residual connection\n",
        "\n",
        "        # MODIFIED: Separate projections - Q has n_heads, K&V have only 1 head each\n",
        "        self.w_q = nn.Linear(self.d_model, self.d_model)\n",
        "        self.w_k = nn.Linear(self.d_model, self.head_size)\n",
        "        self.w_v = nn.Linear(self.d_model, self.head_size)\n",
        "        self.w_o = nn.Linear(self.d_model, self.d_model)\n",
        "\n",
        "        # talking heads: 引入两个小的(n_heads, n_heads)矩阵, 在immediately before and after softmax, 进行head之间的线性变换\n",
        "        # 2 learnable linear transformations, process (1) attention scores (right after Q*V and scaling, before masking),\n",
        "        #   (2) logits (attention logits, after masking, optional padding, softmax, before weighted summing keys)\n",
        "        self.talking_heads = talking_heads\n",
        "        if self.talking_heads:\n",
        "            self.w_talking_weights = nn.Linear(self.n_heads, self.n_heads)\n",
        "            self.w_talking_logits = nn.Linear(self.n_heads, self.n_heads)\n",
        "\n",
        "        self.register_buffer(\"mask\", torch.tril(torch.ones(self.block_size, self.block_size)) # register buffer for low triangular matrix mask\n",
        "                                    .view(1, 1, self.block_size, self.block_size))  # reshape for (B, n_head, T, T) inputs\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, D = x.size() # batch size, sequence length, d_model\n",
        "\n",
        "        # MODIFIED: Separate Q, K, V projections\n",
        "        q = self.w_q(x).view(B, T, self.n_heads, self.head_size).transpose(1, 2) # (B, T, D) --> (B, T, nh, hs) --> (B, nh, T, hs)\n",
        "        k = self.w_k(x).view(B, T, 1, self.head_size).transpose(1, 2) # (B, T, hs) --> (B, T, 1, hs) --> (B, 1, T, hs)\n",
        "        v = self.w_v(x).view(B, T, 1, self.head_size).transpose(1, 2) # (B, T, hs) --> (B, T, 1, hs) --> (B, 1, T, hs)\n",
        "\n",
        "        # MODIFIED: Expand K&V to match Q's head dimension for broadcasting\n",
        "        # expansion is not a must, but will make broadcasting more clear\n",
        "        k = k.expand(B, self.n_heads, T, self.head_size) # (B, 1, T, hs) --> (B, nh, T, hs)\n",
        "        v = v.expand(B, self.n_heads, T, self.head_size) # (B, 1, T, hs) --> (B, nh, T, hs)\n",
        "\n",
        "        # attention\n",
        "        attention = q @ k.transpose(-1, -2) # (B, nh, T, hs) @ (B, nh, hs, T) --> (B, nh, T, T)\n",
        "        attention *= self.head_size ** -0.5 # scaled dot product attention\n",
        "        if self.talking_heads:\n",
        "            attention = attention.permute(0, 2, 3, 1) # (B, nh, T, T) --> (B, T, T, nh); (B, nh, T_q, T_k) --> (B, T_q, T_k, nh) when query and key have different length\n",
        "            attention = self.w_talking_weights(attention) # (B, T, T, nh) @ (nh, nh) --> (B, T, T, nh)\n",
        "            attention = attention.permute(0, 3, 1, 2) # (B, T, T, nh) --> (B, nh, T, T)\n",
        "        attention = attention.masked_fill(self.mask[:, :, :T, :T] == 0, float('-inf'))\n",
        "        attention = F.softmax(attention, dim=-1)\n",
        "        if self.talking_heads:\n",
        "            attention = attention.permute(0, 2, 3, 1)  # (B, nh, T, T) --> (B, T, T, nh)\n",
        "            attention = self.w_talking_logits(attention) # (B, T, T, nh) @ (nh, nh) --> (B, T, T, nh)\n",
        "            attention = attention.permute(0, 3, 1, 2) # (B, T, T, nh) --> (B, nh, T, T)\n",
        "        attention = self.attention_dropout(attention)\n",
        "\n",
        "        # output\n",
        "        y = attention @ v # (B, nh, T, T) @ (B, nh, T, hs) --> (B, nh, T, hs)\n",
        "        y = y.transpose(1, 2).contiguous().view(B, T, D) # (B, nh, T, hs) --> (B, T, nh, hs) --> (B, T, D)\n",
        "        y = self.w_o(y) # (B, T, D) @ (D, D) --> (B, T, D)\n",
        "        y = self.residual_dropout(y)\n",
        "\n",
        "        return y"
      ],
      "metadata": {
        "id": "zHmA1cDh3QUW"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.rand(1, Config.block_size, Config.d_model)\n",
        "print(x)\n",
        "layer = MultiQueryAttention(Config)\n",
        "print(layer(x))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t9ALZD1oN-lV",
        "outputId": "e81bbc1a-b53e-4595-b039-15427f46b073"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[0.4533, 0.3067, 0.4422, 0.7750, 0.0076, 0.1242, 0.6993, 0.8188,\n",
            "          0.5440, 0.0225, 0.7667, 0.6029, 0.4060, 0.3658, 0.9124, 0.9450],\n",
            "         [0.7044, 0.8996, 0.0602, 0.2747, 0.9742, 0.4919, 0.2949, 0.7333,\n",
            "          0.9566, 0.0486, 0.8283, 0.9287, 0.3941, 0.3887, 0.7750, 0.2825],\n",
            "         [0.4007, 0.8477, 0.2999, 0.6903, 0.9829, 0.6201, 0.8296, 0.9280,\n",
            "          0.3081, 0.5020, 0.1918, 0.2444, 0.0215, 0.9860, 0.5557, 0.2889],\n",
            "         [0.8979, 0.9183, 0.0642, 0.1577, 0.6754, 0.8525, 0.8686, 0.2188,\n",
            "          0.6132, 0.2079, 0.6865, 0.2270, 0.3226, 0.7829, 0.2440, 0.4147],\n",
            "         [0.6328, 0.3391, 0.3541, 0.7309, 0.4263, 0.5694, 0.3471, 0.9977,\n",
            "          0.0872, 0.1692, 0.8410, 0.6959, 0.9130, 0.2954, 0.9986, 0.2347],\n",
            "         [0.2777, 0.3209, 0.3991, 0.7506, 0.2244, 0.3726, 0.0978, 0.5931,\n",
            "          0.4410, 0.2730, 0.5415, 0.2961, 0.6942, 0.1885, 0.0642, 0.1999],\n",
            "         [0.0579, 0.4482, 0.7296, 0.4288, 0.6772, 0.2503, 0.7931, 0.4139,\n",
            "          0.0030, 0.6685, 0.1500, 0.3058, 0.3896, 0.3954, 0.7533, 0.3149],\n",
            "         [0.4450, 0.5172, 0.4428, 0.7361, 0.7157, 0.9722, 0.2334, 0.3644,\n",
            "          0.2001, 0.9628, 0.8567, 0.0190, 0.9997, 0.5188, 0.9120, 0.2849]]])\n",
            "tensor([[[-0.2695, -0.0562, -0.2520,  0.1507,  0.0794, -0.0551, -0.0946,\n",
            "           0.3200,  0.1803,  0.0114,  0.1043,  0.2961,  0.1629, -0.2329,\n",
            "           0.0825, -0.0839],\n",
            "         [-0.2157, -0.0857, -0.2235,  0.2201,  0.0871, -0.1112, -0.1207,\n",
            "           0.2845,  0.0530,  0.1135,  0.0469,  0.3089,  0.2631, -0.2578,\n",
            "           0.0700, -0.0688],\n",
            "         [-0.1855, -0.0472, -0.1910,  0.2075,  0.0953, -0.0676, -0.1699,\n",
            "           0.2969, -0.0007,  0.1125,  0.0570,  0.2981,  0.3401, -0.1873,\n",
            "           0.0218,  0.0087],\n",
            "         [-0.1644, -0.0469, -0.1804,  0.2315,  0.0998, -0.0880, -0.1806,\n",
            "           0.2980, -0.0465,  0.1523,  0.0397,  0.2880,  0.3831, -0.1811,\n",
            "           0.0165,  0.0225],\n",
            "         [-0.1657, -0.0547, -0.1743,  0.2388,  0.1046, -0.0987, -0.1633,\n",
            "           0.2954, -0.0328,  0.1510,  0.0330,  0.2776,  0.3738, -0.1806,\n",
            "           0.0112,  0.0203],\n",
            "         [-0.1751, -0.0661, -0.1830,  0.2406,  0.1017, -0.1066, -0.1514,\n",
            "           0.2881, -0.0177,  0.1466,  0.0311,  0.2851,  0.3513, -0.2011,\n",
            "           0.0222, -0.0007],\n",
            "         [-0.1858, -0.0628, -0.1772,  0.2204,  0.1031, -0.0853, -0.1431,\n",
            "           0.2896,  0.0171,  0.1132,  0.0430,  0.2832,  0.3289, -0.1863,\n",
            "           0.0094,  0.0073],\n",
            "         [-0.1873, -0.0639, -0.1752,  0.2232,  0.1049, -0.0902, -0.1363,\n",
            "           0.2928,  0.0233,  0.1139,  0.0413,  0.2753,  0.3255, -0.1832,\n",
            "           0.0080,  0.0073]]], grad_fn=<ViewBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class GroupedQueryAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    Grouped Query Attention: Groups multiple query heads to share K&V heads. Compromise between MHA and MQA.\n",
        "    \"\"\"\n",
        "    def __init__(self, config, talking_heads=True) -> None:\n",
        "        super().__init__()\n",
        "        assert config.d_model % config.n_heads == 0\n",
        "\n",
        "        self.n_heads = config.n_heads\n",
        "        self.d_model = config.d_model\n",
        "        self.head_size = config.d_model // config.n_heads\n",
        "        self.block_size = config.block_size\n",
        "\n",
        "        self.attention_dropout = nn.Dropout(config.dropout_rate) # after softmax\n",
        "        self.residual_dropout = nn.Dropout(config.dropout_rate) # after attention block, before adding with residual connection\n",
        "\n",
        "        self.w_qkv = nn.Linear(self.d_model, 3 * self.d_model)\n",
        "        self.w_o = nn.Linear(self.d_model, self.d_model)\n",
        "\n",
        "        # talking heads: 引入两个小的(n_heads, n_heads)矩阵, 在immediately before and after softmax, 进行head之间的线性变换\n",
        "        # 2 learnable linear transformations, process (1) attention scores (right after Q*V and scaling, before masking),\n",
        "        #   (2) logits (attention logits, after masking, optional padding, softmax, before weighted summing keys)\n",
        "        self.talking_heads = talking_heads\n",
        "        if self.talking_heads:\n",
        "            self.w_talking_weights = nn.Linear(self.n_heads, self.n_heads)\n",
        "            self.w_talking_logits = nn.Linear(self.n_heads, self.n_heads)\n",
        "\n",
        "        self.register_buffer(\"mask\", torch.tril(torch.ones(self.block_size, self.block_size)) # register buffer for low triangular matrix mask\n",
        "                                    .view(1, 1, self.block_size, self.block_size))  # reshape for (B, n_head, T, T) inputs\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, D = x.size() # batch size, sequence length, d_model\n",
        "\n",
        "        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n",
        "        q, k, v = self.w_qkv(x).split(self.d_model, dim=2) # (B, T, D) @ (D, 3D) --> (B, T, 3D) --> split at dim=2 --> (B, T, D)\n",
        "        q = q.view(B, T, self.n_heads, self.head_size).transpose(1, 2) # (B, T, nh, hs) --> (B, nh, T, hs), hs for head_size\n",
        "        k = k.view(B, T, self.n_heads, self.head_size).transpose(1, 2) # (B, T, nh, hs) --> (B, nh, T, hs), hs for head_size\n",
        "        v = v.view(B, T, self.n_heads, self.head_size).transpose(1, 2) # (B, T, nh, hs) --> (B, nh, T, hs), hs for head_size\n",
        "\n",
        "        # attention\n",
        "        attention = q @ k.transpose(-1, -2) # (B, nh, T, hs) @ (B, nh, hs, T) --> (B, nh, T, T)\n",
        "        attention *= self.head_size ** -0.5 # scaled dot product attention\n",
        "        if self.talking_heads:\n",
        "            attention = attention.permute(0, 2, 3, 1) # (B, nh, T, T) --> (B, T, T, nh); (B, nh, T_q, T_k) --> (B, T_q, T_k, nh) when query and key have different length\n",
        "            attention = self.w_talking_weights(attention) # (B, T, T, nh) @ (nh, nh) --> (B, T, T, nh)\n",
        "            attention = attention.permute(0, 3, 1, 2) # (B, T, T, nh) --> (B, nh, T, T)\n",
        "        attention = attention.masked_fill(self.mask[:, :, :T, :T] == 0, float('-inf'))\n",
        "        attention = F.softmax(attention, dim=-1)\n",
        "        if self.talking_heads:\n",
        "            attention = attention.permute(0, 2, 3, 1)  # (B, nh, T, T) --> (B, T, T, nh)\n",
        "            attention = self.w_talking_logits(attention) # (B, T, T, nh) @ (nh, nh) --> (B, T, T, nh)\n",
        "            attention = attention.permute(0, 3, 1, 2) # (B, T, T, nh) --> (B, nh, T, T)\n",
        "        attention = self.attention_dropout(attention)\n",
        "\n",
        "        # output\n",
        "        y = attention @ v # (B, nh, T, T) @ (B, nh, T, hs) --> (B, nh, T, hs)\n",
        "        y = y.transpose(1, 2).contiguous().view(B, T, D) # (B, nh, T, hs) --> (B, T, nh, hs) --> (B, T, D)\n",
        "        y = self.w_o(y) # (B, T, D) @ (D, D) --> (B, T, D)\n",
        "        y = self.residual_dropout(y)\n",
        "\n",
        "        return y"
      ],
      "metadata": {
        "id": "Hb1rTr84Eg76"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadLatentAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    Multi-Head Latent Attention (MLA): Compresses K&V into lower-dimensional latent representations.\n",
        "    \"\"\"\n",
        "    def __init__(self, config, talking_heads=True) -> None:\n",
        "        super().__init__()\n",
        "        assert config.d_model % config.n_heads == 0\n",
        "\n",
        "        self.n_heads = config.n_heads\n",
        "        self.d_model = config.d_model\n",
        "        self.head_size = config.d_model // config.n_heads\n",
        "        self.block_size = config.block_size\n",
        "\n",
        "        self.attention_dropout = nn.Dropout(config.dropout_rate) # after softmax\n",
        "        self.residual_dropout = nn.Dropout(config.dropout_rate) # after attention block, before adding with residual connection\n",
        "\n",
        "        self.w_qkv = nn.Linear(self.d_model, 3 * self.d_model)\n",
        "        self.w_o = nn.Linear(self.d_model, self.d_model)\n",
        "\n",
        "        # talking heads: 引入两个小的(n_heads, n_heads)矩阵, 在immediately before and after softmax, 进行head之间的线性变换\n",
        "        # 2 learnable linear transformations, process (1) attention scores (right after Q*V and scaling, before masking),\n",
        "        #   (2) logits (attention logits, after masking, optional padding, softmax, before weighted summing keys)\n",
        "        self.talking_heads = talking_heads\n",
        "        if self.talking_heads:\n",
        "            self.w_talking_weights = nn.Linear(self.n_heads, self.n_heads)\n",
        "            self.w_talking_logits = nn.Linear(self.n_heads, self.n_heads)\n",
        "\n",
        "        self.register_buffer(\"mask\", torch.tril(torch.ones(self.block_size, self.block_size)) # register buffer for low triangular matrix mask\n",
        "                                    .view(1, 1, self.block_size, self.block_size))  # reshape for (B, n_head, T, T) inputs\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, D = x.size() # batch size, sequence length, d_model\n",
        "\n",
        "        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n",
        "        q, k, v = self.w_qkv(x).split(self.d_model, dim=2) # (B, T, D) @ (D, 3D) --> (B, T, 3D) --> split at dim=2 --> (B, T, D)\n",
        "        q = q.view(B, T, self.n_heads, self.head_size).transpose(1, 2) # (B, T, nh, hs) --> (B, nh, T, hs), hs for head_size\n",
        "        k = k.view(B, T, self.n_heads, self.head_size).transpose(1, 2) # (B, T, nh, hs) --> (B, nh, T, hs), hs for head_size\n",
        "        v = v.view(B, T, self.n_heads, self.head_size).transpose(1, 2) # (B, T, nh, hs) --> (B, nh, T, hs), hs for head_size\n",
        "\n",
        "        # attention\n",
        "        attention = q @ k.transpose(-1, -2) # (B, nh, T, hs) @ (B, nh, hs, T) --> (B, nh, T, T)\n",
        "        attention *= self.head_size ** -0.5 # scaled dot product attention\n",
        "        if self.talking_heads:\n",
        "            attention = attention.permute(0, 2, 3, 1) # (B, nh, T, T) --> (B, T, T, nh); (B, nh, T_q, T_k) --> (B, T_q, T_k, nh) when query and key have different length\n",
        "            attention = self.w_talking_weights(attention) # (B, T, T, nh) @ (nh, nh) --> (B, T, T, nh)\n",
        "            attention = attention.permute(0, 3, 1, 2) # (B, T, T, nh) --> (B, nh, T, T)\n",
        "        attention = attention.masked_fill(self.mask[:, :, :T, :T] == 0, float('-inf'))\n",
        "        attention = F.softmax(attention, dim=-1)\n",
        "        if self.talking_heads:\n",
        "            attention = attention.permute(0, 2, 3, 1)  # (B, nh, T, T) --> (B, T, T, nh)\n",
        "            attention = self.w_talking_logits(attention) # (B, T, T, nh) @ (nh, nh) --> (B, T, T, nh)\n",
        "            attention = attention.permute(0, 3, 1, 2) # (B, T, T, nh) --> (B, nh, T, T)\n",
        "        attention = self.attention_dropout(attention)\n",
        "\n",
        "        # output\n",
        "        y = attention @ v # (B, nh, T, T) @ (B, nh, T, hs) --> (B, nh, T, hs)\n",
        "        y = y.transpose(1, 2).contiguous().view(B, T, D) # (B, nh, T, hs) --> (B, T, nh, hs) --> (B, T, D)\n",
        "        y = self.w_o(y) # (B, T, D) @ (D, D) --> (B, T, D)\n",
        "        y = self.residual_dropout(y)\n",
        "\n",
        "        return y"
      ],
      "metadata": {
        "id": "p9swPY8AEg_A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BpYZAHBpEhBg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GPT"
      ],
      "metadata": {
        "id": "68iySV3hEh9H"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wlRLcXEQEkYG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WsnLqeO-Es62"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aQ3A2DCjEs92"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}