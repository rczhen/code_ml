{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNe77XybFDq5A2oSxJ34d1Y",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rczhen/code_ml/blob/main/attention.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import inspect\n",
        "from dataclasses import dataclass\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F"
      ],
      "metadata": {
        "id": "bYaqKhZZCvgV"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclass\n",
        "class GPTConfig:\n",
        "    block_size: int = 1024\n",
        "    vocab_size: int = 50304 # GPT-2 vocab_size of 50257, padded up to nearest multiple of 64 for efficiency\n",
        "    n_layer: int = 12\n",
        "    n_head: int = 12\n",
        "    n_embd: int = 768\n",
        "    dropout: float = 0.0\n",
        "    bias: bool = True # True: bias in Linears and LayerNorms, like GPT-2. False: a bit better and faster\n",
        "\n",
        "@dataclass\n",
        "class Config:\n",
        "    block_size: int = 8\n",
        "    vocab_size: int = 50304 # GPT-2 vocab_size of 50257, padded up to nearest multiple of 64 for efficiency\n",
        "    n_layers: int = 2\n",
        "    n_heads: int = 4\n",
        "    n_kv_heads: int = 2\n",
        "    d_model: int = 16\n",
        "    dropout_rate: float = 0.0\n",
        "    bias: bool = True # True: bias in Linears and LayerNorms, like GPT-2. False: a bit better and faster"
      ],
      "metadata": {
        "id": "aTdb4pojIvKR"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# v1: standard CausalSelfAttention"
      ],
      "metadata": {
        "id": "losC6CqJ2IAD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CausalSelfAttention(nn.Module):\n",
        "    def __init__(self, config) -> None:\n",
        "        super().__init__()\n",
        "        assert config.d_model % config.n_heads == 0\n",
        "\n",
        "        self.n_heads = config.n_heads\n",
        "        self.d_model = config.d_model\n",
        "        self.head_size = config.d_model // config.n_heads\n",
        "        self.block_size = config.block_size\n",
        "\n",
        "        self.attention_dropout = nn.Dropout(config.dropout_rate) # after softmax\n",
        "        self.residual_dropout = nn.Dropout(config.dropout_rate) # after attention block, before adding with residual connection\n",
        "\n",
        "        self.w_qkv = nn.Linear(self.d_model, 3 * self.d_model)\n",
        "        self.w_o = nn.Linear(self.d_model, self.d_model)\n",
        "\n",
        "        self.register_buffer(\"mask\", torch.tril(torch.ones(self.block_size, self.block_size)) # register buffer for low triangular matrix mask\n",
        "                                    .view(1, 1, self.block_size, self.block_size))  # reshape for (B, n_head, T, T) inputs\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, D = x.size() # batch size, sequence length, d_model\n",
        "\n",
        "        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n",
        "        q, k, v = self.w_qkv(x).split(self.d_model, dim=2) # (B, T, D) @ (D, 3D) --> (B, T, 3D) --> split at dim=2 --> (B, T, D)\n",
        "        q = q.view(B, T, self.n_heads, self.head_size).transpose(1, 2) # (B, T, nh, hs) --> (B, nh, T, hs), hs for head_size\n",
        "        k = k.view(B, T, self.n_heads, self.head_size).transpose(1, 2) # (B, T, nh, hs) --> (B, nh, T, hs), hs for head_size\n",
        "        v = v.view(B, T, self.n_heads, self.head_size).transpose(1, 2) # (B, T, nh, hs) --> (B, nh, T, hs), hs for head_size\n",
        "\n",
        "        # attention\n",
        "        attention = q @ k.transpose(-1, -2) # (B, nh, T, hs) @ (B, nh, hs, T) --> (B, nh, T, T)\n",
        "        attention *= self.head_size ** -0.5 # scaled dot product attention\n",
        "        attention = attention.masked_fill(self.mask[:, :, :T, :T] == 0, float('-inf'))\n",
        "        attention = F.softmax(attention, dim=-1)\n",
        "        attention = self.attention_dropout(attention)\n",
        "\n",
        "        # output\n",
        "        y = attention @ v # (B, nh, T, T) @ (B, nh, T, hs) --> (B, nh, T, hs)\n",
        "        y = y.transpose(1, 2).contiguous().view(B, T, D) # (B, nh, T, hs) --> (B, T, nh, hs) --> (B, T, D)\n",
        "        y = self.w_o(y) # (B, T, D) @ (D, D) --> (B, T, D)\n",
        "        y = self.residual_dropout(y)\n",
        "\n",
        "        return y\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "why scaled?\n",
        "if no normalization, the variance of weights wil be on the order of head_size, here is 16\n",
        "when deviding by sqrt(head_size), bring the variance back\n",
        "\"Scaled\" attention additional divides attention scores by 1/sqrt(head_size).\n",
        "so when input Q,K are unit variance, attentions will be unit variance too\n",
        "and Softmax will stay diffuse and not saturate too much.\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "id": "VCpKzgz7JaH3",
        "outputId": "276c0323-5c68-4e79-8b62-8245e286274e"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nwhy scaled? \\nif no normalization, the variance of weights wil be on the order of head_size, here is 16\\nwhen deviding by sqrt(head_size), bring the variance back\\n\"Scaled\" attention additional divides attention scores by 1/sqrt(head_size). \\nso when input Q,K are unit variance, attentions will be unit variance too\\nand Softmax will stay diffuse and not saturate too much.\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.rand(1, Config.block_size, Config.d_model)\n",
        "print(x)\n",
        "layer = CausalSelfAttention(Config)\n",
        "print(layer(x))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3_tilA7bPIQ3",
        "outputId": "29590bd5-d016-47af-dd87-8191b69d3728"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[0.4969, 0.6570, 0.5957, 0.2806, 0.9898, 0.4197, 0.0525, 0.3336,\n",
            "          0.1488, 0.0221, 0.7811, 0.7002, 0.5456, 0.2738, 0.3161, 0.4923],\n",
            "         [0.3783, 0.2664, 0.5428, 0.1738, 0.7641, 0.9601, 0.5239, 0.8026,\n",
            "          0.0244, 0.0028, 0.8761, 0.8630, 0.2437, 0.8981, 0.1249, 0.6858],\n",
            "         [0.3772, 0.4514, 0.9887, 0.0030, 0.9620, 0.2002, 0.7762, 0.4336,\n",
            "          0.9140, 0.2241, 0.4045, 0.1784, 0.4262, 0.4291, 0.9030, 0.4159],\n",
            "         [0.1943, 0.2074, 0.1744, 0.0545, 0.1915, 0.9909, 0.2459, 0.7920,\n",
            "          0.9644, 0.3800, 0.5882, 0.2471, 0.3493, 0.0587, 0.8093, 0.4968],\n",
            "         [0.0784, 0.3818, 0.0194, 0.9578, 0.2897, 0.9427, 0.2215, 0.1784,\n",
            "          0.7407, 0.5388, 0.2052, 0.3051, 0.4202, 0.8896, 0.3862, 0.0851],\n",
            "         [0.0419, 0.6332, 0.2759, 0.0148, 0.9401, 0.9824, 0.6808, 0.8424,\n",
            "          0.5608, 0.0333, 0.7411, 0.7728, 0.7805, 0.3241, 0.6716, 0.4933],\n",
            "         [0.4805, 0.7104, 0.3978, 0.9024, 0.8240, 0.5586, 0.9696, 0.4489,\n",
            "          0.0664, 0.7289, 0.3661, 0.7462, 0.3376, 0.6270, 0.4117, 0.3921],\n",
            "         [0.7109, 0.2322, 0.3283, 0.5392, 0.7949, 0.0172, 0.9258, 0.6422,\n",
            "          0.9443, 0.9910, 0.7000, 0.0765, 0.4967, 0.2691, 0.8618, 0.9297]]])\n",
            "tensor([[[-0.1100,  0.0807,  0.1508,  0.3218,  0.0775,  0.0779,  0.1641,\n",
            "           0.2782, -0.5003, -0.4013,  0.2174,  0.2017, -0.1648,  0.0448,\n",
            "          -0.1387,  0.0348],\n",
            "         [-0.1112,  0.0699,  0.1366,  0.3985,  0.0657,  0.0996,  0.1819,\n",
            "           0.2839, -0.4629, -0.4133,  0.2471,  0.1977, -0.1449,  0.1409,\n",
            "          -0.2196,  0.0904],\n",
            "         [-0.0993,  0.0809,  0.1730,  0.3932,  0.0663,  0.0785,  0.1404,\n",
            "           0.3059, -0.4514, -0.4071,  0.2065,  0.2219, -0.1833,  0.0641,\n",
            "          -0.1665,  0.1019],\n",
            "         [-0.0687,  0.1409,  0.1442,  0.3983,  0.0575,  0.0556,  0.1868,\n",
            "           0.2857, -0.4120, -0.3402,  0.1791,  0.1845, -0.1508,  0.0838,\n",
            "          -0.1290,  0.1090],\n",
            "         [-0.0331,  0.1690,  0.1636,  0.3287,  0.0656,  0.0378,  0.1738,\n",
            "           0.2790, -0.4102, -0.3365,  0.1103,  0.1666, -0.1350,  0.0770,\n",
            "          -0.1009,  0.1038],\n",
            "         [-0.0323,  0.1545,  0.1676,  0.3482,  0.0703,  0.0515,  0.1881,\n",
            "           0.2864, -0.4236, -0.3241,  0.1374,  0.1669, -0.1125,  0.0892,\n",
            "          -0.1211,  0.0822],\n",
            "         [-0.0413,  0.1211,  0.1806,  0.3443,  0.0660,  0.0534,  0.1716,\n",
            "           0.2893, -0.4284, -0.3576,  0.1506,  0.1880, -0.1224,  0.0974,\n",
            "          -0.1394,  0.0797],\n",
            "         [-0.0453,  0.1544,  0.1774,  0.3493,  0.0710,  0.0382,  0.1588,\n",
            "           0.2958, -0.4070, -0.3410,  0.1369,  0.1881, -0.1358,  0.0947,\n",
            "          -0.1156,  0.0984]]], grad_fn=<ViewBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# v2: Talking Heads Attention"
      ],
      "metadata": {
        "id": "Db73Nty82UX-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CausalSelfAttention(nn.Module):\n",
        "    def __init__(self, config, talking_heads=True) -> None:\n",
        "        super().__init__()\n",
        "        assert config.d_model % config.n_heads == 0\n",
        "\n",
        "        self.n_heads = config.n_heads\n",
        "        self.d_model = config.d_model\n",
        "        self.head_size = config.d_model // config.n_heads\n",
        "        self.block_size = config.block_size\n",
        "\n",
        "        self.attention_dropout = nn.Dropout(config.dropout_rate) # after softmax\n",
        "        self.residual_dropout = nn.Dropout(config.dropout_rate) # after attention block, before adding with residual connection\n",
        "\n",
        "        self.w_qkv = nn.Linear(self.d_model, 3 * self.d_model)\n",
        "        self.w_o = nn.Linear(self.d_model, self.d_model)\n",
        "\n",
        "        # talking heads: 引入两个小的(n_heads, n_heads)矩阵, 在immediately before and after softmax, 进行head之间的线性变换\n",
        "        # 2 learnable linear transformations, process (1) attention scores (right after Q*V and scaling, before masking),\n",
        "        #   (2) logits (attention logits, after masking, optional padding, softmax, before weighted summing keys)\n",
        "        self.talking_heads = talking_heads\n",
        "        if self.talking_heads:\n",
        "            self.w_talking_weights = nn.Linear(self.n_heads, self.n_heads)\n",
        "            self.w_talking_logits = nn.Linear(self.n_heads, self.n_heads)\n",
        "\n",
        "        self.register_buffer(\"mask\", torch.tril(torch.ones(self.block_size, self.block_size)) # register buffer for low triangular matrix mask\n",
        "                                    .view(1, 1, self.block_size, self.block_size))  # reshape for (B, n_head, T, T) inputs\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, D = x.size() # batch size, sequence length, d_model\n",
        "\n",
        "        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n",
        "        q, k, v = self.w_qkv(x).split(self.d_model, dim=2) # (B, T, D) @ (D, 3D) --> (B, T, 3D) --> split at dim=2 --> (B, T, D)\n",
        "        q = q.view(B, T, self.n_heads, self.head_size).transpose(1, 2) # (B, T, nh, hs) --> (B, nh, T, hs), hs for head_size\n",
        "        k = k.view(B, T, self.n_heads, self.head_size).transpose(1, 2) # (B, T, nh, hs) --> (B, nh, T, hs), hs for head_size\n",
        "        v = v.view(B, T, self.n_heads, self.head_size).transpose(1, 2) # (B, T, nh, hs) --> (B, nh, T, hs), hs for head_size\n",
        "\n",
        "        # attention\n",
        "        attention = q @ k.transpose(-1, -2) # (B, nh, T, hs) @ (B, nh, hs, T) --> (B, nh, T, T)\n",
        "        attention *= self.head_size ** -0.5 # scaled dot product attention\n",
        "        if self.talking_heads:\n",
        "            attention = attention.permute(0, 2, 3, 1) # (B, nh, T, T) --> (B, T, T, nh); (B, nh, T_q, T_k) --> (B, T_q, T_k, nh) when query and key have different length\n",
        "            attention = self.w_talking_weights(attention) # (B, T, T, nh) @ (nh, nh) --> (B, T, T, nh)\n",
        "            attention = attention.permute(0, 3, 1, 2) # (B, T, T, nh) --> (B, nh, T, T)\n",
        "        attention = attention.masked_fill(self.mask[:, :, :T, :T] == 0, float('-inf'))\n",
        "        attention = F.softmax(attention, dim=-1)\n",
        "        if self.talking_heads:\n",
        "            attention = attention.permute(0, 2, 3, 1)  # (B, nh, T, T) --> (B, T, T, nh)\n",
        "            attention = self.w_talking_logits(attention) # (B, T, T, nh) @ (nh, nh) --> (B, T, T, nh)\n",
        "            attention = attention.permute(0, 3, 1, 2) # (B, T, T, nh) --> (B, nh, T, T)\n",
        "        attention = self.attention_dropout(attention)\n",
        "\n",
        "        # output\n",
        "        y = attention @ v # (B, nh, T, T) @ (B, nh, T, hs) --> (B, nh, T, hs)\n",
        "        y = y.transpose(1, 2).contiguous().view(B, T, D) # (B, nh, T, hs) --> (B, T, nh, hs) --> (B, T, D)\n",
        "        y = self.w_o(y) # (B, T, D) @ (D, D) --> (B, T, D)\n",
        "        y = self.residual_dropout(y)\n",
        "\n",
        "        return y"
      ],
      "metadata": {
        "id": "k3lQMWKcMETf"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.rand(1, Config.block_size, Config.d_model)\n",
        "print(x)\n",
        "layer = CausalSelfAttention(Config)\n",
        "print(layer(x))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mgVrfGkU3QHR",
        "outputId": "4933d118-a578-484d-a8f0-01adca01b6d1"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[4.7509e-01, 5.5477e-01, 3.4897e-01, 2.4837e-01, 6.4799e-01,\n",
            "          5.6900e-01, 1.4102e-01, 6.8804e-01, 1.6640e-01, 9.2718e-01,\n",
            "          1.6572e-01, 6.8809e-01, 8.1066e-02, 9.5104e-03, 4.3837e-01,\n",
            "          7.3487e-01],\n",
            "         [6.1554e-01, 3.5329e-01, 7.0374e-01, 2.1320e-01, 5.9138e-01,\n",
            "          8.1152e-01, 4.6939e-01, 6.5154e-01, 5.7460e-01, 3.1282e-01,\n",
            "          9.4076e-01, 5.3829e-01, 1.0570e-01, 9.5753e-01, 8.8933e-01,\n",
            "          4.8705e-01],\n",
            "         [6.1586e-01, 7.4315e-02, 7.2140e-01, 8.6935e-01, 8.8169e-01,\n",
            "          8.5381e-01, 3.9540e-02, 8.6814e-01, 3.5598e-01, 4.8922e-01,\n",
            "          4.8109e-01, 5.6206e-01, 1.2296e-02, 2.8425e-01, 1.8736e-01,\n",
            "          8.2965e-01],\n",
            "         [1.5933e-01, 1.6134e-01, 8.9811e-01, 7.9967e-01, 2.2775e-01,\n",
            "          7.9299e-01, 9.1371e-01, 6.0428e-01, 4.6222e-01, 8.8225e-01,\n",
            "          3.0300e-01, 4.2599e-01, 4.6615e-01, 6.6094e-01, 7.5380e-01,\n",
            "          2.0109e-01],\n",
            "         [8.9163e-04, 4.6125e-01, 4.9539e-01, 1.6191e-01, 8.1248e-01,\n",
            "          9.8837e-01, 5.7222e-01, 8.5359e-01, 5.7491e-01, 1.9625e-01,\n",
            "          5.5019e-01, 9.1187e-01, 6.8706e-01, 5.0168e-01, 5.2781e-01,\n",
            "          5.2900e-01],\n",
            "         [7.8873e-01, 5.2198e-01, 6.1198e-01, 7.7925e-01, 3.1628e-01,\n",
            "          5.5705e-01, 3.8739e-01, 2.9148e-01, 9.7060e-01, 4.8569e-02,\n",
            "          5.1583e-01, 7.4695e-01, 8.9976e-02, 1.7377e-02, 3.5782e-01,\n",
            "          7.8804e-01],\n",
            "         [5.0770e-01, 6.8717e-01, 9.9161e-01, 9.6259e-01, 3.9103e-01,\n",
            "          3.5795e-01, 1.8441e-01, 7.0400e-01, 6.7527e-01, 7.5184e-01,\n",
            "          2.6749e-01, 1.3129e-01, 9.2867e-01, 5.0179e-02, 5.7937e-01,\n",
            "          1.4036e-01],\n",
            "         [7.7915e-02, 1.5576e-01, 8.4923e-01, 7.3446e-01, 3.2355e-01,\n",
            "          6.1283e-01, 1.4971e-01, 1.7201e-01, 2.9322e-01, 3.6786e-02,\n",
            "          4.9070e-01, 2.2058e-01, 2.6100e-01, 2.1943e-01, 7.9893e-01,\n",
            "          5.6603e-01]]])\n",
            "tensor([[[ 0.1262, -0.2437,  0.1339, -0.9476, -0.7929, -1.1492, -0.8550,\n",
            "          -0.2178,  0.9413, -0.0663, -0.1229,  0.5069,  0.7883, -0.5781,\n",
            "          -0.6013,  0.3773],\n",
            "         [ 0.0804, -0.2599,  0.0976, -0.9104, -0.7815, -1.1035, -0.8518,\n",
            "          -0.2030,  0.9306, -0.0668, -0.0923,  0.5176,  0.7481, -0.5299,\n",
            "          -0.5704,  0.3367],\n",
            "         [ 0.0862, -0.2690,  0.0939, -0.9149, -0.7700, -1.1289, -0.8595,\n",
            "          -0.1710,  0.9403, -0.0759, -0.1143,  0.5281,  0.7464, -0.5496,\n",
            "          -0.5590,  0.3205],\n",
            "         [ 0.0930, -0.2466,  0.0745, -0.9042, -0.7547, -1.1159, -0.8680,\n",
            "          -0.1929,  0.9224, -0.0572, -0.1057,  0.5154,  0.7398, -0.5388,\n",
            "          -0.5511,  0.3218],\n",
            "         [ 0.0750, -0.2520,  0.0766, -0.8912, -0.7600, -1.1358, -0.8698,\n",
            "          -0.1829,  0.9336, -0.0648, -0.1000,  0.5412,  0.7417, -0.5404,\n",
            "          -0.5638,  0.3140],\n",
            "         [ 0.0793, -0.2577,  0.0897, -0.8861, -0.7691, -1.1182, -0.8510,\n",
            "          -0.1979,  0.9327, -0.0661, -0.0947,  0.5445,  0.7389, -0.5252,\n",
            "          -0.5586,  0.3212],\n",
            "         [ 0.0838, -0.2624,  0.1077, -0.8864, -0.7877, -1.1230, -0.8440,\n",
            "          -0.1949,  0.9387, -0.0651, -0.0982,  0.5445,  0.7530, -0.5263,\n",
            "          -0.5690,  0.3225],\n",
            "         [ 0.0792, -0.2617,  0.1031, -0.8925, -0.7815, -1.1180, -0.8482,\n",
            "          -0.2061,  0.9385, -0.0651, -0.0935,  0.5424,  0.7488, -0.5222,\n",
            "          -0.5681,  0.3290]]], grad_fn=<ViewBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "so7Wjm353QSZ"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# v3: GQA, MQA, MLA"
      ],
      "metadata": {
        "id": "I3zZQz2G3RQP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiQueryAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    Multi-Query Attention: Uses single key and value heads shared across all query heads.\n",
        "    \"\"\"\n",
        "    def __init__(self, config, talking_heads=True) -> None:\n",
        "        super().__init__()\n",
        "        assert config.d_model % config.n_heads == 0\n",
        "\n",
        "        self.n_heads = config.n_heads\n",
        "        self.d_model = config.d_model\n",
        "        self.head_size = config.d_model // config.n_heads\n",
        "        self.block_size = config.block_size\n",
        "\n",
        "        self.attention_dropout = nn.Dropout(config.dropout_rate) # after softmax\n",
        "        self.residual_dropout = nn.Dropout(config.dropout_rate) # after attention block, before adding with residual connection\n",
        "\n",
        "        # MODIFIED: Separate projections - Q has n_heads, K&V have only 1 head each\n",
        "        self.w_q = nn.Linear(self.d_model, self.d_model)\n",
        "        self.w_k = nn.Linear(self.d_model, self.head_size)\n",
        "        self.w_v = nn.Linear(self.d_model, self.head_size)\n",
        "        self.w_o = nn.Linear(self.d_model, self.d_model)\n",
        "\n",
        "        # talking heads: 引入两个小的(n_heads, n_heads)矩阵, 在immediately before and after softmax, 进行head之间的线性变换\n",
        "        # 2 learnable linear transformations, process (1) attention scores (right after Q*V and scaling, before masking),\n",
        "        #   (2) logits (attention logits, after masking, optional padding, softmax, before weighted summing keys)\n",
        "        self.talking_heads = talking_heads\n",
        "        if self.talking_heads:\n",
        "            self.w_talking_weights = nn.Linear(self.n_heads, self.n_heads)\n",
        "            self.w_talking_logits = nn.Linear(self.n_heads, self.n_heads)\n",
        "\n",
        "        self.register_buffer(\"mask\", torch.tril(torch.ones(self.block_size, self.block_size)) # register buffer for low triangular matrix mask\n",
        "                                    .view(1, 1, self.block_size, self.block_size))  # reshape for (B, n_head, T, T) inputs\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, D = x.size() # batch size, sequence length, d_model\n",
        "\n",
        "        # MODIFIED: Separate Q, K, V projections\n",
        "        q = self.w_q(x).view(B, T, self.n_heads, self.head_size).transpose(1, 2) # (B, T, D) --> (B, T, nh, hs) --> (B, nh, T, hs)\n",
        "        k = self.w_k(x).view(B, T, 1, self.head_size).transpose(1, 2) # (B, T, hs) --> (B, T, 1, hs) --> (B, 1, T, hs)\n",
        "        v = self.w_v(x).view(B, T, 1, self.head_size).transpose(1, 2) # (B, T, hs) --> (B, T, 1, hs) --> (B, 1, T, hs)\n",
        "\n",
        "        # MODIFIED: Expand K&V to match Q's head dimension for broadcasting\n",
        "        # expansion is not a must, but will make broadcasting more clear\n",
        "        k = k.expand(B, self.n_heads, T, self.head_size) # (B, 1, T, hs) --> (B, nh, T, hs)\n",
        "        v = v.expand(B, self.n_heads, T, self.head_size) # (B, 1, T, hs) --> (B, nh, T, hs)\n",
        "\n",
        "        # attention\n",
        "        attention = q @ k.transpose(-1, -2) # (B, nh, T, hs) @ (B, nh, hs, T) --> (B, nh, T, T)\n",
        "        attention *= self.head_size ** -0.5 # scaled dot product attention\n",
        "        if self.talking_heads:\n",
        "            attention = attention.permute(0, 2, 3, 1) # (B, nh, T, T) --> (B, T, T, nh); (B, nh, T_q, T_k) --> (B, T_q, T_k, nh) when query and key have different length\n",
        "            attention = self.w_talking_weights(attention) # (B, T, T, nh) @ (nh, nh) --> (B, T, T, nh)\n",
        "            attention = attention.permute(0, 3, 1, 2) # (B, T, T, nh) --> (B, nh, T, T)\n",
        "        attention = attention.masked_fill(self.mask[:, :, :T, :T] == 0, float('-inf'))\n",
        "        attention = F.softmax(attention, dim=-1)\n",
        "        if self.talking_heads:\n",
        "            attention = attention.permute(0, 2, 3, 1)  # (B, nh, T, T) --> (B, T, T, nh)\n",
        "            attention = self.w_talking_logits(attention) # (B, T, T, nh) @ (nh, nh) --> (B, T, T, nh)\n",
        "            attention = attention.permute(0, 3, 1, 2) # (B, T, T, nh) --> (B, nh, T, T)\n",
        "        attention = self.attention_dropout(attention)\n",
        "\n",
        "        # output\n",
        "        y = attention @ v # (B, nh, T, T) @ (B, nh, T, hs) --> (B, nh, T, hs)\n",
        "        y = y.transpose(1, 2).contiguous().view(B, T, D) # (B, nh, T, hs) --> (B, T, nh, hs) --> (B, T, D)\n",
        "        y = self.w_o(y) # (B, T, D) @ (D, D) --> (B, T, D)\n",
        "        y = self.residual_dropout(y)\n",
        "\n",
        "        return y"
      ],
      "metadata": {
        "id": "zHmA1cDh3QUW"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.rand(1, Config.block_size, Config.d_model)\n",
        "print(x)\n",
        "layer = MultiQueryAttention(Config)\n",
        "print(layer(x))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t9ALZD1oN-lV",
        "outputId": "200b1fea-1d8c-461f-9613-3a6e7331b460"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[1.4342e-01, 5.5843e-01, 9.4697e-01, 7.6076e-01, 2.7843e-01,\n",
            "          2.4228e-02, 9.6862e-01, 3.9223e-01, 3.7804e-01, 3.9904e-01,\n",
            "          3.2556e-02, 2.4766e-01, 6.2905e-01, 7.6078e-01, 6.8240e-01,\n",
            "          6.8771e-01],\n",
            "         [9.9645e-01, 2.3885e-01, 5.8239e-01, 6.3197e-01, 2.3005e-01,\n",
            "          3.3318e-01, 3.1115e-01, 7.3059e-01, 9.8998e-01, 7.9331e-01,\n",
            "          8.2870e-01, 1.4672e-01, 2.8890e-01, 7.9187e-02, 3.8313e-01,\n",
            "          4.1179e-02],\n",
            "         [3.2908e-01, 7.6473e-01, 4.6602e-01, 9.8476e-01, 2.4817e-01,\n",
            "          6.2331e-01, 1.1480e-01, 4.7744e-01, 4.6600e-01, 7.3312e-01,\n",
            "          2.9563e-02, 1.0326e-01, 6.4935e-01, 3.7376e-01, 1.6241e-01,\n",
            "          6.3827e-01],\n",
            "         [5.1974e-01, 8.6120e-01, 1.1577e-01, 9.7562e-01, 8.7746e-01,\n",
            "          7.7222e-01, 5.5917e-01, 3.8288e-03, 8.3871e-01, 1.8970e-01,\n",
            "          3.9696e-01, 5.4650e-01, 4.1318e-01, 6.5634e-01, 9.3445e-01,\n",
            "          5.0466e-01],\n",
            "         [7.1505e-01, 8.2612e-01, 9.8310e-01, 2.3374e-02, 7.2700e-01,\n",
            "          4.5582e-01, 4.6918e-01, 1.0524e-01, 2.1586e-01, 2.9081e-01,\n",
            "          2.8155e-01, 9.4072e-01, 9.5837e-01, 6.6569e-01, 9.0264e-01,\n",
            "          3.0387e-01],\n",
            "         [6.6102e-01, 4.7940e-01, 2.4512e-01, 3.9572e-01, 5.2198e-01,\n",
            "          3.2056e-01, 3.5341e-01, 7.7601e-02, 3.4847e-02, 7.8815e-01,\n",
            "          2.5484e-01, 9.4086e-01, 4.4787e-01, 3.8350e-01, 4.3646e-01,\n",
            "          2.9131e-01],\n",
            "         [5.2114e-01, 5.0586e-01, 8.6091e-01, 9.2313e-01, 8.5172e-01,\n",
            "          2.3685e-01, 6.9916e-01, 9.5593e-01, 2.8145e-01, 9.0434e-01,\n",
            "          6.6982e-01, 6.0438e-01, 6.0572e-01, 4.0859e-01, 4.4513e-02,\n",
            "          3.7496e-01],\n",
            "         [6.4087e-01, 1.8900e-01, 4.8051e-01, 5.7709e-04, 1.0724e-01,\n",
            "          6.4317e-01, 8.8805e-01, 3.0006e-01, 7.5073e-01, 2.9651e-01,\n",
            "          9.6958e-02, 6.2723e-01, 1.0066e-01, 6.4069e-01, 3.0791e-01,\n",
            "          3.8423e-01]]])\n",
            "tensor([[[-1.3547e-01,  6.3751e-03, -4.6567e-03, -6.6932e-02, -2.0362e-01,\n",
            "          -3.9501e-02,  2.7179e-01, -1.3416e-01, -1.0436e-01,  1.7791e-01,\n",
            "           3.0804e-01, -1.7737e-02, -2.6476e-01, -9.9829e-02, -6.8280e-02,\n",
            "          -2.5563e-01],\n",
            "         [-1.4992e-01, -1.8569e-02,  7.0302e-02, -5.7142e-02, -1.8417e-01,\n",
            "           4.9778e-03,  2.0207e-01, -1.4586e-01, -1.7584e-01,  9.8744e-02,\n",
            "           2.7228e-01,  3.1094e-02, -2.0194e-01, -1.5002e-01, -5.3494e-02,\n",
            "          -2.8462e-01],\n",
            "         [-1.3304e-01,  5.0963e-04,  1.0173e-01, -2.6112e-02, -1.8541e-01,\n",
            "           9.5254e-03,  1.7194e-01, -1.4173e-01, -1.9612e-01,  7.6933e-02,\n",
            "           2.7070e-01,  2.1763e-02, -2.0966e-01, -1.4684e-01, -6.7225e-02,\n",
            "          -2.8406e-01],\n",
            "         [-1.2963e-01,  1.1199e-02,  7.4751e-02, -2.5502e-02, -1.8436e-01,\n",
            "          -1.1018e-02,  1.9629e-01, -1.3180e-01, -1.6994e-01,  1.0392e-01,\n",
            "           2.8283e-01,  7.2617e-05, -2.2729e-01, -1.3635e-01, -6.7696e-02,\n",
            "          -2.7121e-01],\n",
            "         [-1.1306e-01,  2.2663e-02,  8.3416e-02, -1.7636e-02, -2.0410e-01,\n",
            "          -1.8542e-02,  1.7793e-01, -1.4272e-01, -1.6528e-01,  1.1140e-01,\n",
            "           2.9145e-01, -1.7480e-02, -2.6056e-01, -1.1552e-01, -9.2024e-02,\n",
            "          -2.6675e-01],\n",
            "         [-1.1278e-01,  2.1623e-02,  8.9499e-02, -1.8559e-02, -1.9911e-01,\n",
            "          -2.9546e-02,  1.6554e-01, -1.4421e-01, -1.6346e-01,  1.1171e-01,\n",
            "           2.9268e-01, -2.4479e-02, -2.6365e-01, -1.2543e-01, -9.2779e-02,\n",
            "          -2.6473e-01],\n",
            "         [-1.1600e-01,  1.8175e-02,  9.2388e-02, -1.9046e-02, -1.9470e-01,\n",
            "          -2.5786e-02,  1.6524e-01, -1.4246e-01, -1.6847e-01,  1.0590e-01,\n",
            "           2.8959e-01, -1.8466e-02, -2.5443e-01, -1.3168e-01, -8.7986e-02,\n",
            "          -2.6727e-01],\n",
            "         [-1.2327e-01,  6.9769e-03,  8.6092e-02, -3.2473e-02, -1.9875e-01,\n",
            "          -1.9573e-02,  1.7099e-01, -1.4933e-01, -1.6740e-01,  1.0826e-01,\n",
            "           2.8723e-01, -8.2096e-03, -2.4873e-01, -1.3149e-01, -8.4818e-02,\n",
            "          -2.7183e-01]]], grad_fn=<ViewBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class GroupedQueryAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    Grouped Query Attention: Groups multiple query heads to share K&V heads. Compromise between MHA and MQA.\n",
        "    \"\"\"\n",
        "    def __init__(self, config, talking_heads=True) -> None:\n",
        "        super().__init__()\n",
        "        assert config.d_model % config.n_heads == 0\n",
        "\n",
        "        self.n_heads = config.n_heads\n",
        "        self.d_model = config.d_model\n",
        "        self.head_size = config.d_model // config.n_heads\n",
        "        self.block_size = config.block_size\n",
        "\n",
        "        # MODIFIED: Number of K&V heads (groups)\n",
        "        assert config.n_heads % config.n_kv_heads == 0, \"n_heads must be divisible by n_kv_heads\"\n",
        "        self.n_kv_heads = config.n_kv_heads\n",
        "        self.n_reps = self.n_heads // self.n_kv_heads\n",
        "\n",
        "        self.attention_dropout = nn.Dropout(config.dropout_rate) # after softmax\n",
        "        self.residual_dropout = nn.Dropout(config.dropout_rate) # after attention block, before adding with residual connection\n",
        "\n",
        "         # MODIFIED: Separate QKV projections\n",
        "        self.w_q = nn.Linear(self.d_model, self.d_model)\n",
        "        self.w_k = nn.Linear(self.d_model, self.head_size * self.n_kv_heads)\n",
        "        self.w_v = nn.Linear(self.d_model, self.head_size * self.n_kv_heads)\n",
        "        self.w_o = nn.Linear(self.d_model, self.d_model)\n",
        "\n",
        "        # talking heads: 引入两个小的(n_heads, n_heads)矩阵, 在immediately before and after softmax, 进行head之间的线性变换\n",
        "        # 2 learnable linear transformations, process (1) attention scores (right after Q*V and scaling, before masking),\n",
        "        #   (2) logits (attention logits, after masking, optional padding, softmax, before weighted summing keys)\n",
        "        self.talking_heads = talking_heads\n",
        "        if self.talking_heads:\n",
        "            self.w_talking_weights = nn.Linear(self.n_heads, self.n_heads)\n",
        "            self.w_talking_logits = nn.Linear(self.n_heads, self.n_heads)\n",
        "\n",
        "        self.register_buffer(\"mask\", torch.tril(torch.ones(self.block_size, self.block_size)) # register buffer for low triangular matrix mask\n",
        "                                    .view(1, 1, self.block_size, self.block_size))  # reshape for (B, n_head, T, T) inputs\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, D = x.size() # batch size, sequence length, d_model\n",
        "\n",
        "        # MODIFIED: separate QKV\n",
        "        q = self.w_q(x).view(B, T, self.n_heads, self.head_size).transpose(1, 2) # (B, T, D)@(D, D)-->(B, T, D)-->(B, T, nh, hs)-->(B, nh, T, hs)\n",
        "        k = self.w_k(x).view(B, T, self.n_kv_heads, self.head_size).transpose(1, 2) # (B, T, nh_kv*hs)-->(B, T, nh_kv, hs)-->(B, nh_kv, T, hs)\n",
        "        v = self.w_v(x).view(B, T, self.n_kv_heads, self.head_size).transpose(1, 2) # (B, T, nh_kv*hs)-->(B, T, nh_kv, hs)-->(B, nh_kv, T, hs)\n",
        "\n",
        "        # MODIFIED: Repeat K&V heads with repeat_interleave, extra memory used\n",
        "        k = torch.repeat_interleave(k, self.n_reps, dim=1) # (B, nh_kv, T, hs) --> (B, nh, T, hs)\n",
        "        v = torch.repeat_interleave(v, self.n_reps, dim=1) # (B, nh_kv, T, hs) --> (B, nh, T, hs)\n",
        "\n",
        "        # attention\n",
        "        attention = q @ k.transpose(-1, -2) # (B, nh, T, hs) @ (B, nh, hs, T) --> (B, nh, T, T)\n",
        "        attention *= self.head_size ** -0.5 # scaled dot product attention\n",
        "        if self.talking_heads:\n",
        "            attention = attention.permute(0, 2, 3, 1) # (B, nh, T, T) --> (B, T, T, nh); (B, nh, T_q, T_k) --> (B, T_q, T_k, nh) when query and key have different length\n",
        "            attention = self.w_talking_weights(attention) # (B, T, T, nh) @ (nh, nh) --> (B, T, T, nh)\n",
        "            attention = attention.permute(0, 3, 1, 2) # (B, T, T, nh) --> (B, nh, T, T)\n",
        "        attention = attention.masked_fill(self.mask[:, :, :T, :T] == 0, float('-inf'))\n",
        "        attention = F.softmax(attention, dim=-1)\n",
        "        if self.talking_heads:\n",
        "            attention = attention.permute(0, 2, 3, 1)  # (B, nh, T, T) --> (B, T, T, nh)\n",
        "            attention = self.w_talking_logits(attention) # (B, T, T, nh) @ (nh, nh) --> (B, T, T, nh)\n",
        "            attention = attention.permute(0, 3, 1, 2) # (B, T, T, nh) --> (B, nh, T, T)\n",
        "        attention = self.attention_dropout(attention)\n",
        "\n",
        "        # output\n",
        "        y = attention @ v # (B, nh, T, T) @ (B, nh, T, hs) --> (B, nh, T, hs)\n",
        "        y = y.transpose(1, 2).contiguous().view(B, T, D) # (B, nh, T, hs) --> (B, T, nh, hs) --> (B, T, D)\n",
        "        y = self.w_o(y) # (B, T, D) @ (D, D) --> (B, T, D)\n",
        "        y = self.residual_dropout(y)\n",
        "\n",
        "        return y\n"
      ],
      "metadata": {
        "id": "Hb1rTr84Eg76"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.rand(1, Config.block_size, Config.d_model)\n",
        "print(x)\n",
        "layer = GroupedQueryAttention(Config)\n",
        "print(layer(x))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CpcJytspRNHn",
        "outputId": "4551dddd-ea3d-4977-d63e-159568a91b1a"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[0.9923, 0.0274, 0.6775, 0.8857, 0.2252, 0.4074, 0.7280, 0.5103,\n",
            "          0.0937, 0.0104, 0.3573, 0.6979, 0.3146, 0.7746, 0.0900, 0.8332],\n",
            "         [0.4300, 0.5203, 0.0446, 0.5694, 0.2910, 0.3367, 0.9080, 0.8484,\n",
            "          0.3832, 0.1859, 0.5531, 0.1580, 0.7124, 0.0344, 0.5398, 0.3613],\n",
            "         [0.8035, 0.5223, 0.3308, 0.5732, 0.9738, 0.5370, 0.2289, 0.8004,\n",
            "          0.0371, 0.0939, 0.5391, 0.7671, 0.2368, 0.8424, 0.3405, 0.1614],\n",
            "         [0.7087, 0.6738, 0.9095, 0.4273, 0.9208, 0.7502, 0.5396, 0.9197,\n",
            "          0.6661, 0.5823, 0.6719, 0.8124, 0.1856, 0.2895, 0.7377, 0.8691],\n",
            "         [0.9879, 0.5011, 0.6483, 0.3833, 0.9163, 0.1785, 0.1416, 0.0341,\n",
            "          0.1233, 0.2735, 0.1725, 0.6249, 0.1403, 0.5212, 0.9444, 0.8579],\n",
            "         [0.5202, 0.8323, 0.9396, 0.9410, 0.0360, 0.9103, 0.4516, 0.3952,\n",
            "          0.7082, 0.6757, 0.8405, 0.4517, 0.3588, 0.2418, 0.3794, 0.6383],\n",
            "         [0.5471, 0.0027, 0.6618, 0.6652, 0.7158, 0.9049, 0.3120, 0.8153,\n",
            "          0.8799, 0.1996, 0.3987, 0.4794, 0.8021, 0.1568, 0.1926, 0.1403],\n",
            "         [0.5261, 0.1541, 0.3603, 0.1474, 0.6983, 0.0416, 0.2166, 0.3919,\n",
            "          0.0772, 0.0137, 0.8702, 0.4582, 0.8560, 0.4247, 0.0569, 0.8962]]])\n",
            "tensor([[[ 1.0609e+00, -2.7744e-01, -4.5490e-04, -1.8160e-01,  2.9525e-01,\n",
            "          -1.1087e-01,  4.4548e-01, -8.1822e-01,  5.5478e-01, -3.7279e-01,\n",
            "          -5.2652e-01, -5.3685e-01,  3.8475e-01,  4.2978e-01,  3.4766e-01,\n",
            "          -1.2088e+00],\n",
            "         [ 1.0047e+00, -2.7079e-01, -1.2111e-01, -2.1471e-01,  2.1474e-01,\n",
            "          -1.6851e-01,  4.1572e-01, -7.4484e-01,  6.2350e-01, -3.0011e-01,\n",
            "          -5.1869e-01, -5.6042e-01,  3.1936e-01,  4.4821e-01,  3.3378e-01,\n",
            "          -1.0784e+00],\n",
            "         [ 9.5147e-01, -2.6828e-01, -1.6755e-01, -2.3019e-01,  1.8559e-01,\n",
            "          -2.2100e-01,  4.3350e-01, -7.2102e-01,  6.4106e-01, -2.9680e-01,\n",
            "          -5.0374e-01, -5.7347e-01,  3.3902e-01,  4.7169e-01,  3.1314e-01,\n",
            "          -1.0521e+00],\n",
            "         [ 9.3924e-01, -2.6636e-01, -1.9417e-01, -2.2476e-01,  1.8828e-01,\n",
            "          -2.6321e-01,  4.5227e-01, -7.2366e-01,  6.5556e-01, -3.2159e-01,\n",
            "          -4.9961e-01, -5.7770e-01,  3.7415e-01,  4.8708e-01,  2.8424e-01,\n",
            "          -1.0647e+00],\n",
            "         [ 9.1488e-01, -2.8507e-01, -1.9872e-01, -2.1915e-01,  2.0627e-01,\n",
            "          -2.8554e-01,  4.5078e-01, -7.1897e-01,  6.5498e-01, -3.1287e-01,\n",
            "          -5.2517e-01, -5.6564e-01,  3.6830e-01,  4.8509e-01,  3.0600e-01,\n",
            "          -1.1005e+00],\n",
            "         [ 9.1997e-01, -2.8488e-01, -1.9979e-01, -2.0731e-01,  2.0351e-01,\n",
            "          -2.8044e-01,  4.5047e-01, -7.1745e-01,  6.3622e-01, -2.9690e-01,\n",
            "          -5.2508e-01, -5.5752e-01,  3.6843e-01,  4.7205e-01,  2.9320e-01,\n",
            "          -1.0998e+00],\n",
            "         [ 9.2121e-01, -2.8516e-01, -2.0646e-01, -2.1539e-01,  1.8932e-01,\n",
            "          -2.8298e-01,  4.4519e-01, -7.1728e-01,  6.4852e-01, -2.9949e-01,\n",
            "          -5.1694e-01, -5.6121e-01,  3.6676e-01,  4.8922e-01,  2.9031e-01,\n",
            "          -1.0813e+00],\n",
            "         [ 9.1888e-01, -2.9476e-01, -2.0166e-01, -2.1837e-01,  1.9394e-01,\n",
            "          -2.8385e-01,  4.3125e-01, -7.1911e-01,  6.4782e-01, -2.9745e-01,\n",
            "          -5.1908e-01, -5.5980e-01,  3.5458e-01,  5.0295e-01,  3.0014e-01,\n",
            "          -1.0809e+00]]], grad_fn=<ViewBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# uses broadcasting and reshaping to avoid memory duplication:\n",
        "# Key Changes:\n",
        "\n",
        "# Reshape Q instead of expanding K,V: Groups Q heads into (B, n_kv_heads, n_rep, T, hs)\n",
        "# Use broadcasting: K,V are unsqueezed to (B, n_kv_heads, 1, T, hs) for broadcasting\n",
        "# No memory duplication: All operations work with views and broadcasting\n",
        "\n",
        "class GroupedQueryAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    Memory-efficient Grouped Query Attention (GQA): Compromise between MHA and MQA.\n",
        "    Groups multiple query heads to share K&V heads WITHOUT using repeat_interleave.\n",
        "    \"\"\"\n",
        "    def __init__(self, config, n_kv_heads=None, talking_heads=True) -> None:\n",
        "        super().__init__()\n",
        "        assert config.d_model % config.n_heads == 0\n",
        "\n",
        "        self.n_heads = config.n_heads\n",
        "        # Number of K&V heads (groups)\n",
        "        self.n_kv_heads = n_kv_heads if n_kv_heads is not None else config.n_heads // 4\n",
        "        assert self.n_heads % self.n_kv_heads == 0, \"n_heads must be divisible by n_kv_heads\"\n",
        "\n",
        "        self.d_model = config.d_model\n",
        "        self.head_size = config.d_model // config.n_heads\n",
        "        self.block_size = config.block_size\n",
        "        # Calculate how many query heads per KV group\n",
        "        self.n_rep = self.n_heads // self.n_kv_heads\n",
        "\n",
        "        self.attention_dropout = nn.Dropout(config.dropout_rate)\n",
        "        self.residual_dropout = nn.Dropout(config.dropout_rate)\n",
        "\n",
        "        # Different sizes for Q vs K&V projections\n",
        "        self.w_q = nn.Linear(self.d_model, self.d_model)  # Query: full dimension\n",
        "        self.w_k = nn.Linear(self.d_model, self.n_kv_heads * self.head_size)  # Key: reduced\n",
        "        self.w_v = nn.Linear(self.d_model, self.n_kv_heads * self.head_size)  # Value: reduced\n",
        "        self.w_o = nn.Linear(self.d_model, self.d_model)\n",
        "\n",
        "        self.talking_heads = talking_heads\n",
        "        if self.talking_heads:\n",
        "            self.w_talking_weights = nn.Linear(self.n_heads, self.n_heads)\n",
        "            self.w_talking_logits = nn.Linear(self.n_heads, self.n_heads)\n",
        "\n",
        "        self.register_buffer(\"mask\", torch.tril(torch.ones(self.block_size, self.block_size))\n",
        "                                    .view(1, 1, self.block_size, self.block_size))\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, D = x.size()\n",
        "\n",
        "        # Project to Q, K, V\n",
        "        q = self.w_q(x)  # (B, T, D)\n",
        "        k = self.w_k(x)  # (B, T, n_kv_heads * head_size)\n",
        "        v = self.w_v(x)  # (B, T, n_kv_heads * head_size)\n",
        "\n",
        "        # Reshape Q with all heads\n",
        "        q = q.view(B, T, self.n_heads, self.head_size).transpose(1, 2)  # (B, n_heads, T, hs)\n",
        "\n",
        "        # Reshape K, V with KV heads only\n",
        "        k = k.view(B, T, self.n_kv_heads, self.head_size).transpose(1, 2)  # (B, n_kv_heads, T, hs)\n",
        "        v = v.view(B, T, self.n_kv_heads, self.head_size).transpose(1, 2)  # (B, n_kv_heads, T, hs)\n",
        "\n",
        "        # MEMORY EFFICIENT: Reshape Q to group format instead of expanding K,V\n",
        "        # Group Q heads: (B, n_heads, T, hs) -> (B, n_kv_heads, n_rep, T, hs)\n",
        "        q_grouped = q.view(B, self.n_kv_heads, self.n_rep, T, self.head_size)\n",
        "\n",
        "        # Expand K,V for broadcasting: (B, n_kv_heads, T, hs) -> (B, n_kv_heads, 1, T, hs)\n",
        "        k_expanded = k.unsqueeze(2)  # (B, n_kv_heads, 1, T, hs)\n",
        "        v_expanded = v.unsqueeze(2)  # (B, n_kv_heads, 1, T, hs)\n",
        "\n",
        "        # Compute attention scores with broadcasting\n",
        "        # (B, n_kv_heads, n_rep, T, hs) @ (B, n_kv_heads, 1, hs, T) -> (B, n_kv_heads, n_rep, T, T)\n",
        "        attention = q_grouped @ k_expanded.transpose(-1, -2)\n",
        "        attention *= self.head_size ** -0.5\n",
        "\n",
        "        # Apply talking heads if enabled (need to reshape for linear layers)\n",
        "        if self.talking_heads:\n",
        "            # Reshape back to (B, n_heads, T, T) for talking heads\n",
        "            attention = attention.view(B, self.n_heads, T, T)\n",
        "            attention = attention.permute(0, 2, 3, 1)  # (B, T, T, n_heads)\n",
        "            attention = self.w_talking_weights(attention)\n",
        "            attention = attention.permute(0, 3, 1, 2)  # (B, n_heads, T, T)\n",
        "            # Reshape back to grouped format\n",
        "            attention = attention.view(B, self.n_kv_heads, self.n_rep, T, T)\n",
        "\n",
        "        # Apply causal mask - broadcast across the group dimension\n",
        "        attention = attention.masked_fill(self.mask[:, :, :T, :T].unsqueeze(2) == 0, float('-inf'))\n",
        "        attention = F.softmax(attention, dim=-1)\n",
        "\n",
        "        # Apply talking heads to probabilities if enabled\n",
        "        if self.talking_heads:\n",
        "            # Reshape back to (B, n_heads, T, T) for talking heads\n",
        "            attention = attention.view(B, self.n_heads, T, T)\n",
        "            attention = attention.permute(0, 2, 3, 1)  # (B, T, T, n_heads)\n",
        "            attention = self.w_talking_logits(attention)\n",
        "            attention = attention.permute(0, 3, 1, 2)  # (B, n_heads, T, T)\n",
        "            # Reshape back to grouped format\n",
        "            attention = attention.view(B, self.n_kv_heads, self.n_rep, T, T)\n",
        "\n",
        "        attention = self.attention_dropout(attention)\n",
        "\n",
        "        # Apply attention to values with broadcasting\n",
        "        # (B, n_kv_heads, n_rep, T, T) @ (B, n_kv_heads, 1, T, hs) -> (B, n_kv_heads, n_rep, T, hs)\n",
        "        y = attention @ v_expanded\n",
        "\n",
        "        # Reshape back to standard format: (B, n_kv_heads, n_rep, T, hs) -> (B, n_heads, T, hs)\n",
        "        y = y.view(B, self.n_heads, T, self.head_size)\n",
        "\n",
        "        # Transpose and reshape to output format\n",
        "        y = y.transpose(1, 2).contiguous().view(B, T, D)\n",
        "        y = self.w_o(y)\n",
        "        y = self.residual_dropout(y)\n",
        "\n",
        "        return y\n"
      ],
      "metadata": {
        "id": "yBHO1vlYURme"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9vzb9-b0URpi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadLatentAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    Multi-Head Latent Attention (MLA): Compresses K&V into lower-dimensional latent representations.\n",
        "    \"\"\"\n",
        "    def __init__(self, config, talking_heads=True) -> None:\n",
        "        super().__init__()\n",
        "        assert config.d_model % config.n_heads == 0\n",
        "\n",
        "        self.n_heads = config.n_heads\n",
        "        self.d_model = config.d_model\n",
        "        self.head_size = config.d_model // config.n_heads\n",
        "        self.block_size = config.block_size\n",
        "\n",
        "        self.attention_dropout = nn.Dropout(config.dropout_rate) # after softmax\n",
        "        self.residual_dropout = nn.Dropout(config.dropout_rate) # after attention block, before adding with residual connection\n",
        "\n",
        "        self.w_qkv = nn.Linear(self.d_model, 3 * self.d_model)\n",
        "        self.w_o = nn.Linear(self.d_model, self.d_model)\n",
        "\n",
        "        # talking heads: 引入两个小的(n_heads, n_heads)矩阵, 在immediately before and after softmax, 进行head之间的线性变换\n",
        "        # 2 learnable linear transformations, process (1) attention scores (right after Q*V and scaling, before masking),\n",
        "        #   (2) logits (attention logits, after masking, optional padding, softmax, before weighted summing keys)\n",
        "        self.talking_heads = talking_heads\n",
        "        if self.talking_heads:\n",
        "            self.w_talking_weights = nn.Linear(self.n_heads, self.n_heads)\n",
        "            self.w_talking_logits = nn.Linear(self.n_heads, self.n_heads)\n",
        "\n",
        "        self.register_buffer(\"mask\", torch.tril(torch.ones(self.block_size, self.block_size)) # register buffer for low triangular matrix mask\n",
        "                                    .view(1, 1, self.block_size, self.block_size))  # reshape for (B, n_head, T, T) inputs\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, D = x.size() # batch size, sequence length, d_model\n",
        "\n",
        "        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n",
        "        q, k, v = self.w_qkv(x).split(self.d_model, dim=2) # (B, T, D) @ (D, 3D) --> (B, T, 3D) --> split at dim=2 --> (B, T, D)\n",
        "        q = q.view(B, T, self.n_heads, self.head_size).transpose(1, 2) # (B, T, nh, hs) --> (B, nh, T, hs), hs for head_size\n",
        "        k = k.view(B, T, self.n_heads, self.head_size).transpose(1, 2) # (B, T, nh, hs) --> (B, nh, T, hs), hs for head_size\n",
        "        v = v.view(B, T, self.n_heads, self.head_size).transpose(1, 2) # (B, T, nh, hs) --> (B, nh, T, hs), hs for head_size\n",
        "\n",
        "        # attention\n",
        "        attention = q @ k.transpose(-1, -2) # (B, nh, T, hs) @ (B, nh, hs, T) --> (B, nh, T, T)\n",
        "        attention *= self.head_size ** -0.5 # scaled dot product attention\n",
        "        if self.talking_heads:\n",
        "            attention = attention.permute(0, 2, 3, 1) # (B, nh, T, T) --> (B, T, T, nh); (B, nh, T_q, T_k) --> (B, T_q, T_k, nh) when query and key have different length\n",
        "            attention = self.w_talking_weights(attention) # (B, T, T, nh) @ (nh, nh) --> (B, T, T, nh)\n",
        "            attention = attention.permute(0, 3, 1, 2) # (B, T, T, nh) --> (B, nh, T, T)\n",
        "        attention = attention.masked_fill(self.mask[:, :, :T, :T] == 0, float('-inf'))\n",
        "        attention = F.softmax(attention, dim=-1)\n",
        "        if self.talking_heads:\n",
        "            attention = attention.permute(0, 2, 3, 1)  # (B, nh, T, T) --> (B, T, T, nh)\n",
        "            attention = self.w_talking_logits(attention) # (B, T, T, nh) @ (nh, nh) --> (B, T, T, nh)\n",
        "            attention = attention.permute(0, 3, 1, 2) # (B, T, T, nh) --> (B, nh, T, T)\n",
        "        attention = self.attention_dropout(attention)\n",
        "\n",
        "        # output\n",
        "        y = attention @ v # (B, nh, T, T) @ (B, nh, T, hs) --> (B, nh, T, hs)\n",
        "        y = y.transpose(1, 2).contiguous().view(B, T, D) # (B, nh, T, hs) --> (B, T, nh, hs) --> (B, T, D)\n",
        "        y = self.w_o(y) # (B, T, D) @ (D, D) --> (B, T, D)\n",
        "        y = self.residual_dropout(y)\n",
        "\n",
        "        return y"
      ],
      "metadata": {
        "id": "p9swPY8AEg_A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BpYZAHBpEhBg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GPT"
      ],
      "metadata": {
        "id": "68iySV3hEh9H"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wlRLcXEQEkYG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WsnLqeO-Es62"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aQ3A2DCjEs92"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}