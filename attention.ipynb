{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPhNBvxVQqGER1hg9D+u69i",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rczhen/code_ml/blob/main/attention.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import inspect\n",
        "from dataclasses import dataclass\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F"
      ],
      "metadata": {
        "id": "bYaqKhZZCvgV"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclass\n",
        "class GPTConfig:\n",
        "    block_size: int = 1024\n",
        "    vocab_size: int = 50304 # GPT-2 vocab_size of 50257, padded up to nearest multiple of 64 for efficiency\n",
        "    n_layer: int = 12\n",
        "    n_head: int = 12\n",
        "    n_embd: int = 768\n",
        "    dropout: float = 0.0\n",
        "    bias: bool = True # True: bias in Linears and LayerNorms, like GPT-2. False: a bit better and faster\n",
        "\n",
        "@dataclass\n",
        "class Config:\n",
        "    block_size: int = 8\n",
        "    vocab_size: int = 50304 # GPT-2 vocab_size of 50257, padded up to nearest multiple of 64 for efficiency\n",
        "    n_layers: int = 2\n",
        "    n_heads: int = 4\n",
        "    d_model: int = 16\n",
        "    dropout_rate: float = 0.1\n",
        "    bias: bool = True # True: bias in Linears and LayerNorms, like GPT-2. False: a bit better and faster"
      ],
      "metadata": {
        "id": "aTdb4pojIvKR"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CausalSelfAttention(nn.Module):\n",
        "    def __init__(self, config) -> None:\n",
        "        super().__init__()\n",
        "        assert config.d_model % config.n_heads == 0\n",
        "\n",
        "        self.n_heads = config.n_heads\n",
        "        self.d_model = config.d_model\n",
        "        self.head_size = config.d_model // config.n_heads\n",
        "        self.block_size = config.block_size\n",
        "\n",
        "        self.attention_dropout = nn.Dropout(config.dropout_rate) # after softmax\n",
        "        self.residual_dropout = nn.Dropout(config.dropout_rate) # after attention block, before adding with residual connection\n",
        "\n",
        "        self.w_qkv = nn.Linear(self.d_model, 3 * self.d_model)\n",
        "        self.w_o = nn.Linear(self.d_model, self.d_model)\n",
        "\n",
        "        self.register_buffer(\"mask\", torch.tril(torch.ones(self.block_size, self.block_size)) # register buffer for low triangular matrix mask\n",
        "                                    .view(1, 1, self.block_size, self.block_size))  # reshape for (B, n_head, T, T) inputs\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, D = x.size() # batch size, sequence length, d_model\n",
        "\n",
        "        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n",
        "        q, k, v = self.w_qkv(x).split(self.d_model, dim=2) # (B, T, D) @ (D, 3D) --> (B, T, 3D) --> split at dim=2 --> (B, T, D)\n",
        "        q = q.view(B, T, self.n_heads, self.head_size).transpose(1, 2) # (B, T, nh, hs) --> (B, nh, T, hs), hs for head_size\n",
        "        k = k.view(B, T, self.n_heads, self.head_size).transpose(1, 2) # (B, T, nh, hs) --> (B, nh, T, hs), hs for head_size\n",
        "        v = v.view(B, T, self.n_heads, self.head_size).transpose(1, 2) # (B, T, nh, hs) --> (B, nh, T, hs), hs for head_size\n",
        "\n",
        "        # attention\n",
        "        attention = q @ k.transpose(-1, -2) # (B, nh, T, hs) @ (B, nh, hs, T) --> (B, nh, T, T)\n",
        "        attention *= self.head_size ** -0.5 # scaled dot product attention\n",
        "        attention = attention.masked_fill(self.mask[:, :, :T, :T] == 0, float('-inf'))\n",
        "        attention = F.softmax(attention, dim=-1)\n",
        "        attention = self.attention_dropout(attention)\n",
        "\n",
        "        # output\n",
        "        y = attention @ v # (B, nh, T, T) @ (B, nh, T, hs) --> (B, nh, T, hs)\n",
        "        y = y.transpose(1, 2).contiguous().view(B, T, D) # (B, nh, T, hs) --> (B, T, nh, hs) --> (B, T, D)\n",
        "        y = self.w_o(y) # (B, T, D) @ (D, D) --> (B, T, D)\n",
        "        y = self.residual_dropout(y)\n",
        "\n",
        "        return y\n",
        "\n"
      ],
      "metadata": {
        "id": "VCpKzgz7JaH3"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.rand(1, Config.block_size, Config.d_model)\n",
        "print(x)\n",
        "layer = CausalSelfAttention(Config)\n",
        "print(layer(x))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3_tilA7bPIQ3",
        "outputId": "ed5f8215-af6c-46a3-dc76-456fd8e3dca8"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[0.3122, 0.0226, 0.8950, 0.0783, 0.1770, 0.9433, 0.9514, 0.1017,\n",
            "          0.8026, 0.9098, 0.1098, 0.4137, 0.2848, 0.6199, 0.9206, 0.3427],\n",
            "         [0.4449, 0.6739, 0.8439, 0.0493, 0.0081, 0.6511, 0.3104, 0.7010,\n",
            "          0.5342, 0.0650, 0.9289, 0.1583, 0.7377, 0.7175, 0.9571, 0.8403],\n",
            "         [0.6918, 0.3198, 0.3399, 0.0931, 0.6521, 0.7145, 0.9907, 0.2112,\n",
            "          0.6763, 0.5887, 0.5782, 0.3483, 0.3435, 0.1437, 0.8163, 0.6681],\n",
            "         [0.3850, 0.5111, 0.5583, 0.6228, 0.3647, 0.8345, 0.2221, 0.8506,\n",
            "          0.2525, 0.1807, 0.2781, 0.2693, 0.7080, 0.1854, 0.1001, 0.9481],\n",
            "         [0.8707, 0.5919, 0.0291, 0.1116, 0.9614, 0.3914, 0.1568, 0.6211,\n",
            "          0.6675, 0.4568, 0.7768, 0.0650, 0.8675, 0.5075, 0.0551, 0.6097],\n",
            "         [0.4323, 0.3263, 0.6006, 0.4719, 0.7960, 0.0774, 0.3016, 0.0890,\n",
            "          0.2929, 0.8220, 0.3581, 0.1270, 0.4589, 0.2781, 0.5647, 0.6605],\n",
            "         [0.3650, 0.5132, 0.2096, 0.2240, 0.3324, 0.7566, 0.9816, 0.5836,\n",
            "          0.5430, 0.3579, 0.5746, 0.3559, 0.1183, 0.9290, 0.2001, 0.7497],\n",
            "         [0.1632, 0.8346, 0.8752, 0.6790, 0.4601, 0.7499, 0.7731, 0.9787,\n",
            "          0.3916, 0.1585, 0.1328, 0.4142, 0.7913, 0.7399, 0.7101, 0.5538]]])\n",
            "tensor([[[ 0.1792,  0.1029, -0.0287,  0.1040, -0.1452, -0.1980, -0.2158,\n",
            "          -0.2666,  0.2690, -0.6141,  0.0000,  0.2446, -0.3430,  0.2526,\n",
            "           0.2008,  0.4104],\n",
            "         [ 0.1485,  0.3125,  0.0000,  0.0625, -0.1285, -0.3688, -0.3017,\n",
            "          -0.2069,  0.1708, -0.7012, -0.0314,  0.1969, -0.4509,  0.2804,\n",
            "           0.0000,  0.3834],\n",
            "         [ 0.2047,  0.2210,  0.0131,  0.1381, -0.1910, -0.3488, -0.2466,\n",
            "          -0.2902,  0.2522, -0.0000, -0.0896,  0.2028, -0.4720,  0.3534,\n",
            "           0.0684,  0.3865],\n",
            "         [ 0.2684,  0.2731, -0.0396,  0.1259, -0.1956, -0.4143, -0.2446,\n",
            "          -0.2272,  0.2416, -0.6894, -0.0612,  0.1822, -0.4296,  0.3647,\n",
            "          -0.0067,  0.0000],\n",
            "         [ 0.2945,  0.2808, -0.0663,  0.0000, -0.2579, -0.4445, -0.2115,\n",
            "          -0.2215,  0.2616, -0.5504, -0.0346,  0.1680, -0.3727,  0.3748,\n",
            "          -0.0460,  0.4420],\n",
            "         [ 0.3076,  0.2333, -0.0873,  0.1024, -0.2153, -0.4242, -0.2227,\n",
            "          -0.2001,  0.2405, -0.5737, -0.0510,  0.1585, -0.3651,  0.3742,\n",
            "          -0.0298,  0.4363],\n",
            "         [ 0.3280,  0.1757, -0.0600,  0.0000, -0.1273, -0.3631, -0.2552,\n",
            "          -0.2056,  0.2358, -0.6033, -0.1032,  0.1506, -0.3038,  0.0000,\n",
            "           0.0514,  0.0000],\n",
            "         [ 0.3218,  0.2126, -0.0000,  0.1207, -0.1753, -0.4042, -0.2381,\n",
            "          -0.1940,  0.2418, -0.6162, -0.0368,  0.1718, -0.3729,  0.3374,\n",
            "           0.0242,  0.4662]]], grad_fn=<MulBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WUIVHjGbPJ0r"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}