{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rczhen/code_ml/blob/main/attention.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import inspect\n",
        "from dataclasses import dataclass\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "# reference: https://github.com/karpathy/nanoGPT/blob/master/model.py"
      ],
      "metadata": {
        "id": "bYaqKhZZCvgV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclass\n",
        "class GPTConfig:\n",
        "    block_size: int = 1024\n",
        "    vocab_size: int = 50304 # GPT-2 vocab_size of 50257, padded up to nearest multiple of 64 for efficiency\n",
        "    n_layer: int = 12\n",
        "    n_head: int = 12\n",
        "    n_embd: int = 768\n",
        "    dropout: float = 0.0\n",
        "    bias: bool = True # True: bias in Linears and LayerNorms, like GPT-2. False: a bit better and faster\n",
        "\n",
        "@dataclass\n",
        "class Config:\n",
        "    block_size: int = 8\n",
        "    vocab_size: int = 50304 # GPT-2 vocab_size of 50257, padded up to nearest multiple of 64 for efficiency\n",
        "    n_layers: int = 2\n",
        "    n_heads: int = 4\n",
        "    n_kv_heads: int = 2\n",
        "    d_model: int = 16\n",
        "    dropout_rate: float = 0.0\n",
        "    bias: bool = True # True: bias in Linears and LayerNorms, like GPT-2. False: a bit better and faster.\n",
        "                      # for LayerNorm, the reason not using bias, may be the motivation of RMSNorm, i.e. re-scaling is important, re-centering is not"
      ],
      "metadata": {
        "id": "aTdb4pojIvKR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# v1: standard CausalSelfAttention"
      ],
      "metadata": {
        "id": "losC6CqJ2IAD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CausalSelfAttention(nn.Module):\n",
        "    def __init__(self, config) -> None:\n",
        "        super().__init__()\n",
        "        assert config.d_model % config.n_heads == 0\n",
        "\n",
        "        self.n_heads = config.n_heads\n",
        "        self.d_model = config.d_model\n",
        "        self.head_size = config.d_model // config.n_heads\n",
        "        self.block_size = config.block_size\n",
        "\n",
        "        self.attention_dropout = nn.Dropout(config.dropout_rate) # after softmax\n",
        "        self.residual_dropout = nn.Dropout(config.dropout_rate) # after attention block, before adding with residual connection\n",
        "\n",
        "        self.w_qkv = nn.Linear(self.d_model, 3 * self.d_model)\n",
        "        self.w_o = nn.Linear(self.d_model, self.d_model)\n",
        "\n",
        "        self.register_buffer(\"mask\", torch.tril(torch.ones(self.block_size, self.block_size)) # register buffer for low triangular matrix mask\n",
        "                                    .view(1, 1, self.block_size, self.block_size))  # reshape for (B, n_head, T, T) inputs\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, D = x.size() # batch size, sequence length, d_model\n",
        "\n",
        "        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n",
        "        q, k, v = self.w_qkv(x).split(self.d_model, dim=2) # (B, T, D) @ (D, 3D) --> (B, T, 3D) --> split at dim=2 --> (B, T, D)\n",
        "        q = q.view(B, T, self.n_heads, self.head_size).transpose(1, 2) # (B, T, nh, hs) --> (B, nh, T, hs), hs for head_size\n",
        "        k = k.view(B, T, self.n_heads, self.head_size).transpose(1, 2) # (B, T, nh, hs) --> (B, nh, T, hs), hs for head_size\n",
        "        v = v.view(B, T, self.n_heads, self.head_size).transpose(1, 2) # (B, T, nh, hs) --> (B, nh, T, hs), hs for head_size\n",
        "\n",
        "        # attention\n",
        "        attention = q @ k.transpose(-1, -2) # (B, nh, T, hs) @ (B, nh, hs, T) --> (B, nh, T, T)\n",
        "        attention *= self.head_size ** -0.5 # scaled dot product attention\n",
        "        attention = attention.masked_fill(self.mask[:, :, :T, :T] == 0, float('-inf'))\n",
        "        attention = F.softmax(attention, dim=-1)\n",
        "        attention = self.attention_dropout(attention)\n",
        "\n",
        "        # output\n",
        "        y = attention @ v # (B, nh, T, T) @ (B, nh, T, hs) --> (B, nh, T, hs)\n",
        "        y = y.transpose(1, 2).contiguous().view(B, T, D) # (B, nh, T, hs) --> (B, T, nh, hs) --> (B, T, D)\n",
        "        y = self.w_o(y) # (B, T, D) @ (D, D) --> (B, T, D)\n",
        "        y = self.residual_dropout(y)\n",
        "\n",
        "        return y\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "why scaled?\n",
        "if no normalization, the variance of weights wil be on the order of head_size, here is 16\n",
        "when deviding by sqrt(head_size), bring the variance back\n",
        "\"Scaled\" attention additional divides attention scores by 1/sqrt(head_size).\n",
        "so when input Q,K are unit variance, attentions will be unit variance too\n",
        "and Softmax will stay diffuse and not saturate too much.\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "id": "VCpKzgz7JaH3",
        "outputId": "f3879dd0-c21c-45a0-c32d-efea935fc86f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nwhy scaled?\\nif no normalization, the variance of weights wil be on the order of head_size, here is 16\\nwhen deviding by sqrt(head_size), bring the variance back\\n\"Scaled\" attention additional divides attention scores by 1/sqrt(head_size).\\nso when input Q,K are unit variance, attentions will be unit variance too\\nand Softmax will stay diffuse and not saturate too much.\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.rand(1, Config.block_size, Config.d_model)\n",
        "layer = CausalSelfAttention(Config)\n",
        "print(layer(x))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3_tilA7bPIQ3",
        "outputId": "bde467bf-d8b8-48bd-c614-c76236bb0b0f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[ 0.1569,  0.2025,  0.0082, -0.0556, -0.3686,  0.0776,  0.2143,\n",
            "          -0.0328, -0.1326,  0.0038,  0.1973,  0.1777,  0.3261,  0.4472,\n",
            "          -0.1291, -0.1470],\n",
            "         [ 0.0839,  0.1502,  0.0342, -0.0937, -0.3915,  0.1091,  0.2111,\n",
            "           0.0225, -0.1306,  0.1197,  0.1535,  0.1027,  0.3091,  0.4546,\n",
            "          -0.0607, -0.1843],\n",
            "         [ 0.0777,  0.1380,  0.0271, -0.1116, -0.3637,  0.1233,  0.1699,\n",
            "           0.0354, -0.1330,  0.0848,  0.0887,  0.1241,  0.2519,  0.4527,\n",
            "          -0.0549, -0.1606],\n",
            "         [ 0.0457,  0.1185,  0.0582, -0.0996, -0.3259,  0.0922,  0.1785,\n",
            "           0.0802, -0.1493,  0.0712,  0.0738,  0.0730,  0.3146,  0.4426,\n",
            "          -0.1139, -0.1492],\n",
            "         [ 0.0536,  0.1174,  0.0461, -0.0833, -0.3345,  0.0757,  0.1689,\n",
            "           0.0940, -0.1408,  0.1073,  0.0535,  0.0697,  0.3289,  0.4242,\n",
            "          -0.1137, -0.1474],\n",
            "         [ 0.0273,  0.1011,  0.0419, -0.0870, -0.3143,  0.0929,  0.1638,\n",
            "           0.1210, -0.1451,  0.1236,  0.0162,  0.0780,  0.3344,  0.4003,\n",
            "          -0.1121, -0.1378],\n",
            "         [ 0.0211,  0.0920,  0.0546, -0.1004, -0.3268,  0.0757,  0.1586,\n",
            "           0.1404, -0.1621,  0.1329,  0.0319,  0.0755,  0.3671,  0.4269,\n",
            "          -0.0928, -0.1358],\n",
            "         [ 0.0078,  0.0917,  0.0477, -0.0917, -0.3221,  0.0818,  0.1626,\n",
            "           0.1518, -0.1571,  0.1495,  0.0036,  0.1007,  0.3769,  0.3978,\n",
            "          -0.0924, -0.1314]]], grad_fn=<ViewBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# v2: Talking Heads Attention\n",
        "\n",
        "https://arxiv.org/pdf/2003.02436"
      ],
      "metadata": {
        "id": "Db73Nty82UX-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TalkingHeadsCausalSelfAttention(nn.Module):\n",
        "    def __init__(self, config, talking_heads=True) -> None:\n",
        "        super().__init__()\n",
        "        assert config.d_model % config.n_heads == 0\n",
        "\n",
        "        self.n_heads = config.n_heads\n",
        "        self.d_model = config.d_model\n",
        "        self.head_size = config.d_model // config.n_heads\n",
        "        self.block_size = config.block_size\n",
        "\n",
        "        self.attention_dropout = nn.Dropout(config.dropout_rate) # after softmax\n",
        "        self.residual_dropout = nn.Dropout(config.dropout_rate) # after attention block, before adding with residual connection\n",
        "\n",
        "        self.w_qkv = nn.Linear(self.d_model, 3 * self.d_model)\n",
        "        self.w_o = nn.Linear(self.d_model, self.d_model)\n",
        "\n",
        "        # talking heads: 引入两个小的(n_heads, n_heads)矩阵, 在immediately before and after softmax, 进行head之间的线性变换\n",
        "        # 2 learnable linear transformations, process (1) attention scores (right after Q*V and scaling, before masking),\n",
        "        #   (2) logits (attention logits, after masking, optional padding, softmax, before weighted summing keys)\n",
        "        self.talking_heads = talking_heads\n",
        "        if self.talking_heads:\n",
        "            self.w_talking_weights = nn.Linear(self.n_heads, self.n_heads)\n",
        "            self.w_talking_logits = nn.Linear(self.n_heads, self.n_heads)\n",
        "\n",
        "        self.register_buffer(\"mask\", torch.tril(torch.ones(self.block_size, self.block_size)) # register buffer for low triangular matrix mask\n",
        "                                    .view(1, 1, self.block_size, self.block_size))  # reshape for (B, n_head, T, T) inputs\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, D = x.size() # batch size, sequence length, d_model\n",
        "\n",
        "        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n",
        "        q, k, v = self.w_qkv(x).split(self.d_model, dim=2) # (B, T, D) @ (D, 3D) --> (B, T, 3D) --> split at dim=2 --> (B, T, D)\n",
        "        q = q.view(B, T, self.n_heads, self.head_size).transpose(1, 2) # (B, T, nh, hs) --> (B, nh, T, hs), hs for head_size\n",
        "        k = k.view(B, T, self.n_heads, self.head_size).transpose(1, 2) # (B, T, nh, hs) --> (B, nh, T, hs), hs for head_size\n",
        "        v = v.view(B, T, self.n_heads, self.head_size).transpose(1, 2) # (B, T, nh, hs) --> (B, nh, T, hs), hs for head_size\n",
        "\n",
        "        # attention\n",
        "        attention = q @ k.transpose(-1, -2) # (B, nh, T, hs) @ (B, nh, hs, T) --> (B, nh, T, T)\n",
        "        attention *= self.head_size ** -0.5 # scaled dot product attention\n",
        "        if self.talking_heads:\n",
        "            attention = attention.permute(0, 2, 3, 1) # (B, nh, T, T) --> (B, T, T, nh); (B, nh, T_q, T_k) --> (B, T_q, T_k, nh) when query and key have different length\n",
        "            attention = self.w_talking_weights(attention) # (B, T, T, nh) @ (nh, nh) --> (B, T, T, nh)\n",
        "            attention = attention.permute(0, 3, 1, 2) # (B, T, T, nh) --> (B, nh, T, T)\n",
        "        attention = attention.masked_fill(self.mask[:, :, :T, :T] == 0, float('-inf'))\n",
        "        attention = F.softmax(attention, dim=-1)\n",
        "        if self.talking_heads:\n",
        "            attention = attention.permute(0, 2, 3, 1)  # (B, nh, T, T) --> (B, T, T, nh)\n",
        "            attention = self.w_talking_logits(attention) # (B, T, T, nh) @ (nh, nh) --> (B, T, T, nh)\n",
        "            attention = attention.permute(0, 3, 1, 2) # (B, T, T, nh) --> (B, nh, T, T)\n",
        "        attention = self.attention_dropout(attention)\n",
        "\n",
        "        # output\n",
        "        y = attention @ v # (B, nh, T, T) @ (B, nh, T, hs) --> (B, nh, T, hs)\n",
        "        y = y.transpose(1, 2).contiguous().view(B, T, D) # (B, nh, T, hs) --> (B, T, nh, hs) --> (B, T, D)\n",
        "        y = self.w_o(y) # (B, T, D) @ (D, D) --> (B, T, D)\n",
        "        y = self.residual_dropout(y)\n",
        "\n",
        "        return y"
      ],
      "metadata": {
        "id": "k3lQMWKcMETf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.rand(1, Config.block_size, Config.d_model)\n",
        "layer = TalkingHeadsCausalSelfAttention(Config)\n",
        "print(layer(x))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mgVrfGkU3QHR",
        "outputId": "e64bc470-2571-4fbc-fa21-07de99c2788f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[-2.8340, -0.0426,  1.5041,  ..., -0.4150, -3.3890, -0.2439],\n",
            "         [-2.8062,  0.0277,  1.5124,  ..., -0.4782, -3.4129, -0.2836],\n",
            "         [-2.7896,  0.0685,  1.5190,  ..., -0.5058, -3.4253, -0.2956],\n",
            "         ...,\n",
            "         [-2.8243,  0.0383,  1.5419,  ..., -0.4916, -3.4096, -0.2543],\n",
            "         [-2.8244,  0.0396,  1.5425,  ..., -0.4902, -3.4108, -0.2540],\n",
            "         [-2.8255,  0.0398,  1.5407,  ..., -0.4893, -3.4124, -0.2549]]],\n",
            "       grad_fn=<ViewBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "so7Wjm353QSZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# v3: GQA, MQA, MLA"
      ],
      "metadata": {
        "id": "I3zZQz2G3RQP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiQueryAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    Multi-Query Attention: Uses single key and value heads shared across all query heads.\n",
        "    \"\"\"\n",
        "    def __init__(self, config) -> None:\n",
        "        super().__init__()\n",
        "        assert config.d_model % config.n_heads == 0\n",
        "\n",
        "        self.n_heads = config.n_heads\n",
        "        self.d_model = config.d_model\n",
        "        self.head_size = config.d_model // config.n_heads\n",
        "        self.block_size = config.block_size\n",
        "\n",
        "        self.attention_dropout = nn.Dropout(config.dropout_rate) # after softmax\n",
        "        self.residual_dropout = nn.Dropout(config.dropout_rate) # after attention block, before adding with residual connection\n",
        "\n",
        "        # MODIFIED: Separate projections - Q has n_heads, K&V have only 1 head each\n",
        "        self.w_q = nn.Linear(self.d_model, self.d_model)\n",
        "        self.w_k = nn.Linear(self.d_model, self.head_size)\n",
        "        self.w_v = nn.Linear(self.d_model, self.head_size)\n",
        "        self.w_o = nn.Linear(self.d_model, self.d_model)\n",
        "\n",
        "        self.register_buffer(\"mask\", torch.tril(torch.ones(self.block_size, self.block_size)) # register buffer for low triangular matrix mask\n",
        "                                    .view(1, 1, self.block_size, self.block_size))  # reshape for (B, n_head, T, T) inputs\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, D = x.size() # batch size, sequence length, d_model\n",
        "\n",
        "        # MODIFIED: Separate Q, K, V projections\n",
        "        q = self.w_q(x).view(B, T, self.n_heads, self.head_size).transpose(1, 2) # (B, T, D) --> (B, T, nh, hs) --> (B, nh, T, hs)\n",
        "        k = self.w_k(x).view(B, T, 1, self.head_size).transpose(1, 2) # (B, T, hs) --> (B, T, 1, hs) --> (B, 1, T, hs)\n",
        "        v = self.w_v(x).view(B, T, 1, self.head_size).transpose(1, 2) # (B, T, hs) --> (B, T, 1, hs) --> (B, 1, T, hs)\n",
        "\n",
        "        # MODIFIED: Expand K&V to match Q's head dimension for broadcasting\n",
        "        # expansion is not a must, but will make broadcasting more clear\n",
        "        k = k.expand(B, self.n_heads, T, self.head_size) # (B, 1, T, hs) --> (B, nh, T, hs)\n",
        "        v = v.expand(B, self.n_heads, T, self.head_size) # (B, 1, T, hs) --> (B, nh, T, hs)\n",
        "\n",
        "        # attention\n",
        "        attention = q @ k.transpose(-1, -2) # (B, nh, T, hs) @ (B, nh, hs, T) --> (B, nh, T, T)\n",
        "        attention *= self.head_size ** -0.5 # scaled dot product attention\n",
        "        attention = attention.masked_fill(self.mask[:, :, :T, :T] == 0, float('-inf'))\n",
        "        attention = F.softmax(attention, dim=-1)\n",
        "        attention = self.attention_dropout(attention)\n",
        "\n",
        "        # output\n",
        "        y = attention @ v # (B, nh, T, T) @ (B, nh, T, hs) --> (B, nh, T, hs)\n",
        "        y = y.transpose(1, 2).contiguous().view(B, T, D) # (B, nh, T, hs) --> (B, T, nh, hs) --> (B, T, D)\n",
        "        y = self.w_o(y) # (B, T, D) @ (D, D) --> (B, T, D)\n",
        "        y = self.residual_dropout(y)\n",
        "\n",
        "        return y"
      ],
      "metadata": {
        "id": "zHmA1cDh3QUW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.rand(1, Config.block_size, Config.d_model)\n",
        "layer = MultiQueryAttention(Config)\n",
        "print(layer(x))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t9ALZD1oN-lV",
        "outputId": "a4576010-f721-46cc-8934-376585103dae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[-0.1048,  0.0370,  0.1984,  0.1773, -0.1839,  0.0440,  0.2067,\n",
            "          -0.4353, -0.0939,  0.3132, -0.3061,  0.0848,  0.1260,  0.0328,\n",
            "          -0.1256,  0.0107],\n",
            "         [ 0.0250,  0.0641,  0.1823,  0.1719, -0.0930,  0.0568,  0.0918,\n",
            "          -0.2752, -0.2561,  0.0892, -0.3545,  0.1193,  0.1654, -0.0068,\n",
            "          -0.0926,  0.0482],\n",
            "         [ 0.0592,  0.1439,  0.2031,  0.0955,  0.0162,  0.0046, -0.0912,\n",
            "          -0.2180, -0.2133, -0.0308, -0.4258,  0.1769,  0.0639,  0.0609,\n",
            "          -0.0916,  0.0108],\n",
            "         [ 0.0729,  0.1623,  0.2129,  0.0760,  0.0450, -0.0102, -0.1399,\n",
            "          -0.2054, -0.2114, -0.0646, -0.4486,  0.2016,  0.0450,  0.0770,\n",
            "          -0.0845, -0.0055],\n",
            "         [ 0.0589,  0.1553,  0.2128,  0.0523,  0.0501,  0.0032, -0.1384,\n",
            "          -0.2100, -0.1703, -0.0458, -0.4251,  0.2069,  0.0277,  0.0849,\n",
            "          -0.0972, -0.0168],\n",
            "         [ 0.0599,  0.1499,  0.2060,  0.0518,  0.0500,  0.0150, -0.1305,\n",
            "          -0.1953, -0.1744, -0.0463, -0.4096,  0.1990,  0.0300,  0.0766,\n",
            "          -0.1033, -0.0085],\n",
            "         [ 0.0602,  0.1338,  0.1996,  0.0561,  0.0374,  0.0340, -0.1023,\n",
            "          -0.1966, -0.1805, -0.0377, -0.3870,  0.1949,  0.0474,  0.0609,\n",
            "          -0.1051, -0.0017],\n",
            "         [ 0.0641,  0.1426,  0.2091,  0.0671,  0.0405,  0.0123, -0.1146,\n",
            "          -0.2137, -0.1918, -0.0460, -0.4223,  0.2030,  0.0541,  0.0701,\n",
            "          -0.0912, -0.0088]]], grad_fn=<ViewBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class GroupedQueryAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    Grouped Query Attention: Groups multiple query heads to share K&V heads. Compromise between MHA and MQA.\n",
        "    Simple implementation. Using repeat_interleave which requires extra memory.\n",
        "    \"\"\"\n",
        "    def __init__(self, config) -> None:\n",
        "        super().__init__()\n",
        "        assert config.d_model % config.n_heads == 0\n",
        "\n",
        "        self.n_heads = config.n_heads\n",
        "        self.d_model = config.d_model\n",
        "        self.head_size = config.d_model // config.n_heads\n",
        "        self.block_size = config.block_size\n",
        "\n",
        "        # MODIFIED: Number of K&V heads (groups)\n",
        "        assert config.n_heads % config.n_kv_heads == 0, \"n_heads must be divisible by n_kv_heads\"\n",
        "        self.n_kv_heads = config.n_kv_heads\n",
        "        self.n_reps = self.n_heads // self.n_kv_heads\n",
        "\n",
        "        self.attention_dropout = nn.Dropout(config.dropout_rate) # after softmax\n",
        "        self.residual_dropout = nn.Dropout(config.dropout_rate) # after attention block, before adding with residual connection\n",
        "\n",
        "         # MODIFIED: Separate QKV projections\n",
        "        self.w_q = nn.Linear(self.d_model, self.d_model)\n",
        "        self.w_k = nn.Linear(self.d_model, self.head_size * self.n_kv_heads)\n",
        "        self.w_v = nn.Linear(self.d_model, self.head_size * self.n_kv_heads)\n",
        "        self.w_o = nn.Linear(self.d_model, self.d_model)\n",
        "\n",
        "        self.register_buffer(\"mask\", torch.tril(torch.ones(self.block_size, self.block_size)) # register buffer for low triangular matrix mask\n",
        "                                    .view(1, 1, self.block_size, self.block_size))  # reshape for (B, n_head, T, T) inputs\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, D = x.size() # batch size, sequence length, d_model\n",
        "\n",
        "        # MODIFIED: separate QKV\n",
        "        q = self.w_q(x).view(B, T, self.n_heads, self.head_size).transpose(1, 2) # (B, T, D)@(D, D)-->(B, T, D)-->(B, T, nh, hs)-->(B, nh, T, hs)\n",
        "        k = self.w_k(x).view(B, T, self.n_kv_heads, self.head_size).transpose(1, 2) # (B, T, nh_kv*hs)-->(B, T, nh_kv, hs)-->(B, nh_kv, T, hs)\n",
        "        v = self.w_v(x).view(B, T, self.n_kv_heads, self.head_size).transpose(1, 2) # (B, T, nh_kv*hs)-->(B, T, nh_kv, hs)-->(B, nh_kv, T, hs)\n",
        "\n",
        "        # MODIFIED: Repeat K&V heads with repeat_interleave, extra memory used\n",
        "        k = torch.repeat_interleave(k, self.n_reps, dim=1) # (B, nh_kv, T, hs) --> (B, nh, T, hs)\n",
        "        v = torch.repeat_interleave(v, self.n_reps, dim=1) # (B, nh_kv, T, hs) --> (B, nh, T, hs)\n",
        "\n",
        "        # attention\n",
        "        attention = q @ k.transpose(-1, -2) # (B, nh, T, hs) @ (B, nh, hs, T) --> (B, nh, T, T)\n",
        "        attention *= self.head_size ** -0.5 # scaled dot product attention\n",
        "        attention = attention.masked_fill(self.mask[:, :, :T, :T] == 0, float('-inf'))\n",
        "        attention = F.softmax(attention, dim=-1)\n",
        "        attention = self.attention_dropout(attention)\n",
        "\n",
        "        # output\n",
        "        y = attention @ v # (B, nh, T, T) @ (B, nh, T, hs) --> (B, nh, T, hs)\n",
        "        y = y.transpose(1, 2).contiguous().view(B, T, D) # (B, nh, T, hs) --> (B, T, nh, hs) --> (B, T, D)\n",
        "        y = self.w_o(y) # (B, T, D) @ (D, D) --> (B, T, D)\n",
        "        y = self.residual_dropout(y)\n",
        "\n",
        "        return y\n"
      ],
      "metadata": {
        "id": "Hb1rTr84Eg76"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.rand(1, Config.block_size, Config.d_model)\n",
        "layer = GroupedQueryAttention(Config)\n",
        "print(layer(x))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CpcJytspRNHn",
        "outputId": "d8ac09b2-2d04-4bec-9458-d7dd78a2ba34"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[-0.0998, -0.2203,  0.2570,  0.3095, -0.5632, -0.4385, -0.1056,\n",
            "           0.2443, -0.3195,  0.3184, -0.3649, -0.3327, -0.1754, -0.1521,\n",
            "           0.0035, -0.4032],\n",
            "         [-0.0241, -0.1858,  0.3369,  0.3096, -0.5109, -0.4098, -0.0690,\n",
            "           0.3041, -0.2755,  0.2990, -0.3211, -0.2464, -0.1262, -0.2037,\n",
            "           0.0553, -0.4309],\n",
            "         [-0.0709, -0.1929,  0.3086,  0.3396, -0.5252, -0.4374, -0.0714,\n",
            "           0.3116, -0.2691,  0.3350, -0.3915, -0.2731, -0.1580, -0.1831,\n",
            "           0.0336, -0.4853],\n",
            "         [-0.1138, -0.1755,  0.2827,  0.3573, -0.5378, -0.4545, -0.0732,\n",
            "           0.3054, -0.2665,  0.3543, -0.4031, -0.2883, -0.1889, -0.2050,\n",
            "           0.0135, -0.4879],\n",
            "         [-0.1142, -0.1764,  0.2786,  0.3451, -0.5458, -0.4456, -0.0904,\n",
            "           0.3044, -0.2773,  0.3661, -0.4040, -0.2972, -0.1840, -0.1962,\n",
            "           0.0040, -0.4900],\n",
            "         [-0.1215, -0.1853,  0.3020,  0.3758, -0.5414, -0.4579, -0.0802,\n",
            "           0.3391, -0.2675,  0.3769, -0.4411, -0.2986, -0.2055, -0.2051,\n",
            "           0.0063, -0.5372],\n",
            "         [-0.1312, -0.1594,  0.2689,  0.3573, -0.5231, -0.4522, -0.0748,\n",
            "           0.3122, -0.2462,  0.3658, -0.4199, -0.2809, -0.1827, -0.2016,\n",
            "           0.0026, -0.5182],\n",
            "         [-0.1150, -0.1657,  0.2696,  0.3479, -0.5161, -0.4654, -0.0595,\n",
            "           0.3016, -0.2259,  0.3700, -0.4196, -0.2633, -0.1588, -0.1726,\n",
            "          -0.0123, -0.5276]]], grad_fn=<ViewBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# uses broadcasting and reshaping to avoid memory duplication:\n",
        "\n",
        "# Reshape Q instead of expanding K,V: Groups Q heads into (B, n_kv_heads, n_rep, T, hs)\n",
        "# Use broadcasting: K,V are unsqueezed to (B, n_kv_heads, 1, T, hs) for broadcasting\n",
        "# No memory duplication: All operations work with views and broadcasting\n",
        "\n",
        "class GroupedQueryAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    Grouped Query Attention: Groups multiple query heads to share K&V heads. Compromise between MHA and MQA.\n",
        "    \"\"\"\n",
        "    def __init__(self, config) -> None:\n",
        "        super().__init__()\n",
        "        assert config.d_model % config.n_heads == 0\n",
        "\n",
        "        self.n_heads = config.n_heads\n",
        "        self.d_model = config.d_model\n",
        "        self.head_size = config.d_model // config.n_heads\n",
        "        self.block_size = config.block_size\n",
        "\n",
        "        # MODIFIED: Number of K&V heads (groups)\n",
        "        assert config.n_heads % config.n_kv_heads == 0, \"n_heads must be divisible by n_kv_heads\"\n",
        "        self.n_kv_heads = config.n_kv_heads\n",
        "        self.n_reps = self.n_heads // self.n_kv_heads\n",
        "\n",
        "        self.attention_dropout = nn.Dropout(config.dropout_rate) # after softmax\n",
        "        self.residual_dropout = nn.Dropout(config.dropout_rate) # after attention block, before adding with residual connection\n",
        "\n",
        "        # MODIFIED: Separate QKV projections\n",
        "        self.w_q = nn.Linear(self.d_model, self.d_model)\n",
        "        self.w_k = nn.Linear(self.d_model, self.head_size * self.n_kv_heads)\n",
        "        self.w_v = nn.Linear(self.d_model, self.head_size * self.n_kv_heads)\n",
        "        self.w_o = nn.Linear(self.d_model, self.d_model)\n",
        "\n",
        "        # MODIFIED: mask size to fit the (B, nh_kv, n_rep, T, T) attention shape\n",
        "        self.register_buffer(\"mask\", torch.tril(torch.ones(self.block_size, self.block_size)) # register buffer for low triangular matrix mask\n",
        "                                    .view(1, 1, 1, self.block_size, self.block_size))  # reshape for (B, nh_kv, n_rep, T, T) inputs\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, D = x.size() # batch size, sequence length, d_model\n",
        "\n",
        "        # MODIFIED: separate QKV\n",
        "        q = self.w_q(x).view(B, T, self.n_heads, self.head_size).transpose(1, 2) # (B, T, D)@(D, D)-->(B, T, D)-->(B, T, nh, hs)-->(B, nh, T, hs)\n",
        "        k = self.w_k(x).view(B, T, self.n_kv_heads, self.head_size).transpose(1, 2) # (B, T, nh_kv*hs)-->(B, T, nh_kv, hs)-->(B, nh_kv, T, hs)\n",
        "        v = self.w_v(x).view(B, T, self.n_kv_heads, self.head_size).transpose(1, 2) # (B, T, nh_kv*hs)-->(B, T, nh_kv, hs)-->(B, nh_kv, T, hs)\n",
        "\n",
        "        # MODIFIED v2: reshape Q + expanding KV, no repeat_interleave, no extra memory use.\n",
        "        # Q: (B, nh, T, hs) -(reshape)-> (B, nh_kv, n_rep, T, hs),\n",
        "        # K&V: (B, nh_kv, T, hs) -(reshape)-> (B, nh_kv, 1, T, hs) -(expand)-> (B, nh_kv, n_rep, T, hs)\n",
        "        q = q.view(B, self.n_kv_heads, self.n_reps, T, self.head_size)\n",
        "        k = k.view(B, self.n_kv_heads, 1, T, self.head_size).expand(B, self.n_kv_heads, self.n_reps, T, self.head_size)\n",
        "        v = v.view(B, self.n_kv_heads, 1, T, self.head_size).expand(B, self.n_kv_heads, self.n_reps, T, self.head_size)\n",
        "\n",
        "        # MODIFIED v2: attention\n",
        "        attention = q @ k.transpose(-1, -2) # (B, nh_kv, n_rep, T, hs) @ (B, nh_kv, n_rep, hs, T) --> (B, nh_kv, n_rep, T, T)\n",
        "        attention *= self.head_size ** -0.5 # scaled dot product attention\n",
        "        attention = attention.masked_fill(self.mask[:, :, :, :T, :T] == 0, float('-inf'))\n",
        "        attention = F.softmax(attention, dim=-1)\n",
        "        attention = self.attention_dropout(attention)\n",
        "\n",
        "        # output\n",
        "        y = attention @ v # (B, nh_kv, n_rep, T, T) @ (B, nh_kv, n_rep, T, hs) --> (B, nh_kv, n_rep, T, hs)\n",
        "        y = y.permute(0, 3, 1, 2, 4).contiguous().view(B, T, D) # (B, nh_kv, n_rep, T, hs)-->(B, T, nh_kv, n_rep, hs)-->(B, T, D), was (B, nh, T, hs)-->(B, T, nh, hs)-->(B, T, D)\n",
        "        y = self.w_o(y) # (B, T, D) @ (D, D) --> (B, T, D)\n",
        "        y = self.residual_dropout(y)\n",
        "\n",
        "        return y\n"
      ],
      "metadata": {
        "id": "yBHO1vlYURme"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.rand(1, Config.block_size, Config.d_model)\n",
        "layer = GroupedQueryAttention(Config)\n",
        "print(layer(x))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9vzb9-b0URpi",
        "outputId": "b5c9d48c-1a4b-423a-c830-c3ee59fdbfd2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[ 0.0894, -0.1228,  0.1479,  0.1442,  0.1159, -0.0993,  0.0558,\n",
            "          -0.0434, -0.1432, -0.1889,  0.3234, -0.0256, -0.3636,  0.2188,\n",
            "           0.3530, -0.3256],\n",
            "         [ 0.1047, -0.1770,  0.2269,  0.1470,  0.1749, -0.1796,  0.1427,\n",
            "           0.1188, -0.4242, -0.3087,  0.4202,  0.1873, -0.4917,  0.1946,\n",
            "           0.4046, -0.4180],\n",
            "         [ 0.1040, -0.1385,  0.1524,  0.1543,  0.1503, -0.1322,  0.0959,\n",
            "           0.0836, -0.3886, -0.3345,  0.4065,  0.1637, -0.4828,  0.1715,\n",
            "           0.3891, -0.4110],\n",
            "         [ 0.1297, -0.1269,  0.1058,  0.1403,  0.1385, -0.1293,  0.0749,\n",
            "           0.0716, -0.3534, -0.3310,  0.3972,  0.1511, -0.4868,  0.1714,\n",
            "           0.3889, -0.4163],\n",
            "         [ 0.0965, -0.1708,  0.0774,  0.1488,  0.1709, -0.1204,  0.1045,\n",
            "           0.0893, -0.3621, -0.3165,  0.3536,  0.1568, -0.4565,  0.1682,\n",
            "           0.3469, -0.3900],\n",
            "         [ 0.0837, -0.1741,  0.0810,  0.1271,  0.1624, -0.1013,  0.1034,\n",
            "           0.1005, -0.3408, -0.3286,  0.3465,  0.1372, -0.4302,  0.2185,\n",
            "           0.3306, -0.3916],\n",
            "         [ 0.0963, -0.1694,  0.1036,  0.1301,  0.1542, -0.1125,  0.1100,\n",
            "           0.1029, -0.3516, -0.3294,  0.3703,  0.1421, -0.4483,  0.2156,\n",
            "           0.3483, -0.4019],\n",
            "         [ 0.0665, -0.1884,  0.0972,  0.1399,  0.1794, -0.1054,  0.1230,\n",
            "           0.1203, -0.3967, -0.3382,  0.3543,  0.1795, -0.4464,  0.2024,\n",
            "           0.3322, -0.3971]]], grad_fn=<ViewBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadLatentAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    Multi-Head Latent Attention (MLA): Compresses K&V into lower-dimensional latent representations.\n",
        "    paper: https://arxiv.org/pdf/2405.04434, section 2.1\n",
        "    \"\"\"\n",
        "    def __init__(self, config, latent_dim=16) -> None:\n",
        "        super().__init__()\n",
        "        assert config.d_model % config.n_heads == 0\n",
        "\n",
        "        self.n_heads = config.n_heads\n",
        "        self.d_model = config.d_model\n",
        "        self.head_size = config.d_model // config.n_heads\n",
        "        self.block_size = config.block_size\n",
        "\n",
        "        self.attention_dropout = nn.Dropout(config.dropout_rate) # after softmax\n",
        "        self.residual_dropout = nn.Dropout(config.dropout_rate) # after attention block, before adding with residual connection\n",
        "\n",
        "        # MODIFIED\n",
        "        self.latent_dim = latent_dim\n",
        "        self.w_q = nn.Linear(self.d_model, self.d_model)\n",
        "\n",
        "        # kv share the same latent vector, hence only \"one\" latent vector need to be cached during inference.\n",
        "        # plus, during inference, w_k_decode/w_v_decode can be merged with w_q/w_o respectively, hence no decoding required\n",
        "        self.w_kv_compress = nn.Linear(self.d_model, latent_dim, bias=False)\n",
        "        self.w_k_decode = nn.Linear(latent_dim, self.d_model, bias=False)\n",
        "        self.w_v_decode = nn.Linear(latent_dim, self.d_model, bias=False)\n",
        "\n",
        "        self.w_o = nn.Linear(self.d_model, self.d_model)\n",
        "\n",
        "        self.register_buffer(\"mask\", torch.tril(torch.ones(self.block_size, self.block_size)) # register buffer for low triangular matrix mask\n",
        "                                    .view(1, 1, self.block_size, self.block_size))  # reshape for (B, n_head, T, T) inputs\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, D = x.size() # batch size, sequence length, d_model\n",
        "\n",
        "        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n",
        "        q = self.w_q(x) # (B, T, D) @ (D, D) --> (B, T, D)\n",
        "        latent = self.w_kv_compress(x) # (D, latent_dim)\n",
        "        k = self.w_k_decode(latent) # (B, T, D)\n",
        "        v = self.w_v_decode(latent) # (B, T, D)\n",
        "\n",
        "        q = q.view(B, T, self.n_heads, self.head_size).transpose(1, 2) # (B, T, nh, hs) --> (B, nh, T, hs)\n",
        "        k = k.view(B, T, self.n_heads, self.head_size).transpose(1, 2) # (B, T, nh, hs) --> (B, nh, T, hs)\n",
        "        v = v.view(B, T, self.n_heads, self.head_size).transpose(1, 2) # (B, T, nh, hs) --> (B, nh, T, hs)\n",
        "\n",
        "        # attention\n",
        "        attention = q @ k.transpose(-1, -2) # (B, nh, T, hs) @ (B, nh, hs, T) --> (B, nh, T, T)\n",
        "        attention *= self.head_size ** -0.5 # scaled dot product attention\n",
        "        attention = attention.masked_fill(self.mask[:, :, :T, :T] == 0, float('-inf'))\n",
        "        attention = F.softmax(attention, dim=-1)\n",
        "        attention = self.attention_dropout(attention)\n",
        "\n",
        "        # output\n",
        "        y = attention @ v # (B, nh, T, T) @ (B, nh, T, hs) --> (B, nh, T, hs)\n",
        "        y = y.transpose(1, 2).contiguous().view(B, T, D) # (B, nh, T, hs) --> (B, T, nh, hs) --> (B, T, D)\n",
        "        y = self.w_o(y) # (B, T, D) @ (D, D) --> (B, T, D)\n",
        "        y = self.residual_dropout(y)\n",
        "\n",
        "        return y"
      ],
      "metadata": {
        "id": "p9swPY8AEg_A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.rand(1, Config.block_size, Config.d_model)\n",
        "layer = MultiHeadLatentAttention(Config)\n",
        "print(layer(x))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BpYZAHBpEhBg",
        "outputId": "6050a2a6-7816-46a3-ce85-f0c5459fe23e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[-0.2075,  0.2085,  0.1388, -0.0365, -0.1302,  0.0492,  0.0641,\n",
            "          -0.0202,  0.2320, -0.0848,  0.2486, -0.0417,  0.0033, -0.0470,\n",
            "           0.2917, -0.1073],\n",
            "         [-0.2155,  0.2575,  0.1465, -0.0253, -0.1123,  0.0525,  0.0087,\n",
            "          -0.0201,  0.2502, -0.1432,  0.2767,  0.0568, -0.0008, -0.0500,\n",
            "           0.3169, -0.1091],\n",
            "         [-0.2121,  0.2804,  0.1482, -0.0247, -0.1220,  0.0549, -0.0424,\n",
            "          -0.0265,  0.2719, -0.1567,  0.2916,  0.0959,  0.0063, -0.0386,\n",
            "           0.3454, -0.1459],\n",
            "         [-0.2080,  0.2604,  0.1383, -0.0291, -0.1334,  0.0558, -0.0374,\n",
            "          -0.0242,  0.2811, -0.1271,  0.2855,  0.0742,  0.0047, -0.0344,\n",
            "           0.3431, -0.1514],\n",
            "         [-0.2201,  0.2577,  0.1248, -0.0352, -0.1288,  0.0751, -0.0552,\n",
            "          -0.0147,  0.2945, -0.1353,  0.2889,  0.0880, -0.0098, -0.0156,\n",
            "           0.3605, -0.1758],\n",
            "         [-0.2299,  0.2581,  0.1070, -0.0353, -0.1213,  0.0848, -0.0753,\n",
            "          -0.0078,  0.3108, -0.1415,  0.3037,  0.0999, -0.0234, -0.0014,\n",
            "           0.3790, -0.1836],\n",
            "         [-0.2328,  0.2646,  0.0916, -0.0329, -0.1107,  0.0914, -0.0919,\n",
            "           0.0030,  0.3264, -0.1654,  0.3272,  0.1112, -0.0334,  0.0100,\n",
            "           0.3988, -0.1868],\n",
            "         [-0.2336,  0.2668,  0.0818, -0.0326, -0.1104,  0.0896, -0.1076,\n",
            "           0.0069,  0.3366, -0.1562,  0.3324,  0.1120, -0.0334,  0.0253,\n",
            "           0.4134, -0.1958]]], grad_fn=<ViewBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GPT"
      ],
      "metadata": {
        "id": "68iySV3hEh9H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MLP(nn.Module):\n",
        "    def __init__(self, config) -> None:\n",
        "        super().__init__()\n",
        "\n",
        "        self.fc1 = nn.Linear(config.d_model, 4 * config.d_model)\n",
        "        self.fc2 = nn.Linear(4 * config.d_model, config.d_model)\n",
        "        self.gelu = nn.GELU()\n",
        "        self.dropout = nn.Dropout(config.dropout_rate)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.gelu(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.dropout(x)\n",
        "        return x\n",
        "\n",
        "# https://docs.pytorch.org/docs/stable/generated/torch.nn.GELU.html\n",
        "# gelu(x) = x * phi(x), where phi(x) is the Cumulative Distribution Function for Gaussian Distribution."
      ],
      "metadata": {
        "id": "wlRLcXEQEkYG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Block(nn.Module):\n",
        "    def __init__(self, config) -> None:\n",
        "        super().__init__()\n",
        "\n",
        "        ATTENTION_REGISTRY = {\n",
        "            \"causal\": CausalSelfAttention,\n",
        "            \"talk_heads\": TalkingHeadsCausalSelfAttention,\n",
        "            \"mqa\": MultiQueryAttention,\n",
        "            \"gqa\": GroupedQueryAttention,\n",
        "            \"mla\": MultiHeadLatentAttention,\n",
        "        }\n",
        "        self.attention_layer = ATTENTION_REGISTRY[config.attention_type](config)\n",
        "        self.ffn = MLP(config)\n",
        "        self.ln1 = nn.LayerNorm(config.d_model)\n",
        "        self.ln2 = nn.LayerNorm(config.d_model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Pre-layer-norm: LN before attention and FFN\n",
        "        x = x + self.attention_layer(self.ln1(x))\n",
        "        x = x + self.ffn(self.ln2(x))\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "WsnLqeO-Es62"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GPT(nn.Module):\n",
        "    def __init__(self, config) -> None:\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "\n",
        "        self.transformer = nn.ModuleDict(dict(\n",
        "            w_token_emb = nn.Embedding(config.vocab_size, config.d_model),\n",
        "            w_pos_emb = nn.Embedding(config.block_size, config.d_model),\n",
        "            trans_in_dropout = nn.Dropout(config.dropout_rate), # transformer input dropout\n",
        "            trans_out_ln = nn.LayerNorm(config.d_model), # layer norm after transformer layers, before feeding into lm_head\n",
        "            trans_layers = nn.ModuleList(Block(config) for _ in range(config.n_layers)),\n",
        "        ))\n",
        "\n",
        "        self.lm_head = nn.Linear(config.d_model, config.vocab_size, bias=False)\n",
        "\n",
        "        # init weights\n",
        "        # report num of params\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        # idx and targets are both (B, T) tensor of integers\n",
        "        device = idx.device\n",
        "\n",
        "        B, T = idx.size()\n",
        "        assert T <= self.config.block_size, f\"Sequence length {T}, block size {self.config.block_size}\"\n",
        "\n",
        "        pos = torch.arange(0, T, dtype=torch.long, device=device) # shape: (T), arange returns a 1-D tensor with range [start, end)\n",
        "\n",
        "        # input\n",
        "        token_emb = self.transformer.w_token_emb(idx) # (B, T, d_model)\n",
        "        pos_emb = self.transformer.w_pos_emb(pos) # (T, d_model)\n",
        "        x = token_emb + pos_emb # (B, T, d_model) + (T, d_model) with broadcasting --> (B, T, d_model)\n",
        "\n",
        "        # forward\n",
        "        x = self.transformer.trans_in_dropout(x)\n",
        "        for layer in self.transformer.trans_layers:\n",
        "            x = layer(x)\n",
        "        x = self.transformer.trans_out_ln(x) # (B, T, d_model)\n",
        "        logits = self.lm_head(x) # (B, T, vocab_size)\n",
        "\n",
        "        # loss\n",
        "        if targets is not None:\n",
        "            logits = logits.view(B * T, self.config.vocab_size)\n",
        "            targets = targets.view(B * T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "        else:\n",
        "            loss = None\n",
        "\n",
        "        return logits, loss\n"
      ],
      "metadata": {
        "id": "aQ3A2DCjEs92"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test Attentions with a Simple Task\n",
        "\n",
        "https://github.com/karpathy/ng-video-lecture/blob/master/gpt.py"
      ],
      "metadata": {
        "id": "GZyq3wpPxvaR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
      ],
      "metadata": {
        "id": "pCr-lOohcqFD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(1337)\n",
        "\n",
        "batch_size = 16 # how many independent sequences will we process in parallel?\n",
        "block_size = 32 # what is the maximum context length for predictions?\n",
        "max_iters = 5000\n",
        "eval_interval = 100\n",
        "learning_rate = 1e-3\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 200\n",
        "n_embd = 64\n",
        "n_head = 4\n",
        "n_layer = 4\n",
        "dropout = 0.0\n",
        "\n",
        "with open('input.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "\n",
        "# here are all the unique characters that occur in this text\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "# create a mapping from characters to integers\n",
        "stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "itos = { i:ch for i,ch in enumerate(chars) }\n",
        "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
        "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
        "\n",
        "# Train and test splits\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "\n",
        "# data loading\n",
        "def get_batch(split):\n",
        "    # generate a small batch of data of inputs x and targets y\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    return x, y\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out"
      ],
      "metadata": {
        "id": "-hEBkvVTYx5V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclass\n",
        "class Config:\n",
        "    attention_type: str = 'causal'\n",
        "    block_size: int = 32\n",
        "    vocab_size: int = 65\n",
        "    n_layers: int = 4\n",
        "    n_heads: int = 4\n",
        "    n_kv_heads: int = 2\n",
        "    d_model: int = 64\n",
        "    dropout_rate: float = 0.0\n",
        "    bias: bool = True\n",
        "\n",
        "\n",
        "model = GPT(Config)\n",
        "m = model.to(device)\n",
        "# print the number of parameters in the model\n",
        "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
        "\n",
        "# create a PyTorch optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for iter in range(max_iters):\n",
        "\n",
        "    # every once in a while evaluate the loss on train and val sets\n",
        "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "# # generate from the model\n",
        "# context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "# print(decode(m.generate(context, max_new_tokens=2000)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "57daVJO3cRdt",
        "outputId": "056b2720-1d3f-4496-a67b-abd1aea492db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.210592 M parameters\n",
            "step 0: train loss 4.4870, val loss 4.4812\n",
            "step 100: train loss 2.6412, val loss 2.6553\n",
            "step 200: train loss 2.3580, val loss 2.3581\n",
            "step 300: train loss 2.2012, val loss 2.2083\n",
            "step 400: train loss 2.0734, val loss 2.0922\n",
            "step 500: train loss 1.9744, val loss 1.9897\n",
            "step 600: train loss 1.8508, val loss 1.8655\n",
            "step 700: train loss 1.6373, val loss 1.6542\n",
            "step 800: train loss 1.1933, val loss 1.2377\n",
            "step 900: train loss 0.8628, val loss 0.9049\n",
            "step 1000: train loss 0.6628, val loss 0.6903\n",
            "step 1100: train loss 0.4888, val loss 0.5265\n",
            "step 1200: train loss 0.4063, val loss 0.4287\n",
            "step 1300: train loss 0.3410, val loss 0.3660\n",
            "step 1400: train loss 0.2917, val loss 0.3150\n",
            "step 1500: train loss 0.2529, val loss 0.2758\n",
            "step 1600: train loss 0.2398, val loss 0.2597\n",
            "step 1700: train loss 0.2310, val loss 0.2444\n",
            "step 1800: train loss 0.2068, val loss 0.2260\n",
            "step 1900: train loss 0.1864, val loss 0.2049\n",
            "step 2000: train loss 0.1806, val loss 0.1971\n",
            "step 2100: train loss 0.1768, val loss 0.1891\n",
            "step 2200: train loss 0.1607, val loss 0.1748\n",
            "step 2300: train loss 0.1682, val loss 0.1803\n",
            "step 2400: train loss 0.1526, val loss 0.1662\n",
            "step 2500: train loss 0.1468, val loss 0.1597\n",
            "step 2600: train loss 0.1450, val loss 0.1564\n",
            "step 2700: train loss 0.1505, val loss 0.1627\n",
            "step 2800: train loss 0.1460, val loss 0.1532\n",
            "step 2900: train loss 0.1389, val loss 0.1494\n",
            "step 3000: train loss 0.1318, val loss 0.1454\n",
            "step 3100: train loss 0.1390, val loss 0.1511\n",
            "step 3200: train loss 0.1238, val loss 0.1388\n",
            "step 3300: train loss 0.1283, val loss 0.1396\n",
            "step 3400: train loss 0.1198, val loss 0.1299\n",
            "step 3500: train loss 0.1220, val loss 0.1323\n",
            "step 3600: train loss 0.1164, val loss 0.1272\n",
            "step 3700: train loss 0.1229, val loss 0.1321\n",
            "step 3800: train loss 0.1222, val loss 0.1350\n",
            "step 3900: train loss 0.1152, val loss 0.1266\n",
            "step 4000: train loss 0.1204, val loss 0.1261\n",
            "step 4100: train loss 0.1062, val loss 0.1145\n",
            "step 4200: train loss 0.1142, val loss 0.1210\n",
            "step 4300: train loss 0.1208, val loss 0.1271\n",
            "step 4400: train loss 0.1111, val loss 0.1170\n",
            "step 4500: train loss 0.1087, val loss 0.1155\n",
            "step 4600: train loss 0.1101, val loss 0.1158\n",
            "step 4700: train loss 0.1082, val loss 0.1176\n",
            "step 4800: train loss 0.1071, val loss 0.1154\n",
            "step 4900: train loss 0.1007, val loss 0.1093\n",
            "step 4999: train loss 0.1054, val loss 0.1122\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclass\n",
        "class Config:\n",
        "    attention_type: str = 'talk_heads'\n",
        "    block_size: int = 32\n",
        "    vocab_size: int = 65\n",
        "    n_layers: int = 4\n",
        "    n_heads: int = 4\n",
        "    n_kv_heads: int = 2\n",
        "    d_model: int = 64\n",
        "    dropout_rate: float = 0.0\n",
        "    bias: bool = True\n",
        "\n",
        "\n",
        "model = GPT(Config)\n",
        "m = model.to(device)\n",
        "# print the number of parameters in the model\n",
        "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
        "\n",
        "# create a PyTorch optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for iter in range(max_iters):\n",
        "\n",
        "    # every once in a while evaluate the loss on train and val sets\n",
        "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "# # generate from the model\n",
        "# context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "# print(decode(m.generate(context, max_new_tokens=2000)[0].tolist()))"
      ],
      "metadata": {
        "id": "O7JG21Fukfg5",
        "outputId": "0b5036bc-5af3-4e91-ad77-0157c341ffdf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.210592 M parameters\n",
            "step 0: train loss 4.2271, val loss 4.2197\n",
            "step 100: train loss 2.6239, val loss 2.6475\n",
            "step 200: train loss 2.3401, val loss 2.3560\n",
            "step 300: train loss 2.1910, val loss 2.2089\n",
            "step 400: train loss 2.0817, val loss 2.1092\n",
            "step 500: train loss 1.9874, val loss 2.0083\n",
            "step 600: train loss 1.8646, val loss 1.8761\n",
            "step 700: train loss 1.6545, val loss 1.6903\n",
            "step 800: train loss 1.2388, val loss 1.2895\n",
            "step 900: train loss 0.8888, val loss 0.9229\n",
            "step 1000: train loss 0.6572, val loss 0.6915\n",
            "step 1100: train loss 0.5364, val loss 0.5698\n",
            "step 1200: train loss 0.4070, val loss 0.4465\n",
            "step 1300: train loss 0.3373, val loss 0.3666\n",
            "step 1400: train loss 0.2996, val loss 0.3205\n",
            "step 1500: train loss 0.2669, val loss 0.2877\n",
            "step 1600: train loss 0.2441, val loss 0.2681\n",
            "step 1700: train loss 0.2169, val loss 0.2313\n",
            "step 1800: train loss 0.2198, val loss 0.2385\n",
            "step 1900: train loss 0.1972, val loss 0.2215\n",
            "step 2000: train loss 0.1864, val loss 0.2029\n",
            "step 2100: train loss 0.1669, val loss 0.1879\n",
            "step 2200: train loss 0.1672, val loss 0.1817\n",
            "step 2300: train loss 0.1574, val loss 0.1735\n",
            "step 2400: train loss 0.1524, val loss 0.1659\n",
            "step 2500: train loss 0.1572, val loss 0.1693\n",
            "step 2600: train loss 0.1397, val loss 0.1552\n",
            "step 2700: train loss 0.1323, val loss 0.1450\n",
            "step 2800: train loss 0.1333, val loss 0.1485\n",
            "step 2900: train loss 0.1415, val loss 0.1540\n",
            "step 3000: train loss 0.1340, val loss 0.1493\n",
            "step 3100: train loss 0.1366, val loss 0.1485\n",
            "step 3200: train loss 0.1251, val loss 0.1378\n",
            "step 3300: train loss 0.1189, val loss 0.1280\n",
            "step 3400: train loss 0.1256, val loss 0.1356\n",
            "step 3500: train loss 0.1268, val loss 0.1326\n",
            "step 3600: train loss 0.1208, val loss 0.1289\n",
            "step 3700: train loss 0.1138, val loss 0.1236\n",
            "step 3800: train loss 0.1208, val loss 0.1293\n",
            "step 3900: train loss 0.1121, val loss 0.1188\n",
            "step 4000: train loss 0.1207, val loss 0.1292\n",
            "step 4100: train loss 0.1119, val loss 0.1215\n",
            "step 4200: train loss 0.1083, val loss 0.1148\n",
            "step 4300: train loss 0.1066, val loss 0.1140\n",
            "step 4400: train loss 0.1130, val loss 0.1197\n",
            "step 4500: train loss 0.1171, val loss 0.1241\n",
            "step 4600: train loss 0.1085, val loss 0.1177\n",
            "step 4700: train loss 0.1153, val loss 0.1225\n",
            "step 4800: train loss 0.1015, val loss 0.1104\n",
            "step 4900: train loss 0.1151, val loss 0.1221\n",
            "step 4999: train loss 0.1087, val loss 0.1157\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclass\n",
        "class Config:\n",
        "    attention_type: str = 'mqa'\n",
        "    block_size: int = 32\n",
        "    vocab_size: int = 65\n",
        "    n_layers: int = 4\n",
        "    n_heads: int = 4\n",
        "    n_kv_heads: int = 2\n",
        "    d_model: int = 64\n",
        "    dropout_rate: float = 0.0\n",
        "    bias: bool = True\n",
        "\n",
        "\n",
        "model = GPT(Config)\n",
        "m = model.to(device)\n",
        "# print the number of parameters in the model\n",
        "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
        "\n",
        "# create a PyTorch optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for iter in range(max_iters):\n",
        "\n",
        "    # every once in a while evaluate the loss on train and val sets\n",
        "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "# # generate from the model\n",
        "# context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "# print(decode(m.generate(context, max_new_tokens=2000)[0].tolist()))"
      ],
      "metadata": {
        "id": "rhkdJC_WkfnH",
        "outputId": "49e451c6-9269-43ec-9d21-9264d5fba036",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.185472 M parameters\n",
            "step 0: train loss 4.3284, val loss 4.3242\n",
            "step 100: train loss 2.6253, val loss 2.6457\n",
            "step 200: train loss 2.4919, val loss 2.5095\n",
            "step 300: train loss 2.3973, val loss 2.4069\n",
            "step 400: train loss 2.3516, val loss 2.3733\n",
            "step 500: train loss 2.3048, val loss 2.3052\n",
            "step 600: train loss 2.2480, val loss 2.2665\n",
            "step 700: train loss 2.2068, val loss 2.2288\n",
            "step 800: train loss 2.1658, val loss 2.1994\n",
            "step 900: train loss 2.1147, val loss 2.1528\n",
            "step 1000: train loss 2.0840, val loss 2.1300\n",
            "step 1100: train loss 2.0514, val loss 2.1113\n",
            "step 1200: train loss 2.0286, val loss 2.0981\n",
            "step 1300: train loss 2.0089, val loss 2.0779\n",
            "step 1400: train loss 1.9822, val loss 2.0471\n",
            "step 1500: train loss 1.9602, val loss 2.0363\n",
            "step 1600: train loss 1.9391, val loss 2.0137\n",
            "step 1700: train loss 1.9245, val loss 2.0073\n",
            "step 1800: train loss 1.9038, val loss 2.0025\n",
            "step 1900: train loss 1.8975, val loss 1.9981\n",
            "step 2000: train loss 1.8744, val loss 1.9879\n",
            "step 2100: train loss 1.8701, val loss 1.9873\n",
            "step 2200: train loss 1.8482, val loss 1.9611\n",
            "step 2300: train loss 1.8369, val loss 1.9495\n",
            "step 2400: train loss 1.8298, val loss 1.9583\n",
            "step 2500: train loss 1.8161, val loss 1.9261\n",
            "step 2600: train loss 1.8033, val loss 1.9463\n",
            "step 2700: train loss 1.7924, val loss 1.9180\n",
            "step 2800: train loss 1.7814, val loss 1.9226\n",
            "step 2900: train loss 1.7658, val loss 1.8987\n",
            "step 3000: train loss 1.7555, val loss 1.8974\n",
            "step 3100: train loss 1.7521, val loss 1.8984\n",
            "step 3200: train loss 1.7420, val loss 1.8553\n",
            "step 3300: train loss 1.7458, val loss 1.8829\n",
            "step 3400: train loss 1.7212, val loss 1.8611\n",
            "step 3500: train loss 1.7264, val loss 1.8771\n",
            "step 3600: train loss 1.7264, val loss 1.8777\n",
            "step 3700: train loss 1.7187, val loss 1.8763\n",
            "step 3800: train loss 1.7128, val loss 1.8658\n",
            "step 3900: train loss 1.6962, val loss 1.8343\n",
            "step 4000: train loss 1.6899, val loss 1.8492\n",
            "step 4100: train loss 1.6989, val loss 1.8355\n",
            "step 4200: train loss 1.6871, val loss 1.8344\n",
            "step 4300: train loss 1.6850, val loss 1.8451\n",
            "step 4400: train loss 1.6681, val loss 1.8436\n",
            "step 4500: train loss 1.6760, val loss 1.8452\n",
            "step 4600: train loss 1.6673, val loss 1.8353\n",
            "step 4700: train loss 1.6680, val loss 1.8255\n",
            "step 4800: train loss 1.6534, val loss 1.8313\n",
            "step 4900: train loss 1.6358, val loss 1.8056\n",
            "step 4999: train loss 1.6523, val loss 1.8181\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclass\n",
        "class Config:\n",
        "    attention_type: str = 'gqa'\n",
        "    block_size: int = 32\n",
        "    vocab_size: int = 65\n",
        "    n_layers: int = 4\n",
        "    n_heads: int = 4\n",
        "    n_kv_heads: int = 2\n",
        "    d_model: int = 64\n",
        "    dropout_rate: float = 0.0\n",
        "    bias: bool = True\n",
        "\n",
        "\n",
        "model = GPT(Config)\n",
        "m = model.to(device)\n",
        "# print the number of parameters in the model\n",
        "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
        "\n",
        "# create a PyTorch optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for iter in range(max_iters):\n",
        "\n",
        "    # every once in a while evaluate the loss on train and val sets\n",
        "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "# # generate from the model\n",
        "# context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "# print(decode(m.generate(context, max_new_tokens=2000)[0].tolist()))"
      ],
      "metadata": {
        "id": "HL4uIvSmkfyc",
        "outputId": "324ea78b-7c8e-4247-f655-e62d9cd19455",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.193792 M parameters\n",
            "step 0: train loss 4.2811, val loss 4.2823\n",
            "step 100: train loss 2.6382, val loss 2.6425\n",
            "step 200: train loss 2.4949, val loss 2.5020\n",
            "step 300: train loss 2.4018, val loss 2.4161\n",
            "step 400: train loss 2.3529, val loss 2.3487\n",
            "step 500: train loss 2.2826, val loss 2.2993\n",
            "step 600: train loss 2.2474, val loss 2.2645\n",
            "step 700: train loss 2.1883, val loss 2.2123\n",
            "step 800: train loss 2.1669, val loss 2.1887\n",
            "step 900: train loss 2.1161, val loss 2.1564\n",
            "step 1000: train loss 2.0819, val loss 2.1237\n",
            "step 1100: train loss 2.0446, val loss 2.1096\n",
            "step 1200: train loss 2.0093, val loss 2.0791\n",
            "step 1300: train loss 1.9978, val loss 2.0664\n",
            "step 1400: train loss 1.9771, val loss 2.0363\n",
            "step 1500: train loss 1.9579, val loss 2.0327\n",
            "step 1600: train loss 1.9309, val loss 2.0276\n",
            "step 1700: train loss 1.9093, val loss 2.0122\n",
            "step 1800: train loss 1.8935, val loss 1.9917\n",
            "step 1900: train loss 1.8828, val loss 1.9743\n",
            "step 2000: train loss 1.8619, val loss 1.9665\n",
            "step 2100: train loss 1.8501, val loss 1.9557\n",
            "step 2200: train loss 1.8375, val loss 1.9409\n",
            "step 2300: train loss 1.8279, val loss 1.9356\n",
            "step 2400: train loss 1.8146, val loss 1.9380\n",
            "step 2500: train loss 1.7981, val loss 1.9092\n",
            "step 2600: train loss 1.7935, val loss 1.9078\n",
            "step 2700: train loss 1.7766, val loss 1.9065\n",
            "step 2800: train loss 1.7643, val loss 1.8922\n",
            "step 2900: train loss 1.7593, val loss 1.9084\n",
            "step 3000: train loss 1.7393, val loss 1.8843\n",
            "step 3100: train loss 1.7367, val loss 1.8902\n",
            "step 3200: train loss 1.7492, val loss 1.8740\n",
            "step 3300: train loss 1.7073, val loss 1.8637\n",
            "step 3400: train loss 1.7174, val loss 1.8659\n",
            "step 3500: train loss 1.7295, val loss 1.8646\n",
            "step 3600: train loss 1.6967, val loss 1.8671\n",
            "step 3700: train loss 1.7004, val loss 1.8552\n",
            "step 3800: train loss 1.6878, val loss 1.8565\n",
            "step 3900: train loss 1.6962, val loss 1.8490\n",
            "step 4000: train loss 1.6844, val loss 1.8403\n",
            "step 4100: train loss 1.6781, val loss 1.8286\n",
            "step 4200: train loss 1.6633, val loss 1.8201\n",
            "step 4300: train loss 1.6686, val loss 1.8258\n",
            "step 4400: train loss 1.6687, val loss 1.8214\n",
            "step 4500: train loss 1.6602, val loss 1.8122\n",
            "step 4600: train loss 1.6572, val loss 1.8278\n",
            "step 4700: train loss 1.6517, val loss 1.8283\n",
            "step 4800: train loss 1.6459, val loss 1.8103\n",
            "step 4900: train loss 1.6527, val loss 1.7990\n",
            "step 4999: train loss 1.6479, val loss 1.7981\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclass\n",
        "class Config:\n",
        "    attention_type: str = 'mla'\n",
        "    block_size: int = 32\n",
        "    vocab_size: int = 65\n",
        "    n_layers: int = 4\n",
        "    n_heads: int = 4\n",
        "    n_kv_heads: int = 2\n",
        "    d_model: int = 64\n",
        "    dropout_rate: float = 0.0\n",
        "    bias: bool = True\n",
        "\n",
        "\n",
        "model = GPT(Config)\n",
        "m = model.to(device)\n",
        "# print the number of parameters in the model\n",
        "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
        "\n",
        "# create a PyTorch optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for iter in range(max_iters):\n",
        "\n",
        "    # every once in a while evaluate the loss on train and val sets\n",
        "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "# # generate from the model\n",
        "# context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "# print(decode(m.generate(context, max_new_tokens=2000)[0].tolist()))"
      ],
      "metadata": {
        "id": "u4AOESipkf3G",
        "outputId": "2bdbd24a-9f6b-4079-f12e-f4302714f592",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.18944 M parameters\n",
            "step 0: train loss 4.3211, val loss 4.3351\n",
            "step 100: train loss 2.6422, val loss 2.6467\n",
            "step 200: train loss 2.5181, val loss 2.5328\n",
            "step 300: train loss 2.4352, val loss 2.4318\n",
            "step 400: train loss 2.3668, val loss 2.3769\n",
            "step 500: train loss 2.3253, val loss 2.3384\n",
            "step 600: train loss 2.2967, val loss 2.3073\n",
            "step 700: train loss 2.2611, val loss 2.2794\n",
            "step 800: train loss 2.2204, val loss 2.2563\n",
            "step 900: train loss 2.1763, val loss 2.2236\n",
            "step 1000: train loss 2.1505, val loss 2.1800\n",
            "step 1100: train loss 2.1271, val loss 2.1670\n",
            "step 1200: train loss 2.1007, val loss 2.1408\n",
            "step 1300: train loss 2.0637, val loss 2.1127\n",
            "step 1400: train loss 2.0502, val loss 2.0949\n",
            "step 1500: train loss 2.0251, val loss 2.0857\n",
            "step 1600: train loss 1.9922, val loss 2.0839\n",
            "step 1700: train loss 1.9729, val loss 2.0589\n",
            "step 1800: train loss 1.9651, val loss 2.0461\n",
            "step 1900: train loss 1.9336, val loss 2.0342\n",
            "step 2000: train loss 1.9321, val loss 2.0240\n",
            "step 2100: train loss 1.9122, val loss 2.0171\n",
            "step 2200: train loss 1.9059, val loss 2.0026\n",
            "step 2300: train loss 1.8791, val loss 1.9916\n",
            "step 2400: train loss 1.8687, val loss 1.9859\n",
            "step 2500: train loss 1.8442, val loss 1.9806\n",
            "step 2600: train loss 1.8499, val loss 1.9705\n",
            "step 2700: train loss 1.8413, val loss 1.9542\n",
            "step 2800: train loss 1.8242, val loss 1.9518\n",
            "step 2900: train loss 1.8078, val loss 1.9496\n",
            "step 3000: train loss 1.8039, val loss 1.9287\n",
            "step 3100: train loss 1.7925, val loss 1.9144\n",
            "step 3200: train loss 1.7972, val loss 1.9389\n",
            "step 3300: train loss 1.7788, val loss 1.9228\n",
            "step 3400: train loss 1.7708, val loss 1.9190\n",
            "step 3500: train loss 1.7563, val loss 1.9015\n",
            "step 3600: train loss 1.7427, val loss 1.9015\n",
            "step 3700: train loss 1.7575, val loss 1.8953\n",
            "step 3800: train loss 1.7408, val loss 1.9109\n",
            "step 3900: train loss 1.7297, val loss 1.9061\n",
            "step 4000: train loss 1.7429, val loss 1.8862\n",
            "step 4100: train loss 1.7284, val loss 1.8844\n",
            "step 4200: train loss 1.7205, val loss 1.8752\n",
            "step 4300: train loss 1.7140, val loss 1.8834\n",
            "step 4400: train loss 1.7079, val loss 1.8782\n",
            "step 4500: train loss 1.6967, val loss 1.8565\n",
            "step 4600: train loss 1.6909, val loss 1.8718\n",
            "step 4700: train loss 1.6906, val loss 1.8686\n",
            "step 4800: train loss 1.6922, val loss 1.8675\n",
            "step 4900: train loss 1.7007, val loss 1.8521\n",
            "step 4999: train loss 1.6794, val loss 1.8569\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Results:\n",
        "causal: step 4999: train loss 0.1054, val loss 0.1122, 0.210592 M parameters\n",
        "talk: step 4999: train loss 0.1087, val loss 0.1157, 0.210592 M parameters\n",
        "mqa: step 4999: train loss 1.6523, val loss 1.8181, 0.185472 M parameters\n",
        "gqa: step 4999: train loss 1.6479, val loss 1.7981 (n_heads=4, n_kv_heads=2), 0.193792 M parameters\n",
        "mla: step 4999: train loss 1.6794, val loss 1.8569 (d_model=64, latent_dim=16), 0.18944 M parameters"
      ],
      "metadata": {
        "id": "guNZ9DSKk4gJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DIN (Q & KV have different seq length)"
      ],
      "metadata": {
        "id": "MB4RIho0bRo2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "User behavior: [B, L, D], batch size, length of behavior sequence, dim of embedding.\n",
        "Candidate item: [B, D].\n",
        "\n",
        "Expected tensors:\n",
        "Output user embedding: [B, D]\n",
        "当然不一定像 BERT 那样为了保证可以叠 layer，从而使 user embedding 保持 dim of D\n",
        "最新的 spice，click history 3 heads, purchase history 1 head, 最终 user sequence vector 为 3 * 64 + 64\n",
        "\n",
        "attention scores: [B, n_heads, L], attention scores of candidate item, wrt all L behaviors\n",
        "\n",
        "To perform multi-head attentions, there must be linear transformations on input embeddings.\n",
        "Inner product between candidate embedding and behavior embedding, can only provide one-head embedding.\n",
        "\n",
        "Attention 也不一定像 cross attention 这样实现\n",
        "DIN 原文里，是 k, q out product + 自身, 过一层 dim=36 的 FC layer。SP 也是用过 MLP 得到 attention scores\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "class DIN:\n",
        "    def __init__(self, d_model, n_heads):\n",
        "        self.d_model = d_model\n",
        "        self.n_heads = n_heads\n",
        "        self.d_head = d_model // n_heads\n",
        "\n",
        "        self.w_q = nn.Linear(d_model, d_model)\n",
        "        self.w_kv = nn.Linear(d_model, 2 * d_model)\n",
        "\n",
        "        # self.out = nn.Linear(d_model, d_model)\n",
        "\n",
        "    def forward(self, seq, candidates):\n",
        "        B, L, D = seq.size()\n",
        "\n",
        "        q = self.w_q(candidates)  # q: (B, d_model)\n",
        "        k, v = self.w_kv(seq).split(self.d_model, dim=2)  # k, v: (B, L, d_model)\n",
        "\n",
        "        q = q.view(B, 1, self.n_heads, self.d_head).transpose(1, 2)  # (B, n_heads, 1, d_head)\n",
        "        k = k.view(B, L, self.n_heads, self.d_head).transpose(1, 2)  # (B, n_heads, L, d_head)\n",
        "        v = v.view(B, L, self.n_heads, self.d_head).transpose(1, 2)  # (B, n_heads, L, d_head)\n",
        "\n",
        "        attentions = q @ k.transpose(-2, -1)  # (B, n_heads, 1, d_head) @ (B, n_heads, d_head, L) -> (B, n_heads, 1, L)\n",
        "        attentions = attentions / (self.d_head ** 0.5)  # scaled\n",
        "        attentions = F.softmax(attentions, dim=-1)\n",
        "\n",
        "        y = attentions @ v  # (B, n_heads, 1, L) @ (B, n_heads, L, d_head) -> (B, n_heads, 1, d_head)\n",
        "\n",
        "        y = y.transpose(1, 2).contiguous().view(B, self.d_model)  # (B, n_heads, 1, d_head) -> (B, d_model)\n",
        "\n",
        "        # y = self.out(y)  # (B, d_model) -> (B, d_model)\n",
        "\n",
        "        return y\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "attention can also be implemented with broadcasting + reduce sum\n",
        "attentions: (B, n_heads, 1, L)\n",
        "v: (B, n_heads, L, d_head)\n",
        "v * attentions.transpose(-1, -2), with broadcasting the (L, 1) part d_head time -> (B, n_heads, L, d_head)\n",
        "then sum along axis = -2 -> (B, n_heads, d_head)\n",
        "\n",
        "broadcasting: [m, n] +-*/ [m, 1] or [1, n], will duplicate the latter n/m times\n",
        "https://www.youtube.com/watch?v=tKcLaGdvabM&ab_channel=DeepLearningAI\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "0WKbTYwBbZ_L"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}