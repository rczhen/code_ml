{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPu29rLK4LF6JJnuF/0tjOx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rczhen/code_ml/blob/main/attention.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import inspect\n",
        "from dataclasses import dataclass\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F"
      ],
      "metadata": {
        "id": "bYaqKhZZCvgV"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclass\n",
        "class GPTConfig:\n",
        "    block_size: int = 1024\n",
        "    vocab_size: int = 50304 # GPT-2 vocab_size of 50257, padded up to nearest multiple of 64 for efficiency\n",
        "    n_layer: int = 12\n",
        "    n_head: int = 12\n",
        "    n_embd: int = 768\n",
        "    dropout: float = 0.0\n",
        "    bias: bool = True # True: bias in Linears and LayerNorms, like GPT-2. False: a bit better and faster\n",
        "\n",
        "@dataclass\n",
        "class Config:\n",
        "    block_size: int = 8\n",
        "    vocab_size: int = 50304 # GPT-2 vocab_size of 50257, padded up to nearest multiple of 64 for efficiency\n",
        "    n_layers: int = 2\n",
        "    n_heads: int = 4\n",
        "    d_model: int = 16\n",
        "    dropout_rate: float = 0.0\n",
        "    bias: bool = True # True: bias in Linears and LayerNorms, like GPT-2. False: a bit better and faster"
      ],
      "metadata": {
        "id": "aTdb4pojIvKR"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CausalSelfAttention(nn.Module):\n",
        "    def __init__(self, config) -> None:\n",
        "        super().__init__()\n",
        "        assert config.d_model % config.n_heads == 0\n",
        "\n",
        "        self.n_heads = config.n_heads\n",
        "        self.d_model = config.d_model\n",
        "        self.head_size = config.d_model // config.n_heads\n",
        "        self.block_size = config.block_size\n",
        "\n",
        "        self.attention_dropout = nn.Dropout(config.dropout_rate) # after softmax\n",
        "        self.residual_dropout = nn.Dropout(config.dropout_rate) # after attention block, before adding with residual connection\n",
        "\n",
        "        self.w_qkv = nn.Linear(self.d_model, 3 * self.d_model)\n",
        "        self.w_o = nn.Linear(self.d_model, self.d_model)\n",
        "\n",
        "        self.register_buffer(\"mask\", torch.tril(torch.ones(self.block_size, self.block_size)) # register buffer for low triangular matrix mask\n",
        "                                    .view(1, 1, self.block_size, self.block_size))  # reshape for (B, n_head, T, T) inputs\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, D = x.size() # batch size, sequence length, d_model\n",
        "\n",
        "        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n",
        "        q, k, v = self.w_qkv(x).split(self.d_model, dim=2) # (B, T, D) @ (D, 3D) --> (B, T, 3D) --> split at dim=2 --> (B, T, D)\n",
        "        q = q.view(B, T, self.n_heads, self.head_size).transpose(1, 2) # (B, T, nh, hs) --> (B, nh, T, hs), hs for head_size\n",
        "        k = k.view(B, T, self.n_heads, self.head_size).transpose(1, 2) # (B, T, nh, hs) --> (B, nh, T, hs), hs for head_size\n",
        "        v = v.view(B, T, self.n_heads, self.head_size).transpose(1, 2) # (B, T, nh, hs) --> (B, nh, T, hs), hs for head_size\n",
        "\n",
        "        # attention\n",
        "        attention = q @ k.transpose(-1, -2) # (B, nh, T, hs) @ (B, nh, hs, T) --> (B, nh, T, T)\n",
        "        attention *= self.head_size ** -0.5 # scaled dot product attention\n",
        "        attention = attention.masked_fill(self.mask[:, :, :T, :T] == 0, float('-inf'))\n",
        "        attention = F.softmax(attention, dim=-1)\n",
        "        attention = self.attention_dropout(attention)\n",
        "\n",
        "        # output\n",
        "        y = attention @ v # (B, nh, T, T) @ (B, nh, T, hs) --> (B, nh, T, hs)\n",
        "        y = y.transpose(1, 2).contiguous().view(B, T, D) # (B, nh, T, hs) --> (B, T, nh, hs) --> (B, T, D)\n",
        "        y = self.w_o(y) # (B, T, D) @ (D, D) --> (B, T, D)\n",
        "        y = self.residual_dropout(y)\n",
        "\n",
        "        return y\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "why scaled?\n",
        "if no normalization, the variance of weights wil be on the order of head_size, here is 16\n",
        "when deviding by sqrt(head_size), bring the variance back\n",
        "\"Scaled\" attention additional divides attention scores by 1/sqrt(head_size).\n",
        "so when input Q,K are unit variance, attentions will be unit variance too\n",
        "and Softmax will stay diffuse and not saturate too much.\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "id": "VCpKzgz7JaH3",
        "outputId": "be597d1e-5829-4714-89e3-42a98b37e6f9"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nwhy scaled? \\nif no normalization, the variance of weights wil be on the order of head_size, here is 16\\nwhen deviding by sqrt(head_size), bring the variance back\\n\"Scaled\" attention additional divides attention scores by 1/sqrt(head_size). \\nso when input Q,K are unit variance, attentions will be unit variance too\\nand Softmax will stay diffuse and not saturate too much.\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.rand(1, Config.block_size, Config.d_model)\n",
        "print(x)\n",
        "layer = CausalSelfAttention(Config)\n",
        "print(layer(x))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3_tilA7bPIQ3",
        "outputId": "4f715ecc-5792-4b48-b9a8-5ec7140206f1"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[0.5348, 0.4341, 0.4442, 0.5332, 0.4855, 0.7372, 0.8768, 0.4357,\n",
            "          0.3864, 0.5594, 0.1571, 0.1295, 0.4696, 0.7287, 0.9868, 0.8231],\n",
            "         [0.7122, 0.8425, 0.4135, 0.6154, 0.7098, 0.0762, 0.3721, 0.8747,\n",
            "          0.8312, 0.4790, 0.2323, 0.8670, 0.6137, 0.8522, 0.5907, 0.9117],\n",
            "         [0.2219, 0.5944, 0.6904, 0.6469, 0.8516, 0.9672, 0.8572, 0.9335,\n",
            "          0.1321, 0.1047, 0.6608, 0.3437, 0.5292, 0.3708, 0.1160, 0.3417],\n",
            "         [0.4557, 0.9365, 0.1227, 0.0639, 0.4600, 0.2101, 0.2688, 0.0186,\n",
            "          0.5006, 0.6234, 0.2310, 0.2598, 0.2191, 0.0989, 0.3002, 0.9022],\n",
            "         [0.3349, 0.2037, 0.1727, 0.9087, 0.4454, 0.9141, 0.4793, 0.0218,\n",
            "          0.7232, 0.7357, 0.3814, 0.8412, 0.6648, 0.3664, 0.3311, 0.3038],\n",
            "         [0.2569, 0.0541, 0.4904, 0.1427, 0.8338, 0.5350, 0.7984, 0.9666,\n",
            "          0.9394, 0.0824, 0.8052, 0.2830, 0.7710, 0.9442, 0.8201, 0.5451],\n",
            "         [0.9633, 0.0586, 0.4428, 0.1028, 0.0578, 0.4362, 0.4324, 0.9554,\n",
            "          0.9741, 0.3322, 0.4701, 0.2976, 0.2531, 0.7127, 0.3330, 0.7840],\n",
            "         [0.8896, 0.1605, 0.4818, 0.2478, 0.1232, 0.5168, 0.7822, 0.7644,\n",
            "          0.2560, 0.0464, 0.7696, 0.3987, 0.9363, 0.4396, 0.8690, 0.0014]]])\n",
            "tensor([[[ 4.1515e-01,  6.1883e-01, -3.0700e-01,  5.2609e-01,  6.0544e-01,\n",
            "          -3.0501e-02, -7.2930e-01,  3.4503e-01,  3.4323e-01, -1.0466e-01,\n",
            "          -2.4507e-01,  3.2964e-02, -3.7777e-02,  7.7627e-03, -1.8668e-01,\n",
            "          -4.2581e-01],\n",
            "         [ 4.7802e-01,  5.7294e-01, -4.3246e-01,  5.8949e-01,  6.4178e-01,\n",
            "          -2.8559e-02, -7.1520e-01,  3.9842e-01,  4.5517e-01, -5.6118e-02,\n",
            "          -2.9750e-01,  1.8513e-02, -1.1246e-02,  6.7076e-02, -1.2225e-01,\n",
            "          -4.5230e-01],\n",
            "         [ 4.2206e-01,  4.5236e-01, -4.2897e-01,  5.3364e-01,  5.7465e-01,\n",
            "          -5.9764e-02, -6.0078e-01,  3.9825e-01,  3.5993e-01, -1.9259e-02,\n",
            "          -2.6668e-01,  7.8133e-02, -1.2004e-02,  3.2119e-02, -7.7592e-02,\n",
            "          -4.0128e-01],\n",
            "         [ 3.8869e-01,  4.3580e-01, -4.2956e-01,  4.9971e-01,  5.4165e-01,\n",
            "          -6.8193e-02, -5.5248e-01,  3.4165e-01,  3.4633e-01, -4.6474e-03,\n",
            "          -2.2273e-01,  5.7144e-02, -3.5036e-02,  1.0107e-02, -9.3303e-02,\n",
            "          -3.6941e-01],\n",
            "         [ 3.9453e-01,  3.7346e-01, -4.3443e-01,  4.6695e-01,  5.5476e-01,\n",
            "          -8.6817e-02, -5.0792e-01,  3.2378e-01,  3.3664e-01,  9.1358e-03,\n",
            "          -2.2588e-01,  8.1125e-02, -3.9976e-02,  8.6819e-04, -9.3948e-02,\n",
            "          -3.2376e-01],\n",
            "         [ 3.8644e-01,  4.0720e-01, -4.2154e-01,  4.8541e-01,  5.5236e-01,\n",
            "          -7.6334e-02, -5.1266e-01,  3.5739e-01,  3.4273e-01, -5.1428e-04,\n",
            "          -2.4139e-01,  1.0226e-01, -3.9938e-02,  6.9222e-03, -7.1388e-02,\n",
            "          -3.4050e-01],\n",
            "         [ 4.0337e-01,  4.3487e-01, -4.1899e-01,  5.0509e-01,  5.6766e-01,\n",
            "          -6.7395e-02, -5.3595e-01,  3.6082e-01,  3.7081e-01, -1.8005e-02,\n",
            "          -2.6432e-01,  9.5924e-02, -4.2863e-02,  1.1551e-02, -6.4208e-02,\n",
            "          -3.5566e-01],\n",
            "         [ 4.1707e-01,  4.1615e-01, -4.2034e-01,  4.9320e-01,  5.8890e-01,\n",
            "          -5.7714e-02, -5.3954e-01,  3.7647e-01,  3.7112e-01, -2.9432e-02,\n",
            "          -2.7140e-01,  1.0653e-01, -3.6674e-02,  1.9699e-02, -5.0417e-02,\n",
            "          -3.5310e-01]]], grad_fn=<ViewBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "k3lQMWKcMETf"
      },
      "execution_count": 22,
      "outputs": []
    }
  ]
}