{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM0q9RCXViuxpt9Rc1vdUPo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rczhen/code_ml/blob/main/attention.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import inspect\n",
        "from dataclasses import dataclass\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "# reference: https://github.com/karpathy/nanoGPT/blob/master/model.py"
      ],
      "metadata": {
        "id": "bYaqKhZZCvgV"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclass\n",
        "class GPTConfig:\n",
        "    block_size: int = 1024\n",
        "    vocab_size: int = 50304 # GPT-2 vocab_size of 50257, padded up to nearest multiple of 64 for efficiency\n",
        "    n_layer: int = 12\n",
        "    n_head: int = 12\n",
        "    n_embd: int = 768\n",
        "    dropout: float = 0.0\n",
        "    bias: bool = True # True: bias in Linears and LayerNorms, like GPT-2. False: a bit better and faster\n",
        "\n",
        "@dataclass\n",
        "class Config:\n",
        "    block_size: int = 8\n",
        "    vocab_size: int = 50304 # GPT-2 vocab_size of 50257, padded up to nearest multiple of 64 for efficiency\n",
        "    n_layers: int = 2\n",
        "    n_heads: int = 4\n",
        "    n_kv_heads: int = 2\n",
        "    d_model: int = 16\n",
        "    dropout_rate: float = 0.0\n",
        "    bias: bool = True # True: bias in Linears and LayerNorms, like GPT-2. False: a bit better and faster.\n",
        "                      # for LayerNorm, the reason not using bias, may be the motivation of RMSNorm, i.e. re-scaling is important, re-centering is not"
      ],
      "metadata": {
        "id": "aTdb4pojIvKR"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# v1: standard CausalSelfAttention"
      ],
      "metadata": {
        "id": "losC6CqJ2IAD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CausalSelfAttention(nn.Module):\n",
        "    def __init__(self, config) -> None:\n",
        "        super().__init__()\n",
        "        assert config.d_model % config.n_heads == 0\n",
        "\n",
        "        self.n_heads = config.n_heads\n",
        "        self.d_model = config.d_model\n",
        "        self.head_size = config.d_model // config.n_heads\n",
        "        self.block_size = config.block_size\n",
        "\n",
        "        self.attention_dropout = nn.Dropout(config.dropout_rate) # after softmax\n",
        "        self.residual_dropout = nn.Dropout(config.dropout_rate) # after attention block, before adding with residual connection\n",
        "\n",
        "        self.w_qkv = nn.Linear(self.d_model, 3 * self.d_model)\n",
        "        self.w_o = nn.Linear(self.d_model, self.d_model)\n",
        "\n",
        "        self.register_buffer(\"mask\", torch.tril(torch.ones(self.block_size, self.block_size)) # register buffer for low triangular matrix mask\n",
        "                                    .view(1, 1, self.block_size, self.block_size))  # reshape for (B, n_head, T, T) inputs\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, D = x.size() # batch size, sequence length, d_model\n",
        "\n",
        "        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n",
        "        q, k, v = self.w_qkv(x).split(self.d_model, dim=2) # (B, T, D) @ (D, 3D) --> (B, T, 3D) --> split at dim=2 --> (B, T, D)\n",
        "        q = q.view(B, T, self.n_heads, self.head_size).transpose(1, 2) # (B, T, nh, hs) --> (B, nh, T, hs), hs for head_size\n",
        "        k = k.view(B, T, self.n_heads, self.head_size).transpose(1, 2) # (B, T, nh, hs) --> (B, nh, T, hs), hs for head_size\n",
        "        v = v.view(B, T, self.n_heads, self.head_size).transpose(1, 2) # (B, T, nh, hs) --> (B, nh, T, hs), hs for head_size\n",
        "\n",
        "        # attention\n",
        "        attention = q @ k.transpose(-1, -2) # (B, nh, T, hs) @ (B, nh, hs, T) --> (B, nh, T, T)\n",
        "        attention *= self.head_size ** -0.5 # scaled dot product attention\n",
        "        attention = attention.masked_fill(self.mask[:, :, :T, :T] == 0, float('-inf'))\n",
        "        attention = F.softmax(attention, dim=-1)\n",
        "        attention = self.attention_dropout(attention)\n",
        "\n",
        "        # output\n",
        "        y = attention @ v # (B, nh, T, T) @ (B, nh, T, hs) --> (B, nh, T, hs)\n",
        "        y = y.transpose(1, 2).contiguous().view(B, T, D) # (B, nh, T, hs) --> (B, T, nh, hs) --> (B, T, D)\n",
        "        y = self.w_o(y) # (B, T, D) @ (D, D) --> (B, T, D)\n",
        "        y = self.residual_dropout(y)\n",
        "\n",
        "        return y\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "why scaled?\n",
        "if no normalization, the variance of weights wil be on the order of head_size, here is 16\n",
        "when deviding by sqrt(head_size), bring the variance back\n",
        "\"Scaled\" attention additional divides attention scores by 1/sqrt(head_size).\n",
        "so when input Q,K are unit variance, attentions will be unit variance too\n",
        "and Softmax will stay diffuse and not saturate too much.\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "id": "VCpKzgz7JaH3",
        "outputId": "f265088b-5225-4689-cd92-eff73c3612f0"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nwhy scaled? \\nif no normalization, the variance of weights wil be on the order of head_size, here is 16\\nwhen deviding by sqrt(head_size), bring the variance back\\n\"Scaled\" attention additional divides attention scores by 1/sqrt(head_size). \\nso when input Q,K are unit variance, attentions will be unit variance too\\nand Softmax will stay diffuse and not saturate too much.\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.rand(1, Config.block_size, Config.d_model)\n",
        "layer = CausalSelfAttention(Config)\n",
        "print(layer(x))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3_tilA7bPIQ3",
        "outputId": "eff013cf-7e88-4caf-f6ad-8c6a24546981"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[ 0.4004,  0.5980,  0.5896, -0.0539,  0.1002, -0.0575, -0.2603,\n",
            "          -0.3139,  0.3912,  0.0885,  0.0016,  0.4248, -0.1102, -0.1208,\n",
            "          -0.2419,  0.3066],\n",
            "         [ 0.3270,  0.6477,  0.5268, -0.0702,  0.1035, -0.1017, -0.3017,\n",
            "          -0.3382,  0.3299,  0.1279,  0.0534,  0.4702,  0.0201, -0.1228,\n",
            "          -0.3237,  0.2428],\n",
            "         [ 0.2985,  0.5682,  0.5130, -0.0482,  0.1458, -0.0866, -0.3149,\n",
            "          -0.3121,  0.3063,  0.1244,  0.0519,  0.4533,  0.0221, -0.0707,\n",
            "          -0.2767,  0.2646],\n",
            "         [ 0.2555,  0.5477,  0.4681, -0.0656,  0.1501, -0.0931, -0.2824,\n",
            "          -0.3103,  0.2383,  0.1213,  0.0534,  0.4383,  0.0465, -0.0278,\n",
            "          -0.2521,  0.2358],\n",
            "         [ 0.2507,  0.5460,  0.4752, -0.0700,  0.1498, -0.1363, -0.2806,\n",
            "          -0.2952,  0.2235,  0.1202,  0.0859,  0.4288,  0.0455, -0.0308,\n",
            "          -0.2474,  0.2685],\n",
            "         [ 0.2504,  0.5350,  0.4752, -0.0766,  0.1567, -0.1345, -0.2461,\n",
            "          -0.2719,  0.2220,  0.0921,  0.0915,  0.4133,  0.0514, -0.0114,\n",
            "          -0.2299,  0.2546],\n",
            "         [ 0.2754,  0.5501,  0.5070, -0.0725,  0.1510, -0.1625, -0.2353,\n",
            "          -0.2990,  0.2132,  0.0928,  0.0824,  0.4411,  0.0222, -0.0358,\n",
            "          -0.2293,  0.2892],\n",
            "         [ 0.2568,  0.5421,  0.5002, -0.0682,  0.1496, -0.1534, -0.2399,\n",
            "          -0.2967,  0.2079,  0.0941,  0.0793,  0.4467,  0.0401, -0.0214,\n",
            "          -0.2296,  0.2692]]], grad_fn=<ViewBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# v2: Talking Heads Attention"
      ],
      "metadata": {
        "id": "Db73Nty82UX-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CausalSelfAttention(nn.Module):\n",
        "    def __init__(self, config, talking_heads=True) -> None:\n",
        "        super().__init__()\n",
        "        assert config.d_model % config.n_heads == 0\n",
        "\n",
        "        self.n_heads = config.n_heads\n",
        "        self.d_model = config.d_model\n",
        "        self.head_size = config.d_model // config.n_heads\n",
        "        self.block_size = config.block_size\n",
        "\n",
        "        self.attention_dropout = nn.Dropout(config.dropout_rate) # after softmax\n",
        "        self.residual_dropout = nn.Dropout(config.dropout_rate) # after attention block, before adding with residual connection\n",
        "\n",
        "        self.w_qkv = nn.Linear(self.d_model, 3 * self.d_model)\n",
        "        self.w_o = nn.Linear(self.d_model, self.d_model)\n",
        "\n",
        "        # talking heads: 引入两个小的(n_heads, n_heads)矩阵, 在immediately before and after softmax, 进行head之间的线性变换\n",
        "        # 2 learnable linear transformations, process (1) attention scores (right after Q*V and scaling, before masking),\n",
        "        #   (2) logits (attention logits, after masking, optional padding, softmax, before weighted summing keys)\n",
        "        self.talking_heads = talking_heads\n",
        "        if self.talking_heads:\n",
        "            self.w_talking_weights = nn.Linear(self.n_heads, self.n_heads)\n",
        "            self.w_talking_logits = nn.Linear(self.n_heads, self.n_heads)\n",
        "\n",
        "        self.register_buffer(\"mask\", torch.tril(torch.ones(self.block_size, self.block_size)) # register buffer for low triangular matrix mask\n",
        "                                    .view(1, 1, self.block_size, self.block_size))  # reshape for (B, n_head, T, T) inputs\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, D = x.size() # batch size, sequence length, d_model\n",
        "\n",
        "        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n",
        "        q, k, v = self.w_qkv(x).split(self.d_model, dim=2) # (B, T, D) @ (D, 3D) --> (B, T, 3D) --> split at dim=2 --> (B, T, D)\n",
        "        q = q.view(B, T, self.n_heads, self.head_size).transpose(1, 2) # (B, T, nh, hs) --> (B, nh, T, hs), hs for head_size\n",
        "        k = k.view(B, T, self.n_heads, self.head_size).transpose(1, 2) # (B, T, nh, hs) --> (B, nh, T, hs), hs for head_size\n",
        "        v = v.view(B, T, self.n_heads, self.head_size).transpose(1, 2) # (B, T, nh, hs) --> (B, nh, T, hs), hs for head_size\n",
        "\n",
        "        # attention\n",
        "        attention = q @ k.transpose(-1, -2) # (B, nh, T, hs) @ (B, nh, hs, T) --> (B, nh, T, T)\n",
        "        attention *= self.head_size ** -0.5 # scaled dot product attention\n",
        "        if self.talking_heads:\n",
        "            attention = attention.permute(0, 2, 3, 1) # (B, nh, T, T) --> (B, T, T, nh); (B, nh, T_q, T_k) --> (B, T_q, T_k, nh) when query and key have different length\n",
        "            attention = self.w_talking_weights(attention) # (B, T, T, nh) @ (nh, nh) --> (B, T, T, nh)\n",
        "            attention = attention.permute(0, 3, 1, 2) # (B, T, T, nh) --> (B, nh, T, T)\n",
        "        attention = attention.masked_fill(self.mask[:, :, :T, :T] == 0, float('-inf'))\n",
        "        attention = F.softmax(attention, dim=-1)\n",
        "        if self.talking_heads:\n",
        "            attention = attention.permute(0, 2, 3, 1)  # (B, nh, T, T) --> (B, T, T, nh)\n",
        "            attention = self.w_talking_logits(attention) # (B, T, T, nh) @ (nh, nh) --> (B, T, T, nh)\n",
        "            attention = attention.permute(0, 3, 1, 2) # (B, T, T, nh) --> (B, nh, T, T)\n",
        "        attention = self.attention_dropout(attention)\n",
        "\n",
        "        # output\n",
        "        y = attention @ v # (B, nh, T, T) @ (B, nh, T, hs) --> (B, nh, T, hs)\n",
        "        y = y.transpose(1, 2).contiguous().view(B, T, D) # (B, nh, T, hs) --> (B, T, nh, hs) --> (B, T, D)\n",
        "        y = self.w_o(y) # (B, T, D) @ (D, D) --> (B, T, D)\n",
        "        y = self.residual_dropout(y)\n",
        "\n",
        "        return y"
      ],
      "metadata": {
        "id": "k3lQMWKcMETf"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.rand(1, Config.block_size, Config.d_model)\n",
        "layer = CausalSelfAttention(Config)\n",
        "print(layer(x))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mgVrfGkU3QHR",
        "outputId": "ed5e1496-ea6d-4a07-8d05-7a25c4a0a045"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[ 0.0665, -0.2212, -0.1294, -0.6010,  0.2886, -0.0849, -0.1126,\n",
            "           0.5110, -0.2082, -0.4438, -0.1416,  0.0666,  0.1182,  0.2938,\n",
            "           0.1773, -0.0441],\n",
            "         [ 0.0755, -0.2447, -0.1591, -0.5085,  0.2118, -0.1085, -0.0031,\n",
            "           0.5469, -0.2007, -0.4433, -0.1104,  0.1142,  0.0752,  0.1899,\n",
            "           0.1563, -0.0866],\n",
            "         [ 0.0481, -0.2075, -0.1224, -0.4976,  0.2144, -0.1059, -0.0053,\n",
            "           0.5651, -0.1682, -0.4552, -0.0725,  0.0670,  0.0768,  0.2074,\n",
            "           0.1868, -0.0627],\n",
            "         [ 0.0658, -0.2205, -0.1184, -0.4823,  0.2013, -0.0866,  0.0091,\n",
            "           0.5799, -0.1682, -0.4842, -0.0686,  0.0645,  0.0737,  0.2011,\n",
            "           0.1866, -0.0600],\n",
            "         [ 0.0394, -0.2117, -0.1212, -0.5000,  0.2139, -0.1206, -0.0051,\n",
            "           0.5750, -0.1629, -0.4585, -0.0665,  0.0563,  0.0997,  0.1984,\n",
            "           0.1722, -0.0850],\n",
            "         [ 0.0494, -0.2151, -0.1200, -0.4883,  0.2011, -0.1035,  0.0121,\n",
            "           0.5853, -0.1697, -0.4641, -0.0616,  0.0436,  0.0911,  0.1993,\n",
            "           0.1764, -0.0790],\n",
            "         [ 0.0518, -0.2161, -0.1382, -0.4902,  0.2003, -0.1052,  0.0291,\n",
            "           0.5882, -0.1694, -0.4557, -0.0725,  0.0505,  0.0898,  0.1895,\n",
            "           0.1725, -0.0886],\n",
            "         [ 0.0427, -0.2124, -0.1329, -0.4965,  0.1959, -0.1094,  0.0094,\n",
            "           0.5912, -0.1836, -0.4385, -0.0785,  0.0273,  0.1091,  0.2058,\n",
            "           0.1712, -0.0886]]], grad_fn=<ViewBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "so7Wjm353QSZ"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# v3: GQA, MQA, MLA"
      ],
      "metadata": {
        "id": "I3zZQz2G3RQP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiQueryAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    Multi-Query Attention: Uses single key and value heads shared across all query heads.\n",
        "    \"\"\"\n",
        "    def __init__(self, config) -> None:\n",
        "        super().__init__()\n",
        "        assert config.d_model % config.n_heads == 0\n",
        "\n",
        "        self.n_heads = config.n_heads\n",
        "        self.d_model = config.d_model\n",
        "        self.head_size = config.d_model // config.n_heads\n",
        "        self.block_size = config.block_size\n",
        "\n",
        "        self.attention_dropout = nn.Dropout(config.dropout_rate) # after softmax\n",
        "        self.residual_dropout = nn.Dropout(config.dropout_rate) # after attention block, before adding with residual connection\n",
        "\n",
        "        # MODIFIED: Separate projections - Q has n_heads, K&V have only 1 head each\n",
        "        self.w_q = nn.Linear(self.d_model, self.d_model)\n",
        "        self.w_k = nn.Linear(self.d_model, self.head_size)\n",
        "        self.w_v = nn.Linear(self.d_model, self.head_size)\n",
        "        self.w_o = nn.Linear(self.d_model, self.d_model)\n",
        "\n",
        "        self.register_buffer(\"mask\", torch.tril(torch.ones(self.block_size, self.block_size)) # register buffer for low triangular matrix mask\n",
        "                                    .view(1, 1, self.block_size, self.block_size))  # reshape for (B, n_head, T, T) inputs\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, D = x.size() # batch size, sequence length, d_model\n",
        "\n",
        "        # MODIFIED: Separate Q, K, V projections\n",
        "        q = self.w_q(x).view(B, T, self.n_heads, self.head_size).transpose(1, 2) # (B, T, D) --> (B, T, nh, hs) --> (B, nh, T, hs)\n",
        "        k = self.w_k(x).view(B, T, 1, self.head_size).transpose(1, 2) # (B, T, hs) --> (B, T, 1, hs) --> (B, 1, T, hs)\n",
        "        v = self.w_v(x).view(B, T, 1, self.head_size).transpose(1, 2) # (B, T, hs) --> (B, T, 1, hs) --> (B, 1, T, hs)\n",
        "\n",
        "        # MODIFIED: Expand K&V to match Q's head dimension for broadcasting\n",
        "        # expansion is not a must, but will make broadcasting more clear\n",
        "        k = k.expand(B, self.n_heads, T, self.head_size) # (B, 1, T, hs) --> (B, nh, T, hs)\n",
        "        v = v.expand(B, self.n_heads, T, self.head_size) # (B, 1, T, hs) --> (B, nh, T, hs)\n",
        "\n",
        "        # attention\n",
        "        attention = q @ k.transpose(-1, -2) # (B, nh, T, hs) @ (B, nh, hs, T) --> (B, nh, T, T)\n",
        "        attention *= self.head_size ** -0.5 # scaled dot product attention\n",
        "        attention = attention.masked_fill(self.mask[:, :, :T, :T] == 0, float('-inf'))\n",
        "        attention = F.softmax(attention, dim=-1)\n",
        "        attention = self.attention_dropout(attention)\n",
        "\n",
        "        # output\n",
        "        y = attention @ v # (B, nh, T, T) @ (B, nh, T, hs) --> (B, nh, T, hs)\n",
        "        y = y.transpose(1, 2).contiguous().view(B, T, D) # (B, nh, T, hs) --> (B, T, nh, hs) --> (B, T, D)\n",
        "        y = self.w_o(y) # (B, T, D) @ (D, D) --> (B, T, D)\n",
        "        y = self.residual_dropout(y)\n",
        "\n",
        "        return y"
      ],
      "metadata": {
        "id": "zHmA1cDh3QUW"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.rand(1, Config.block_size, Config.d_model)\n",
        "layer = MultiQueryAttention(Config)\n",
        "print(layer(x))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t9ALZD1oN-lV",
        "outputId": "e06db8ab-a932-4858-ae45-4d0cd4ceae15"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[-0.0089, -0.1340, -0.2575, -0.0265,  0.1558,  0.3195,  0.2710,\n",
            "          -0.2836, -0.1693, -0.0023,  0.1426, -0.0382, -0.1539,  0.0310,\n",
            "           0.1011,  0.0823],\n",
            "         [-0.0349, -0.1151, -0.3152, -0.0322,  0.1249,  0.2989,  0.2809,\n",
            "          -0.2436, -0.1454,  0.0034,  0.1665, -0.0396, -0.1526,  0.0235,\n",
            "           0.0900,  0.1204],\n",
            "         [ 0.0193, -0.1448, -0.2811, -0.0327,  0.1149,  0.2927,  0.2965,\n",
            "          -0.2318, -0.1360,  0.0403,  0.2389,  0.0062, -0.1406,  0.0070,\n",
            "           0.0880,  0.0946],\n",
            "         [ 0.0499, -0.1823, -0.2870, -0.0109,  0.1140,  0.3278,  0.3338,\n",
            "          -0.2283, -0.1626,  0.0739,  0.3091,  0.0432, -0.1271,  0.0119,\n",
            "           0.0722,  0.0947],\n",
            "         [ 0.0237, -0.1871, -0.3040, -0.0161,  0.1049,  0.3428,  0.3317,\n",
            "          -0.2379, -0.1550,  0.0535,  0.2957,  0.0377, -0.1396,  0.0060,\n",
            "           0.0905,  0.1305],\n",
            "         [ 0.0360, -0.1961, -0.2937, -0.0102,  0.1115,  0.3513,  0.3367,\n",
            "          -0.2427, -0.1639,  0.0606,  0.3052,  0.0442, -0.1368,  0.0081,\n",
            "           0.0885,  0.1201],\n",
            "         [ 0.0519, -0.2031, -0.2770, -0.0128,  0.1123,  0.3473,  0.3354,\n",
            "          -0.2447, -0.1608,  0.0664,  0.3167,  0.0538, -0.1350,  0.0030,\n",
            "           0.0912,  0.1105],\n",
            "         [ 0.0468, -0.2026, -0.2845, -0.0161,  0.1041,  0.3453,  0.3379,\n",
            "          -0.2400, -0.1524,  0.0675,  0.3218,  0.0554, -0.1363, -0.0008,\n",
            "           0.0941,  0.1189]]], grad_fn=<ViewBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class GroupedQueryAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    Grouped Query Attention: Groups multiple query heads to share K&V heads. Compromise between MHA and MQA.\n",
        "    Simple implementation. Using repeat_interleave which requires extra memory.\n",
        "    \"\"\"\n",
        "    def __init__(self, config) -> None:\n",
        "        super().__init__()\n",
        "        assert config.d_model % config.n_heads == 0\n",
        "\n",
        "        self.n_heads = config.n_heads\n",
        "        self.d_model = config.d_model\n",
        "        self.head_size = config.d_model // config.n_heads\n",
        "        self.block_size = config.block_size\n",
        "\n",
        "        # MODIFIED: Number of K&V heads (groups)\n",
        "        assert config.n_heads % config.n_kv_heads == 0, \"n_heads must be divisible by n_kv_heads\"\n",
        "        self.n_kv_heads = config.n_kv_heads\n",
        "        self.n_reps = self.n_heads // self.n_kv_heads\n",
        "\n",
        "        self.attention_dropout = nn.Dropout(config.dropout_rate) # after softmax\n",
        "        self.residual_dropout = nn.Dropout(config.dropout_rate) # after attention block, before adding with residual connection\n",
        "\n",
        "         # MODIFIED: Separate QKV projections\n",
        "        self.w_q = nn.Linear(self.d_model, self.d_model)\n",
        "        self.w_k = nn.Linear(self.d_model, self.head_size * self.n_kv_heads)\n",
        "        self.w_v = nn.Linear(self.d_model, self.head_size * self.n_kv_heads)\n",
        "        self.w_o = nn.Linear(self.d_model, self.d_model)\n",
        "\n",
        "        self.register_buffer(\"mask\", torch.tril(torch.ones(self.block_size, self.block_size)) # register buffer for low triangular matrix mask\n",
        "                                    .view(1, 1, self.block_size, self.block_size))  # reshape for (B, n_head, T, T) inputs\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, D = x.size() # batch size, sequence length, d_model\n",
        "\n",
        "        # MODIFIED: separate QKV\n",
        "        q = self.w_q(x).view(B, T, self.n_heads, self.head_size).transpose(1, 2) # (B, T, D)@(D, D)-->(B, T, D)-->(B, T, nh, hs)-->(B, nh, T, hs)\n",
        "        k = self.w_k(x).view(B, T, self.n_kv_heads, self.head_size).transpose(1, 2) # (B, T, nh_kv*hs)-->(B, T, nh_kv, hs)-->(B, nh_kv, T, hs)\n",
        "        v = self.w_v(x).view(B, T, self.n_kv_heads, self.head_size).transpose(1, 2) # (B, T, nh_kv*hs)-->(B, T, nh_kv, hs)-->(B, nh_kv, T, hs)\n",
        "\n",
        "        # MODIFIED: Repeat K&V heads with repeat_interleave, extra memory used\n",
        "        k = torch.repeat_interleave(k, self.n_reps, dim=1) # (B, nh_kv, T, hs) --> (B, nh, T, hs)\n",
        "        v = torch.repeat_interleave(v, self.n_reps, dim=1) # (B, nh_kv, T, hs) --> (B, nh, T, hs)\n",
        "\n",
        "        # attention\n",
        "        attention = q @ k.transpose(-1, -2) # (B, nh, T, hs) @ (B, nh, hs, T) --> (B, nh, T, T)\n",
        "        attention *= self.head_size ** -0.5 # scaled dot product attention\n",
        "        attention = attention.masked_fill(self.mask[:, :, :T, :T] == 0, float('-inf'))\n",
        "        attention = F.softmax(attention, dim=-1)\n",
        "        attention = self.attention_dropout(attention)\n",
        "\n",
        "        # output\n",
        "        y = attention @ v # (B, nh, T, T) @ (B, nh, T, hs) --> (B, nh, T, hs)\n",
        "        y = y.transpose(1, 2).contiguous().view(B, T, D) # (B, nh, T, hs) --> (B, T, nh, hs) --> (B, T, D)\n",
        "        y = self.w_o(y) # (B, T, D) @ (D, D) --> (B, T, D)\n",
        "        y = self.residual_dropout(y)\n",
        "\n",
        "        return y\n"
      ],
      "metadata": {
        "id": "Hb1rTr84Eg76"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.rand(1, Config.block_size, Config.d_model)\n",
        "layer = GroupedQueryAttention(Config)\n",
        "print(layer(x))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CpcJytspRNHn",
        "outputId": "f0018213-a3b0-4c4a-a78d-6bd82eb76582"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[-0.0879,  0.2400,  0.3814,  0.4572, -0.6883, -0.5715,  0.4995,\n",
            "           0.0522,  0.3881,  0.4449,  0.2672,  0.5377, -0.1666, -0.1189,\n",
            "          -0.2822, -0.0406],\n",
            "         [-0.0295,  0.2227,  0.3507,  0.3563, -0.6217, -0.5170,  0.4981,\n",
            "           0.0667,  0.2725,  0.4023,  0.1905,  0.4688, -0.1318, -0.0580,\n",
            "          -0.3032, -0.0014],\n",
            "         [-0.0785,  0.1981,  0.3914,  0.3099, -0.6436, -0.5381,  0.5371,\n",
            "           0.0389,  0.3159,  0.4041,  0.1765,  0.4879, -0.1453, -0.0330,\n",
            "          -0.3453,  0.0031],\n",
            "         [-0.0412,  0.2127,  0.3815,  0.3468, -0.5874, -0.5222,  0.4987,\n",
            "           0.0622,  0.2921,  0.3794,  0.1878,  0.4841, -0.1380, -0.0410,\n",
            "          -0.3167, -0.0192],\n",
            "         [-0.0593,  0.2154,  0.3756,  0.3351, -0.6243, -0.5268,  0.5278,\n",
            "           0.0546,  0.3093,  0.3947,  0.1951,  0.5102, -0.1402, -0.0340,\n",
            "          -0.3339,  0.0181],\n",
            "         [-0.0679,  0.2016,  0.3807,  0.3361, -0.6380, -0.5300,  0.5290,\n",
            "           0.0374,  0.3119,  0.4002,  0.1829,  0.4891, -0.1548, -0.0328,\n",
            "          -0.3314,  0.0078],\n",
            "         [-0.0705,  0.2022,  0.3955,  0.3374, -0.6540, -0.5072,  0.5228,\n",
            "           0.0046,  0.3024,  0.4051,  0.1701,  0.4491, -0.1886, -0.0297,\n",
            "          -0.3265,  0.0183],\n",
            "         [-0.0705,  0.1988,  0.4020,  0.3702, -0.6648, -0.5015,  0.5212,\n",
            "          -0.0095,  0.3112,  0.4032,  0.1694,  0.4535, -0.2048, -0.0366,\n",
            "          -0.3208,  0.0157]]], grad_fn=<ViewBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# uses broadcasting and reshaping to avoid memory duplication:\n",
        "\n",
        "# Reshape Q instead of expanding K,V: Groups Q heads into (B, n_kv_heads, n_rep, T, hs)\n",
        "# Use broadcasting: K,V are unsqueezed to (B, n_kv_heads, 1, T, hs) for broadcasting\n",
        "# No memory duplication: All operations work with views and broadcasting\n",
        "\n",
        "class GroupedQueryAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    Grouped Query Attention: Groups multiple query heads to share K&V heads. Compromise between MHA and MQA.\n",
        "    \"\"\"\n",
        "    def __init__(self, config) -> None:\n",
        "        super().__init__()\n",
        "        assert config.d_model % config.n_heads == 0\n",
        "\n",
        "        self.n_heads = config.n_heads\n",
        "        self.d_model = config.d_model\n",
        "        self.head_size = config.d_model // config.n_heads\n",
        "        self.block_size = config.block_size\n",
        "\n",
        "        # MODIFIED: Number of K&V heads (groups)\n",
        "        assert config.n_heads % config.n_kv_heads == 0, \"n_heads must be divisible by n_kv_heads\"\n",
        "        self.n_kv_heads = config.n_kv_heads\n",
        "        self.n_reps = self.n_heads // self.n_kv_heads\n",
        "\n",
        "        self.attention_dropout = nn.Dropout(config.dropout_rate) # after softmax\n",
        "        self.residual_dropout = nn.Dropout(config.dropout_rate) # after attention block, before adding with residual connection\n",
        "\n",
        "        # MODIFIED: Separate QKV projections\n",
        "        self.w_q = nn.Linear(self.d_model, self.d_model)\n",
        "        self.w_k = nn.Linear(self.d_model, self.head_size * self.n_kv_heads)\n",
        "        self.w_v = nn.Linear(self.d_model, self.head_size * self.n_kv_heads)\n",
        "        self.w_o = nn.Linear(self.d_model, self.d_model)\n",
        "\n",
        "        # MODIFIED: mask size to fit the (B, nh_kv, n_rep, T, T) attention shape\n",
        "        self.register_buffer(\"mask\", torch.tril(torch.ones(self.block_size, self.block_size)) # register buffer for low triangular matrix mask\n",
        "                                    .view(1, 1, 1, self.block_size, self.block_size))  # reshape for (B, nh_kv, n_rep, T, T) inputs\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, D = x.size() # batch size, sequence length, d_model\n",
        "\n",
        "        # MODIFIED: separate QKV\n",
        "        q = self.w_q(x).view(B, T, self.n_heads, self.head_size).transpose(1, 2) # (B, T, D)@(D, D)-->(B, T, D)-->(B, T, nh, hs)-->(B, nh, T, hs)\n",
        "        k = self.w_k(x).view(B, T, self.n_kv_heads, self.head_size).transpose(1, 2) # (B, T, nh_kv*hs)-->(B, T, nh_kv, hs)-->(B, nh_kv, T, hs)\n",
        "        v = self.w_v(x).view(B, T, self.n_kv_heads, self.head_size).transpose(1, 2) # (B, T, nh_kv*hs)-->(B, T, nh_kv, hs)-->(B, nh_kv, T, hs)\n",
        "\n",
        "        # MODIFIED v2: reshape Q + expanding KV, no repeat_interleave, no extra memory use.\n",
        "        # Q: (B, nh, T, hs) -(reshape)-> (B, nh_kv, n_rep, T, hs),\n",
        "        # K&V: (B, nh_kv, T, hs) -(reshape)-> (B, nh_kv, 1, T, hs) -(expand)-> (B, nh_kv, n_rep, T, hs)\n",
        "        q = q.view(B, self.n_kv_heads, self.n_reps, T, self.head_size)\n",
        "        k = k.view(B, self.n_kv_heads, 1, T, self.head_size).expand(B, self.n_kv_heads, self.n_reps, T, self.head_size)\n",
        "        v = v.view(B, self.n_kv_heads, 1, T, self.head_size).expand(B, self.n_kv_heads, self.n_reps, T, self.head_size)\n",
        "\n",
        "        # MODIFIED v2: attention\n",
        "        attention = q @ k.transpose(-1, -2) # (B, nh_kv, n_rep, T, hs) @ (B, nh_kv, n_rep, hs, T) --> (B, nh_kv, n_rep, T, T)\n",
        "        attention *= self.head_size ** -0.5 # scaled dot product attention\n",
        "        attention = attention.masked_fill(self.mask[:, :, :, :T, :T] == 0, float('-inf'))\n",
        "        attention = F.softmax(attention, dim=-1)\n",
        "        attention = self.attention_dropout(attention)\n",
        "\n",
        "        # output\n",
        "        y = attention @ v # (B, nh_kv, n_rep, T, T) @ (B, nh_kv, n_rep, T, hs) --> (B, nh_kv, n_rep, T, hs)\n",
        "        y = y.permute(0, 3, 1, 2, 4).contiguous().view(B, T, D) # (B, nh_kv, n_rep, T, hs)-->(B, T, nh_kv, n_rep, hs)-->(B, T, D), was (B, nh, T, hs)-->(B, T, nh, hs)-->(B, T, D)\n",
        "        y = self.w_o(y) # (B, T, D) @ (D, D) --> (B, T, D)\n",
        "        y = self.residual_dropout(y)\n",
        "\n",
        "        return y\n"
      ],
      "metadata": {
        "id": "yBHO1vlYURme"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.rand(1, Config.block_size, Config.d_model)\n",
        "layer = GroupedQueryAttention(Config)\n",
        "print(layer(x))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9vzb9-b0URpi",
        "outputId": "f93e9ef5-74b1-41fb-bfea-245cbcf7f63c"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[ 0.4045,  0.1040, -0.1319,  0.0223, -0.0322,  0.3646, -0.1368,\n",
            "          -0.2340,  0.0120, -0.3832, -0.0699, -0.2371, -0.0072, -0.4912,\n",
            "           0.0115, -0.3148],\n",
            "         [ 0.2969,  0.1523, -0.0991,  0.0581, -0.1015,  0.2936, -0.1960,\n",
            "          -0.1718, -0.0031, -0.4423,  0.0079, -0.1886, -0.0448, -0.4556,\n",
            "           0.0661, -0.2229],\n",
            "         [ 0.3067,  0.1638, -0.1413,  0.0782, -0.1533,  0.3009, -0.1730,\n",
            "          -0.1629,  0.0260, -0.4450, -0.0553, -0.1745, -0.0252, -0.5111,\n",
            "           0.0664, -0.3143],\n",
            "         [ 0.2796,  0.2039, -0.1180,  0.0715, -0.1636,  0.3043, -0.1814,\n",
            "          -0.1636,  0.0194, -0.4972, -0.0223, -0.1814, -0.0279, -0.5082,\n",
            "           0.0759, -0.2658],\n",
            "         [ 0.2330,  0.2026, -0.0992,  0.0606, -0.1850,  0.2850, -0.1863,\n",
            "          -0.1281, -0.0032, -0.4625, -0.0174, -0.1755, -0.0331, -0.5127,\n",
            "           0.0734, -0.2376],\n",
            "         [ 0.2497,  0.2138, -0.1104,  0.0779, -0.2040,  0.2711, -0.1984,\n",
            "          -0.1141,  0.0188, -0.4913, -0.0115, -0.1745, -0.0456, -0.5136,\n",
            "           0.0977, -0.2554],\n",
            "         [ 0.2331,  0.2105, -0.0971,  0.0865, -0.2244,  0.2654, -0.2147,\n",
            "          -0.0768, -0.0013, -0.4719, -0.0169, -0.1698, -0.0499, -0.5247,\n",
            "           0.0925, -0.2782],\n",
            "         [ 0.2616,  0.2020, -0.1000,  0.0831, -0.2071,  0.2764, -0.2159,\n",
            "          -0.0894, -0.0092, -0.4595, -0.0060, -0.1690, -0.0508, -0.4944,\n",
            "           0.0807, -0.2983]]], grad_fn=<ViewBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadLatentAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    Multi-Head Latent Attention (MLA): Compresses K&V into lower-dimensional latent representations.\n",
        "    paper: https://arxiv.org/pdf/2405.04434, section 2.1\n",
        "    \"\"\"\n",
        "    def __init__(self, config, latent_dim=16) -> None:\n",
        "        super().__init__()\n",
        "        assert config.d_model % config.n_heads == 0\n",
        "\n",
        "        self.n_heads = config.n_heads\n",
        "        self.d_model = config.d_model\n",
        "        self.head_size = config.d_model // config.n_heads\n",
        "        self.block_size = config.block_size\n",
        "\n",
        "        self.attention_dropout = nn.Dropout(config.dropout_rate) # after softmax\n",
        "        self.residual_dropout = nn.Dropout(config.dropout_rate) # after attention block, before adding with residual connection\n",
        "\n",
        "        # MODIFIED\n",
        "        self.latent_dim = latent_dim\n",
        "        self.w_q = nn.Linear(self.d_model, self.d_model)\n",
        "\n",
        "        # kv share the same latent vector, hence only \"one\" latent vector need to be cached during inference.\n",
        "        # plus, during inference, w_k_decode/w_v_decode can be merged with w_q/w_o respectively, hence no decoding required\n",
        "        self.w_kv_compress = nn.Linear(self.d_model, latent_dim, bias=False)\n",
        "        self.w_k_decode = nn.Linear(latent_dim, self.d_model, bias=False)\n",
        "        self.w_v_decode = nn.Linear(latent_dim, self.d_model, bias=False)\n",
        "\n",
        "        self.w_o = nn.Linear(self.d_model, self.d_model)\n",
        "\n",
        "        self.register_buffer(\"mask\", torch.tril(torch.ones(self.block_size, self.block_size)) # register buffer for low triangular matrix mask\n",
        "                                    .view(1, 1, self.block_size, self.block_size))  # reshape for (B, n_head, T, T) inputs\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, D = x.size() # batch size, sequence length, d_model\n",
        "\n",
        "        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n",
        "        q = self.w_q(x) # (B, T, D) @ (D, D) --> (B, T, D)\n",
        "        latent = self.w_kv_compress(x) # (D, latent_dim)\n",
        "        k = self.w_k_decode(latent) # (B, T, D)\n",
        "        v = self.w_v_decode(latent) # (B, T, D)\n",
        "\n",
        "        q = q.view(B, T, self.n_heads, self.head_size).transpose(1, 2) # (B, T, nh, hs) --> (B, nh, T, hs)\n",
        "        k = k.view(B, T, self.n_heads, self.head_size).transpose(1, 2) # (B, T, nh, hs) --> (B, nh, T, hs)\n",
        "        v = v.view(B, T, self.n_heads, self.head_size).transpose(1, 2) # (B, T, nh, hs) --> (B, nh, T, hs)\n",
        "\n",
        "        # attention\n",
        "        attention = q @ k.transpose(-1, -2) # (B, nh, T, hs) @ (B, nh, hs, T) --> (B, nh, T, T)\n",
        "        attention *= self.head_size ** -0.5 # scaled dot product attention\n",
        "        attention = attention.masked_fill(self.mask[:, :, :T, :T] == 0, float('-inf'))\n",
        "        attention = F.softmax(attention, dim=-1)\n",
        "        attention = self.attention_dropout(attention)\n",
        "\n",
        "        # output\n",
        "        y = attention @ v # (B, nh, T, T) @ (B, nh, T, hs) --> (B, nh, T, hs)\n",
        "        y = y.transpose(1, 2).contiguous().view(B, T, D) # (B, nh, T, hs) --> (B, T, nh, hs) --> (B, T, D)\n",
        "        y = self.w_o(y) # (B, T, D) @ (D, D) --> (B, T, D)\n",
        "        y = self.residual_dropout(y)\n",
        "\n",
        "        return y"
      ],
      "metadata": {
        "id": "p9swPY8AEg_A"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.rand(1, Config.block_size, Config.d_model)\n",
        "layer = MultiHeadLatentAttention(Config)\n",
        "print(layer(x))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BpYZAHBpEhBg",
        "outputId": "2ed49a18-52f0-44dd-b8ab-278cfcdd6eed"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[ 2.8464e-02, -2.1961e-01, -1.5361e-01,  4.4400e-02, -1.5287e-01,\n",
            "           2.8343e-01,  1.2061e-01, -1.2442e-02,  2.9561e-02, -1.2725e-02,\n",
            "          -1.0118e-01,  5.4990e-02,  2.6925e-01,  1.5270e-01,  4.3015e-01,\n",
            "          -5.4792e-02],\n",
            "         [ 9.9094e-03, -1.7150e-01, -1.3730e-01,  1.0227e-01, -1.8065e-01,\n",
            "           2.9052e-01,  1.0725e-01, -4.3471e-02,  5.4774e-02, -2.5308e-02,\n",
            "          -8.8920e-02, -3.6775e-04,  1.5665e-01,  1.1298e-01,  3.9812e-01,\n",
            "           4.1996e-03],\n",
            "         [-6.1434e-03, -1.2881e-01, -1.0459e-01,  1.3003e-01, -1.8325e-01,\n",
            "           2.7150e-01,  6.8840e-02, -3.5086e-02,  9.8818e-02, -5.4927e-02,\n",
            "          -8.5095e-02, -2.2444e-03,  1.3021e-01,  9.1252e-02,  3.7169e-01,\n",
            "           6.3924e-03],\n",
            "         [-1.2102e-02, -1.0252e-01, -9.9437e-02,  1.4513e-01, -1.8700e-01,\n",
            "           2.3622e-01,  4.6900e-02, -3.2036e-02,  1.3167e-01, -7.9713e-02,\n",
            "          -5.2629e-02, -1.1770e-02,  1.1101e-01,  7.5719e-02,  3.2709e-01,\n",
            "           4.0546e-03],\n",
            "         [-2.5903e-02, -9.7067e-02, -1.2377e-01,  1.6518e-01, -1.8079e-01,\n",
            "           2.3776e-01,  5.3009e-02, -5.0873e-02,  1.2350e-01, -6.9995e-02,\n",
            "          -6.9371e-02, -2.5777e-02,  1.0735e-01,  8.8061e-02,  3.2678e-01,\n",
            "          -2.6873e-02],\n",
            "         [-5.2382e-02, -9.4700e-02, -1.2311e-01,  1.6851e-01, -1.8690e-01,\n",
            "           2.4961e-01,  5.0337e-02, -4.0217e-02,  1.3819e-01, -5.6615e-02,\n",
            "          -7.6454e-02, -2.9508e-02,  1.3305e-01,  1.0954e-01,  3.2531e-01,\n",
            "          -3.4254e-02],\n",
            "         [-5.8328e-02, -8.9814e-02, -1.1732e-01,  1.6776e-01, -1.9242e-01,\n",
            "           2.3811e-01,  3.7913e-02, -4.3395e-02,  1.5229e-01, -5.5180e-02,\n",
            "          -6.9160e-02, -3.1250e-02,  1.3492e-01,  1.1315e-01,  3.0914e-01,\n",
            "          -3.4085e-02],\n",
            "         [-6.5186e-02, -8.5920e-02, -1.1292e-01,  1.7297e-01, -1.9387e-01,\n",
            "           2.3492e-01,  2.4736e-02, -4.5748e-02,  1.6130e-01, -5.9199e-02,\n",
            "          -7.2423e-02, -3.0967e-02,  1.4065e-01,  1.1946e-01,  3.0371e-01,\n",
            "          -4.2889e-02]]], grad_fn=<ViewBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GPT"
      ],
      "metadata": {
        "id": "68iySV3hEh9H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MLP(nn.Moduele):\n",
        "    def __init__(self, config) -> None:\n",
        "        super().__init__()\n",
        "\n",
        "        self.fc1 = nn.Linear(config.d_model, 4 * config.d_model)\n",
        "        self.fc2 = nn.Linear(4 * config.d_model, 4 * config.d_model)\n",
        "        self.gelu = nn.GELU()\n",
        "        self.dropout = nn.Dropout(config.dropout_rate)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.gelu(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.dropout(x)\n",
        "        return x\n",
        "\n",
        "# https://docs.pytorch.org/docs/stable/generated/torch.nn.GELU.html\n",
        "# gelu(x) = x * phi(x), where phi(x) is the Cumulative Distribution Function for Gaussian Distribution."
      ],
      "metadata": {
        "id": "wlRLcXEQEkYG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Block(nn.Module):\n",
        "    def __init__(self, config) -> None:\n",
        "        super().__init__()\n",
        "\n",
        "        self.attention_layer = CausalSelfAttention(config)\n",
        "        self.ffn = MLP(config)\n",
        "        self.ln1 = nn.LayerNorm(config.d_model)\n",
        "        self.ln2 = nn.LayerNorm(config.d_model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Pre-layer-norm: LN before attention and FFN\n",
        "        x = x + self.attention_layer(self.ln1(x))\n",
        "        x = x + self.ffn(self.ln2(x))\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "WsnLqeO-Es62"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aQ3A2DCjEs92"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DIN (Q & KV have different seq length)"
      ],
      "metadata": {
        "id": "MB4RIho0bRo2"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0WKbTYwBbZ_L"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}