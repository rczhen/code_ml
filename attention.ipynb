{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMkucD6nPGpClvXC7A7z3Do",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rczhen/code_ml/blob/main/attention.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import inspect\n",
        "from dataclasses import dataclass\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F"
      ],
      "metadata": {
        "id": "bYaqKhZZCvgV"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclass\n",
        "class GPTConfig:\n",
        "    block_size: int = 1024\n",
        "    vocab_size: int = 50304 # GPT-2 vocab_size of 50257, padded up to nearest multiple of 64 for efficiency\n",
        "    n_layer: int = 12\n",
        "    n_head: int = 12\n",
        "    n_embd: int = 768\n",
        "    dropout: float = 0.0\n",
        "    bias: bool = True # True: bias in Linears and LayerNorms, like GPT-2. False: a bit better and faster\n",
        "\n",
        "@dataclass\n",
        "class Config:\n",
        "    block_size: int = 8\n",
        "    vocab_size: int = 50304 # GPT-2 vocab_size of 50257, padded up to nearest multiple of 64 for efficiency\n",
        "    n_layers: int = 2\n",
        "    n_heads: int = 4\n",
        "    d_model: int = 16\n",
        "    dropout_rate: float = 0.0\n",
        "    bias: bool = True # True: bias in Linears and LayerNorms, like GPT-2. False: a bit better and faster"
      ],
      "metadata": {
        "id": "aTdb4pojIvKR"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# v1: standard CausalSelfAttention"
      ],
      "metadata": {
        "id": "losC6CqJ2IAD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CausalSelfAttention(nn.Module):\n",
        "    def __init__(self, config) -> None:\n",
        "        super().__init__()\n",
        "        assert config.d_model % config.n_heads == 0\n",
        "\n",
        "        self.n_heads = config.n_heads\n",
        "        self.d_model = config.d_model\n",
        "        self.head_size = config.d_model // config.n_heads\n",
        "        self.block_size = config.block_size\n",
        "\n",
        "        self.attention_dropout = nn.Dropout(config.dropout_rate) # after softmax\n",
        "        self.residual_dropout = nn.Dropout(config.dropout_rate) # after attention block, before adding with residual connection\n",
        "\n",
        "        self.w_qkv = nn.Linear(self.d_model, 3 * self.d_model)\n",
        "        self.w_o = nn.Linear(self.d_model, self.d_model)\n",
        "\n",
        "        self.register_buffer(\"mask\", torch.tril(torch.ones(self.block_size, self.block_size)) # register buffer for low triangular matrix mask\n",
        "                                    .view(1, 1, self.block_size, self.block_size))  # reshape for (B, n_head, T, T) inputs\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, D = x.size() # batch size, sequence length, d_model\n",
        "\n",
        "        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n",
        "        q, k, v = self.w_qkv(x).split(self.d_model, dim=2) # (B, T, D) @ (D, 3D) --> (B, T, 3D) --> split at dim=2 --> (B, T, D)\n",
        "        q = q.view(B, T, self.n_heads, self.head_size).transpose(1, 2) # (B, T, nh, hs) --> (B, nh, T, hs), hs for head_size\n",
        "        k = k.view(B, T, self.n_heads, self.head_size).transpose(1, 2) # (B, T, nh, hs) --> (B, nh, T, hs), hs for head_size\n",
        "        v = v.view(B, T, self.n_heads, self.head_size).transpose(1, 2) # (B, T, nh, hs) --> (B, nh, T, hs), hs for head_size\n",
        "\n",
        "        # attention\n",
        "        attention = q @ k.transpose(-1, -2) # (B, nh, T, hs) @ (B, nh, hs, T) --> (B, nh, T, T)\n",
        "        attention *= self.head_size ** -0.5 # scaled dot product attention\n",
        "        attention = attention.masked_fill(self.mask[:, :, :T, :T] == 0, float('-inf'))\n",
        "        attention = F.softmax(attention, dim=-1)\n",
        "        attention = self.attention_dropout(attention)\n",
        "\n",
        "        # output\n",
        "        y = attention @ v # (B, nh, T, T) @ (B, nh, T, hs) --> (B, nh, T, hs)\n",
        "        y = y.transpose(1, 2).contiguous().view(B, T, D) # (B, nh, T, hs) --> (B, T, nh, hs) --> (B, T, D)\n",
        "        y = self.w_o(y) # (B, T, D) @ (D, D) --> (B, T, D)\n",
        "        y = self.residual_dropout(y)\n",
        "\n",
        "        return y\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "why scaled?\n",
        "if no normalization, the variance of weights wil be on the order of head_size, here is 16\n",
        "when deviding by sqrt(head_size), bring the variance back\n",
        "\"Scaled\" attention additional divides attention scores by 1/sqrt(head_size).\n",
        "so when input Q,K are unit variance, attentions will be unit variance too\n",
        "and Softmax will stay diffuse and not saturate too much.\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "id": "VCpKzgz7JaH3",
        "outputId": "ba7477ec-cfc2-4f35-81b9-ee007992eea8"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nwhy scaled? \\nif no normalization, the variance of weights wil be on the order of head_size, here is 16\\nwhen deviding by sqrt(head_size), bring the variance back\\n\"Scaled\" attention additional divides attention scores by 1/sqrt(head_size). \\nso when input Q,K are unit variance, attentions will be unit variance too\\nand Softmax will stay diffuse and not saturate too much.\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.rand(1, Config.block_size, Config.d_model)\n",
        "print(x)\n",
        "layer = CausalSelfAttention(Config)\n",
        "print(layer(x))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3_tilA7bPIQ3",
        "outputId": "a9375288-d9ef-4b36-b378-4f5697bf01f0"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[0.1335, 0.5033, 0.0119, 0.9521, 0.0588, 0.9284, 0.3936, 0.6225,\n",
            "          0.6452, 0.3971, 0.9477, 0.7729, 0.7849, 0.9677, 0.5737, 0.1898],\n",
            "         [0.0463, 0.7900, 0.2029, 0.2007, 0.9862, 0.8995, 0.8501, 0.4497,\n",
            "          0.0779, 0.4141, 0.6336, 0.6112, 0.7698, 0.5033, 0.5788, 0.3431],\n",
            "         [0.2839, 0.9898, 0.9354, 0.7228, 0.7328, 0.5463, 0.4106, 0.4918,\n",
            "          0.5689, 0.0622, 0.2844, 0.7631, 0.0073, 0.0599, 0.2667, 0.1991],\n",
            "         [0.1219, 0.4216, 0.6814, 0.1920, 0.7979, 0.9971, 0.0258, 0.6272,\n",
            "          0.6598, 0.0274, 0.6750, 0.7370, 0.5341, 0.4181, 0.9826, 0.5319],\n",
            "         [0.5073, 0.6594, 0.3810, 0.3264, 0.5320, 0.2145, 0.2166, 0.9515,\n",
            "          0.6532, 0.6341, 0.8759, 0.1684, 0.2516, 0.3369, 0.1295, 0.6470],\n",
            "         [0.8165, 0.4878, 0.8657, 0.7369, 0.4041, 0.3254, 0.9028, 0.6001,\n",
            "          0.1756, 0.1283, 0.1517, 0.7840, 0.8976, 0.4481, 0.9875, 0.7592],\n",
            "         [0.1032, 0.7290, 0.5401, 0.6724, 0.4569, 0.0051, 0.1456, 0.9306,\n",
            "          0.9603, 0.6033, 0.2806, 0.2433, 0.3356, 0.1294, 0.7252, 0.6078],\n",
            "         [0.5655, 0.2020, 0.1409, 0.4767, 0.6185, 0.9751, 0.9227, 0.1304,\n",
            "          0.0354, 0.6776, 0.1661, 0.8632, 0.4176, 0.5310, 0.0424, 0.8274]]])\n",
            "tensor([[[ 0.3622, -0.7585,  0.1746,  0.0592,  0.2357, -0.3599, -0.1901,\n",
            "           0.0520, -0.3618, -0.2674, -0.1691,  0.1149,  0.7611,  0.2544,\n",
            "           0.0546,  0.0048],\n",
            "         [ 0.2423, -0.6702,  0.1369,  0.0238,  0.1893, -0.2531, -0.2107,\n",
            "          -0.0359, -0.3884, -0.2788, -0.2291,  0.0507,  0.6437,  0.1947,\n",
            "           0.0010,  0.0230],\n",
            "         [ 0.1961, -0.6541,  0.1146,  0.0338,  0.1808, -0.1504, -0.2186,\n",
            "          -0.0599, -0.3505, -0.2530, -0.1988,  0.0311,  0.5926,  0.1497,\n",
            "          -0.0091,  0.0116],\n",
            "         [ 0.1464, -0.6422,  0.1174, -0.0093,  0.1411, -0.0987, -0.1941,\n",
            "          -0.0706, -0.3751, -0.3001, -0.2131, -0.0340,  0.5564,  0.1751,\n",
            "           0.0406,  0.0481],\n",
            "         [ 0.1530, -0.6416,  0.1218,  0.0176,  0.1322, -0.1102, -0.1940,\n",
            "          -0.0674, -0.3470, -0.2728, -0.2225, -0.0059,  0.5623,  0.1437,\n",
            "           0.0175,  0.0113],\n",
            "         [ 0.1296, -0.6246,  0.1097,  0.0144,  0.1494, -0.0535, -0.2091,\n",
            "          -0.0967, -0.3285, -0.2667, -0.2125, -0.0127,  0.5353,  0.1431,\n",
            "           0.0256,  0.0134],\n",
            "         [ 0.1069, -0.6356,  0.1028,  0.0202,  0.1504, -0.0275, -0.1921,\n",
            "          -0.0881, -0.3304, -0.2694, -0.2131, -0.0254,  0.5364,  0.1133,\n",
            "           0.0222,  0.0173],\n",
            "         [ 0.1162, -0.6369,  0.1127,  0.0172,  0.1424, -0.0450, -0.2095,\n",
            "          -0.0968, -0.3204, -0.2695, -0.2309, -0.0144,  0.5424,  0.1465,\n",
            "           0.0334, -0.0056]]], grad_fn=<ViewBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# v2: Talking Heads Attention"
      ],
      "metadata": {
        "id": "Db73Nty82UX-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CausalSelfAttention(nn.Module):\n",
        "    def __init__(self, config, talking_heads=True) -> None:\n",
        "        super().__init__()\n",
        "        assert config.d_model % config.n_heads == 0\n",
        "\n",
        "        self.n_heads = config.n_heads\n",
        "        self.d_model = config.d_model\n",
        "        self.head_size = config.d_model // config.n_heads\n",
        "        self.block_size = config.block_size\n",
        "\n",
        "        self.attention_dropout = nn.Dropout(config.dropout_rate) # after softmax\n",
        "        self.residual_dropout = nn.Dropout(config.dropout_rate) # after attention block, before adding with residual connection\n",
        "\n",
        "        self.w_qkv = nn.Linear(self.d_model, 3 * self.d_model)\n",
        "        self.w_o = nn.Linear(self.d_model, self.d_model)\n",
        "\n",
        "        # talking heads: 引入两个小的(n_heads, n_heads)矩阵, 在immediately before and after softmax, 进行head之间的线性变换\n",
        "        # 2 learnable linear transformations, process (1) attention scores (right after Q*V and scaling, before masking),\n",
        "        #   (2) logits (attention logits, after masking, optional padding, softmax, before weighted summing keys)\n",
        "        self.talking_heads = talking_heads\n",
        "        if self.talking_heads:\n",
        "            self.w_talking_weights = nn.Linear(self.n_heads, self.n_heads)\n",
        "            self.w_talking_logits = nn.Linear(self.n_heads, self.n_heads)\n",
        "\n",
        "        self.register_buffer(\"mask\", torch.tril(torch.ones(self.block_size, self.block_size)) # register buffer for low triangular matrix mask\n",
        "                                    .view(1, 1, self.block_size, self.block_size))  # reshape for (B, n_head, T, T) inputs\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, D = x.size() # batch size, sequence length, d_model\n",
        "\n",
        "        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n",
        "        q, k, v = self.w_qkv(x).split(self.d_model, dim=2) # (B, T, D) @ (D, 3D) --> (B, T, 3D) --> split at dim=2 --> (B, T, D)\n",
        "        q = q.view(B, T, self.n_heads, self.head_size).transpose(1, 2) # (B, T, nh, hs) --> (B, nh, T, hs), hs for head_size\n",
        "        k = k.view(B, T, self.n_heads, self.head_size).transpose(1, 2) # (B, T, nh, hs) --> (B, nh, T, hs), hs for head_size\n",
        "        v = v.view(B, T, self.n_heads, self.head_size).transpose(1, 2) # (B, T, nh, hs) --> (B, nh, T, hs), hs for head_size\n",
        "\n",
        "        # attention\n",
        "        attention = q @ k.transpose(-1, -2) # (B, nh, T, hs) @ (B, nh, hs, T) --> (B, nh, T, T)\n",
        "        attention *= self.head_size ** -0.5 # scaled dot product attention\n",
        "        if self.talking_heads:\n",
        "            attention = attention.permute(0, 2, 3, 1) # (B, nh, T, T) --> (B, T, T, nh); (B, nh, T_q, T_k) --> (B, T_q, T_k, nh) when query and key have different length\n",
        "            attention = self.w_talking_weights(attention) # (B, T, T, nh) @ (nh, nh) --> (B, T, T, nh)\n",
        "            attention = attention.permute(0, 3, 1, 2) # (B, T, T, nh) --> (B, nh, T, T)\n",
        "        attention = attention.masked_fill(self.mask[:, :, :T, :T] == 0, float('-inf'))\n",
        "        attention = F.softmax(attention, dim=-1)\n",
        "        if self.talking_heads:\n",
        "            attention = attention.permute(0, 2, 3, 1)  # (B, nh, T, T) --> (B, T, T, nh)\n",
        "            attention = self.w_talking_logits(attention) # (B, T, T, nh) @ (nh, nh) --> (B, T, T, nh)\n",
        "            attention = attention.permute(0, 3, 1, 2) # (B, T, T, nh) --> (B, nh, T, T)\n",
        "        attention = self.attention_dropout(attention)\n",
        "\n",
        "        # output\n",
        "        y = attention @ v # (B, nh, T, T) @ (B, nh, T, hs) --> (B, nh, T, hs)\n",
        "        y = y.transpose(1, 2).contiguous().view(B, T, D) # (B, nh, T, hs) --> (B, T, nh, hs) --> (B, T, D)\n",
        "        y = self.w_o(y) # (B, T, D) @ (D, D) --> (B, T, D)\n",
        "        y = self.residual_dropout(y)\n",
        "\n",
        "        return y"
      ],
      "metadata": {
        "id": "k3lQMWKcMETf"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.rand(1, Config.block_size, Config.d_model)\n",
        "print(x)\n",
        "layer = CausalSelfAttention(Config)\n",
        "print(layer(x))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mgVrfGkU3QHR",
        "outputId": "101b01d0-75d7-408d-981b-c7a00f65b7b7"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[0.3813, 0.6721, 0.2603, 0.6302, 0.2283, 0.3321, 0.0882, 0.1664,\n",
            "          0.2291, 0.5071, 0.8659, 0.1470, 0.1494, 0.0381, 0.5740, 0.2529],\n",
            "         [0.3449, 0.9917, 0.6983, 0.1112, 0.0900, 0.0011, 0.0673, 0.5607,\n",
            "          0.7021, 0.1708, 0.8078, 0.9408, 0.2962, 0.9869, 0.4653, 0.2010],\n",
            "         [0.6817, 0.4621, 0.2949, 0.4915, 0.9488, 0.6643, 0.8228, 0.3356,\n",
            "          0.9932, 0.5010, 0.5966, 0.1304, 0.2402, 0.8760, 0.5761, 0.6892],\n",
            "         [0.5934, 0.0857, 0.3116, 0.7343, 0.2737, 0.7082, 0.7385, 0.3015,\n",
            "          0.6424, 0.8895, 0.7715, 0.2097, 0.6284, 0.7914, 0.7759, 0.0373],\n",
            "         [0.9506, 0.1744, 0.7485, 0.9021, 0.7730, 0.3930, 0.5014, 0.2275,\n",
            "          0.1787, 0.6184, 0.5131, 0.0772, 0.2575, 0.4504, 0.1519, 0.9919],\n",
            "         [0.9278, 0.5570, 0.5830, 0.9215, 0.5986, 0.2957, 0.8409, 0.5637,\n",
            "          0.7597, 0.0286, 0.2481, 0.5496, 0.9791, 0.4818, 0.2578, 0.9870],\n",
            "         [0.8867, 0.8018, 0.6264, 0.6848, 0.3668, 0.9750, 0.4369, 0.2738,\n",
            "          0.5364, 0.0901, 0.5150, 0.4216, 0.0146, 0.4573, 0.2406, 0.2189],\n",
            "         [0.0794, 0.3033, 0.3535, 0.1117, 0.4001, 0.0584, 0.0970, 0.3857,\n",
            "          0.2837, 0.3838, 0.3005, 0.6019, 0.6721, 0.6087, 0.8271, 0.2449]]])\n",
            "tensor([[[ 0.1566,  0.2597, -0.1255,  0.5075,  0.0934,  0.4435, -0.6405,\n",
            "          -0.0073, -0.3818,  0.3221, -1.0635,  0.1825,  0.3751, -0.0871,\n",
            "           0.7212,  0.4482],\n",
            "         [ 0.1705,  0.2336, -0.1640,  0.5214,  0.0925,  0.4422, -0.6978,\n",
            "          -0.0421, -0.3700,  0.3573, -1.1248,  0.1655,  0.4205, -0.0820,\n",
            "           0.7655,  0.4988],\n",
            "         [ 0.1400,  0.2480, -0.1465,  0.5003,  0.1027,  0.4363, -0.7109,\n",
            "          -0.0296, -0.4008,  0.3653, -1.1365,  0.1659,  0.4053, -0.0909,\n",
            "           0.7999,  0.4760],\n",
            "         [ 0.1300,  0.2366, -0.1116,  0.5097,  0.0854,  0.4432, -0.6957,\n",
            "           0.0056, -0.4061,  0.3742, -1.1280,  0.1497,  0.4064, -0.1122,\n",
            "           0.7997,  0.4734],\n",
            "         [ 0.1282,  0.2518, -0.1038,  0.5027,  0.0896,  0.4423, -0.6895,\n",
            "           0.0120, -0.4238,  0.3759, -1.1262,  0.1495,  0.4005, -0.1111,\n",
            "           0.8042,  0.4675],\n",
            "         [ 0.1349,  0.2622, -0.0939,  0.5100,  0.0891,  0.4637, -0.6911,\n",
            "           0.0098, -0.4284,  0.4030, -1.1441,  0.1365,  0.4139, -0.1248,\n",
            "           0.8314,  0.4831],\n",
            "         [ 0.1279,  0.2583, -0.0928,  0.5087,  0.0888,  0.4670, -0.6921,\n",
            "           0.0096, -0.4267,  0.3989, -1.1415,  0.1340,  0.4135, -0.1368,\n",
            "           0.8353,  0.4842],\n",
            "         [ 0.1272,  0.2535, -0.0970,  0.5034,  0.0884,  0.4566, -0.6918,\n",
            "           0.0111, -0.4265,  0.3901, -1.1325,  0.1362,  0.4121, -0.1350,\n",
            "           0.8242,  0.4767]]], grad_fn=<ViewBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "so7Wjm353QSZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# v3: GQA, MQA, MLA"
      ],
      "metadata": {
        "id": "I3zZQz2G3RQP"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zHmA1cDh3QUW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}